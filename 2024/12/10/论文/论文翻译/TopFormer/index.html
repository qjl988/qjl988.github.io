<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>TopFormer Token Pyramid Transformer for Mobile Semantic Segmentation | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation  文章链接   Abstract 尽管vision transformers（ViTs）在计算机视觉领域取得了巨大的成功，但沉重的计算成本阻碍了它们在密集预测任务中的应用，如移动设备上的语义分割。在本文中，我们提出了一个名为Token Pyramid Vis">
<meta property="og:type" content="article">
<meta property="og:title" content="TopFormer Token Pyramid Transformer for Mobile Semantic Segmentation">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/TopFormer/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation  文章链接   Abstract 尽管vision transformers（ViTs）在计算机视觉领域取得了巨大的成功，但沉重的计算成本阻碍了它们在密集预测任务中的应用，如移动设备上的语义分割。在本文中，我们提出了一个名为Token Pyramid Vis">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:03:33.635Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/TopFormer/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'TopFormer Token Pyramid Transformer for Mobile Semantic Segmentation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">TopFormer Token Pyramid Transformer for Mobile Semantic Segmentation</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">TopFormer Token Pyramid Transformer for Mobile Semantic Segmentation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:03:33.635Z" title="更新于 2024-12-11 01:03:33">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">7.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>23分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="topformer-token-pyramid-transformer-for-mobile-semantic-segmentation"><a class="markdownIt-Anchor" href="#topformer-token-pyramid-transformer-for-mobile-semantic-segmentation"></a> TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation</h1>
<blockquote>
<p><a href="zotero://open-pdf/library/items/YSWHMLFG">文章链接</a></p>
</blockquote>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>尽管vision transformers（ViTs）在计算机视觉领域取得了巨大的成功，但沉重的计算成本阻碍了它们在密集预测任务中的应用，如移动设备上的语义分割。在本文中，我们提出了一个名为Token Pyramid Vision Transformer（TopFormer）的移动友好架构。所提出的TopFormer将不同尺度的Token作为输入，以产生尺度感知的语义特征，然后将其注入到相应的Token中以增强表示。实验结果表明，我们的方法在几个语义分割数据集上明显优于基于CNN和ViT的网络，并且在准确性和延迟之间实现了良好的权衡。在ADE20K数据集上，TopFormer在基于ARM的移动设备上以较低的延迟实现了比MobileNetV3高5%的mIoU精度。此外，TopFormer的微小版本在基于ARM的移动设备上实现了具有竞争力的实时推理结果。</p>
<h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1. Introduction</h2>
<p>vision transformers（ViTs）在少数视觉任务中表现出相当强的效果，如图像分类[11]、物体检测[28]和语义分割[58]。尽管取得了成功，但具有注意力机制的transformers架构[42]需要强大的计算资源，超出了许多移动和嵌入式设备的能力。在本文中，我们旨在探索一种专门为密集预测任务（如语义分割）设计的移动友好型vision transformers。</p>
<p>为了使vision transformers适应各种密集的预测任务，最近的vision transformers，如PVT[43]、CvT[45]、LeViT[12]、MobileViT[31]采用了分层结构，这通常用于卷积神经网络（CNN），例如AlexNet[23]、ResNet[15]。这些vision transformers在高分辨率的标记上应用全局自我注意及其变体，由于标记数量的二次复杂性，带来了沉重的计算成本。</p>
<p>为了提高效率，最近的一些作品，例如Swin Transformer[28]、Shuffle Transformer[19]、Twins[7]和HR-Former[51]，在局部/窗口区域内计算自我注意。然而，在移动设备上，窗口分区是令人惊讶的耗时。此外，Token slimming[40]和Mobile-Former[6]通过减少Token的数量来降低计算能力，但牺牲了它们的识别精度。</p>
<p>在这些vision transformers中，MobileViT[31]和Mobile-Former[6]是专门为移动设备设计的。它们都结合了CNN和ViTs的优势。对于图像分类，MobileViT在参数数量相近的情况下取得了比MobileNets[16, 36]更好的性能。Mobile-Former在较少的FLOPs数量下取得了比MobileNets更好的性能。然而，与MobileNets相比，它们在移动设备上的实际延迟方面没有显示出优势，正如[31]中所报告的那样。这提出了一个问题。有没有可能设计出在移动语义分割任务上比MobileNets有更好的性能、更低延迟的移动友好网络？</p>
<p><strong>受MobileViT和Mobile-Former的启发，我们也利用了CNN和ViT的优势。一个基于CNN的模块，表示为Token Pyramid模块，用来处理高分辨率的图像，以快速产生局部特征<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>金字塔。考虑到移动设备的计算能力非常有限，这里我们使用一些堆叠的轻量级MobileNetV2块，采用快速下采样策略来建立一个标记金字塔。为了获得丰富的语义和大的感受野，我们采用了基于ViT的模块，称为语义提取器，并将标记作为输入。为了进一步降低计算成本，使用平均池运算法则，将tokens减少到极小的数量，例如，输入大小的1/(64×64)。与ViT[11]、T2T-ViT[49]和LeViT[12]使用嵌入层的最后输出作为输入标记不同，我们将来自不同规模（阶段）的标记汇集成极小的数字（分辨率）并沿通道维度串联。然后，新的标记被送入transformer块以产生全局语义。由于Transformer块中的剩余连接，学习到的语义与标记的尺度有关，表示为scaleeaware全局语义。</strong></p>
<p>为了获得密集预测任务的强大层次特征，标度感知的全局语义被来自不同标度的标记通道所分割，然后标度感知的全局语义与相应的标记融合以增强表示。增强后的标记被用作分割头的输入。</p>
<p><img src="/img/loading.gif" data-original="images/image-20221205003950603.png" alt="image-20221205003950603" /></p>
<blockquote>
<p>图1 ADE20K估值集上的延迟、mIoU性能与模型大小的关系。以前的模型被标记为红点，而我们的模型则显示为蓝点。我们的方法实现了更好的延迟/准确度权衡。延迟是在单个高通骁龙865上测量的，输入尺寸为512×512，只使用了一个ARM CPU内核进行速度测试。没有使用其他加速手段，例如，GPU或量化。*表示输入尺寸为448×448。</p>
</blockquote>
<p>为了证明我们方法的有效性，我们在具有挑战性的分割数据集上进行了实验。ADE20K [59], Pascal Context [33] 和COCOStuff [1]. 我们考察了硬件上的延迟，即一个现成的基于ARM的计算核心。如图1所示，我们的方法以较低的延迟获得了比MobileNets更好的结果。为了证明我们方法的通用性，我们还对COCO[27]数据集进行了物体检测的实验。总结一下，我们的贡献如下。</p>
<ul>
<li>所提出的TopFormer将来自不同尺度的标记作为输入，并将这些标记汇集成非常小的数字，以便以非常低的计算成本获得尺度感知的语义。</li>
<li>所提出的语义注入模块可以将标度感知的语义注入到相应的标记中，以建立强大的分层特征，这对于密集的预测任务至关重要。</li>
<li>在ADE20K数据集上，所提出的基础模型可以达到比MobileNetV3好5%的mIoU，并且在基于ARM的移动设备上的延迟更低。微型版本可以在基于ARM的移动设备上进行实时分割，结果具有竞争力。</li>
</ul>
<h2 id="2-related-work"><a class="markdownIt-Anchor" href="#2-related-work"></a> 2. Related Work</h2>
<p>在本节中，我们从三个方面来回顾最近的方法。1）轻量级vision transformers，2）高效卷积神经网络，3）移动语义分割。</p>
<h3 id="21-轻量-vision-transformers"><a class="markdownIt-Anchor" href="#21-轻量-vision-transformers"></a> 2.1. 轻量 Vision Transformers</h3>
<p>对于在图像识别中使用transformers结构，有许多探索[18, 44, 55]。ViT[11]是第一个将纯transformers应用于图像分类的工作，取得了最先进的性能。在这之后，DeiT[41]引入了基于tokens的蒸馏，以减少训练transformers所需的数据量。T2T-ViT[49]通过递归地将相邻的tokens聚合为一个tokens来减少tokens的长度，将图像结构化为tokens。Swin Transformer[28]在每个局部窗口内计算自我注意，从而使输入标记的数量产生线性计算复杂性。然而，这些vision transformers和后续产品往往有大量的参数和沉重的计算复杂性。</p>
<p>为了建立一个轻量级的vision transformers，LeViT[12]设计了一个混合架构，使用堆叠的标准卷积层与stride-2来减少标记物的数量，然后附加一个改进的vision transformers来提取语义。对于分类任务，LeViT在CPU上的表现明显优于EfficientNet。MobileViT[31]采用了相同的策略，并使用MobilenetV2块而不是标准的卷积层对特征图进行下采样。Mobile-Former采用了带有双向桥的并行结构，并利用了MobileNet和transformer的优势。然而，在移动设备上，MobileViT和其他基于ViT的网络明显比MobileNets[16, 17, 36]慢，这在[31]中有所报道。对于分割任务，输入图像总是高分辨率的。因此，基于ViT的网络要比MobileNets执行得更快就更具有挑战性。在本文中，我们的目标是设计一个轻量级的vision transformers，它可以在分割任务中以较低的延迟胜过移动网络。</p>
<h3 id="22-efficient-convolutional-neural-networks"><a class="markdownIt-Anchor" href="#22-efficient-convolutional-neural-networks"></a> 2.2. Efficient Convolutional Neural Networks</h3>
<p>在移动和嵌入式设备上部署视觉模型的需求越来越大，这鼓励了对高效卷积神经网络设计的研究。MobileNet[16,17,36]提出了一个倒置的瓶颈结构，它将深度和点的卷积堆叠起来。IGCNet[53]和ShuffleNet[30,54]使用通道洗牌/permutation操作符来使多组卷积层的信息跨组流动。GhostNet[13]使用更便宜的算子，深度卷积，以产生更多的特征。AdderNet[2]利用加法来交换大量的乘法。MobileNeXt[60]翻转了倒置残差块的结构，提出了一个连接高维表征的构建块来代替。EfficientNet[38, 39]和TinyNet[14]研究深度、宽度和分辨率的复合缩放。</p>
<h3 id="23-mobile-semantic-segmentation"><a class="markdownIt-Anchor" href="#23-mobile-semantic-segmentation"></a> 2.3. Mobile Semantic Segmentation</h3>
<p>最准确的分割网络通常需要数十亿FLOPs的计算，这可能超过移动和嵌入式设备的计算能力。为了加快分割速度并降低计算成本，ICNet[56]使用多尺度图像作为输入，并使用级联网络，以提高工作效率。DFANet[24]利用轻量级骨干网来加快其网络速度，并提出了一个跨级别的特征聚合来提高准确性。SwiftNet[35]利用横向连接作为低成本的解决方案，在保持速度的同时恢复预测分辨率。BiSeNet[47]引入了空间路径和语义路径以减少计算量。AlignSeg[20]和SFNet[25]将相邻层次的特征图对齐，并使用特征金字塔框架进一步增强特征图。ESPNets[32]通过将标准卷积分解为点状卷积和扩张卷积的空间金字塔来节省计算量。AutoML技术[5, 10, 34]被用来搜索场景解析的有效架构。NRD[52]用动态卷积滤波网络动态地生成神经表征。LRASPP[16]采用MobileNetV3作为编码器，并提出了一个新的高效分割解码器Lite Reduced Atrous Spatial Pyramid Pooling（LR-ASPP），它仍然是移动语义分割的强大基线。</p>
<h2 id="3-architecture"><a class="markdownIt-Anchor" href="#3-architecture"></a> 3. Architecture</h2>
<p>我们的整体网络结构如图2所示。我们可以看到，我们的网络由几个部分组成。标记金字塔模块、语义提取器、语义注入模块和分割头。代号金字塔模块将图像作为输入并产生代号金字塔。vision transformers被用作语义提取器，它将tokens金字塔作为输入并产生标度器语义。语义注入模块将语义注入到相应比例的标记中，以增强表示。最后，分割头使用增强的标记金字塔来执行分割任务。接下来，我们介绍这些模块的细节。</p>
<p><img src="/img/loading.gif" data-original="images/image-20221205004024508.png" alt="image-20221205004024508" /></p>
<h3 id="31-token-pyramid-module"><a class="markdownIt-Anchor" href="#31-token-pyramid-module"></a> 3.1. Token Pyramid Module</h3>
<p>受MobileNets[36]的启发，提议的Token Pyramid Module由堆叠的MobileNet块组成[36]。与MobileNets不同的是，Token Pyramid Module不以获得丰富的语义和大的接受域为目的，而是使用较少的块来构建一个token pyramid。我们在第3.4节中展示了Token Pyramid模块的层设置。</p>
<p>如图2所示，以图像I∈R3×H×W为输入，其中3、H、W分别表示I的RGB通道、高度、宽度，我们的标记金字塔模块首先将图像通过一些MobileNetV2块[36]，产生一系列标记{T1，…，TN }，其中N表示尺度的数量2。之后，标记{T1, …, TN }，被平均汇集到目标尺寸，例如，R H 64 × W 64。最后，来自不同尺度的标记沿着通道维度被串联起来，产生新的标记。新的标记将被送入视觉transformer，以产生规模感知语义。由于新标记的数量很少，即使新标记的通道很大，视觉transformer也能以很低的计算成本运行。</p>
<h3 id="32-vision-transformer-as-scale-aware-semantics-extractor"><a class="markdownIt-Anchor" href="#32-vision-transformer-as-scale-aware-semantics-extractor"></a> 3.2. Vision Transformer as Scale-aware Semantics Extractor</h3>
<p>Scale-aware Semantics Extractor由一些堆叠的Transformer块组成。Transformer块的数量为L。Transformer块由多头注意模块、前馈网络（FFN）和剩余连接组成。为了保持标记的空间形状并减少重塑的数量，我们用一个1×1的卷积层代替线性层。此外，TopFormer的所有非线性激活都是ReLU6[17]，而不是ViT的GELU函数。</p>
<p>对于多头注意力模块，我们遵循LeViT[12]的设置，将键K和查询Q的头尺寸设置为D=16，将值V的头设置为2D=32通道。减少K和Q的通道可以减少计算注意力图和输出时的计算成本。同时，我们还放弃了层归一化层，并在每个卷积中附加了一个批归一化。批量归一化可以在推理过程中与前面的卷积合并，这样可以比层归一化运行得更快。</p>
<p>对于前馈网络，我们遵循文献[19，48]，通过在两个1×1卷积层之间插入一个深度卷积层来增强vision transformers的局部连接。为了降低计算成本，FFN的扩展系数被设置为2。transformers块的数量为L，然后头的数量将在3.4小节给出。</p>
<p>如图2所示，vision transformers将来自不同尺度的标记作为输入。为了进一步减少计算量，使用平均池运算法则将来自不同尺度的tokens数量减少到输入大小的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mrow><mn>64</mn><mo>×</mo><mn>64</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{64×64}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2484389999999999em;vertical-align:-0.403331em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">6</span><span class="mord mtight">4</span><span class="mbin mtight">×</span><span class="mord mtight">6</span><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.403331em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。来自不同尺度的集合标记具有相同的分辨率，它们被串联起来作为vision transformers的输入。视觉transformer可以获得全图像的接受域和丰富的语义。更具体地说，全局性的自我注意沿着空间维度在标记之间交换信息。1×1卷积层将在来自不同尺度的标记之间交换信息。在每个Transformer块中，在交换了所有尺度的标记的信息后，学会了残差映射，然后残差映射被添加到标记中，以增强表示和语义。最后，在通过几个transformer块之后，就得到了尺度感知的语义。</p>
<h3 id="33-semantics-injection-module-and-segmentation-head"><a class="markdownIt-Anchor" href="#33-semantics-injection-module-and-segmentation-head"></a> 3.3. Semantics Injection Module and Segmentation Head</h3>
<p>在获得标度感知语义后，我们直接将它们与其他标记<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">T^N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span></span></span>一起加入。然而，在标记<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msup><mi>T</mi><mn>1</mn></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>T</mi><mi>N</mi></msup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">{\{T^1, ..., T^N\}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord"><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span>和标度意识语义之间存在着明显的语义差距。为此，在融合这些标记之前，引入语义注入模块来缓解语义差距。如图2所示，<strong>语义注入模块（SIM）将tokens金字塔模块的本地tokens和视觉transformer的全局语义作为输入。本地标记通过1×1卷积层，然后进行批量归一化，产生要注入的特征。全局语义被送入1×1卷积层，然后是批量归一化层和sigmoid层以产生语义权重，同时，全局语义也通过1×1卷积层，然后是批量归一化。三个输出具有相同的大小。然后，全局语义通过Hadamard生产注入到本地标记中，全局语义也在注入后与特征相加。几个SIM的输出共享相同数量的通道，表示为M。</strong></p>
<p>在语义注入后，来自不同尺度的增强标记同时捕获了丰富的空间和语义信息，这对语义分割至关重要。<strong>此外，语义注入还缓解了标记之间的语义差距。所提出的分割头首先将低分辨率的标记上采样到与高分辨率的标记相同的大小，并将所有尺度的标记进行元素加和。最后，该特征通过两个卷积层来产生最终的分割图。</strong></p>
<h3 id="34-architecture-and-variants"><a class="markdownIt-Anchor" href="#34-architecture-and-variants"></a> 3.4. Architecture and Variants</h3>
<p>为了定制各种复杂度的网络，我们分别引入了TopFormer-T（TopFormer-T）、TopFormer-Small（TopFormer-S）和TopFormer-Base（TopFormer-B）。</p>
<p>表1中给出了基础、小型和微型模型的尺寸和FLOPs。基本型、小型和微型模型在每个多头自留地模块中分别有8个、6个和4个头，并有M=256、M=192和M=128作为目标通道数。关于网络配置的更多细节，请参考补充材料。</p>
<p>为了在准确性和实际延迟之间取得更好的权衡，我们选择最后三个标度T2、T3和T4的标记作为SIM和分割头的输入。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221126200902245.png" alt="image-20221126200902245" /></p>
<blockquote>
<p>表1 ADE20K值集的结果。延迟和GFLOPs计算采用512×512分辨率的图像作为输入。†表示以448×448分辨率作为输入的结果。延迟是基于单个高通Snapdragon 865处理器测量的。所有结果都是在单线程下评估的。按照MMSegmentation的设置，基于CNN的方法使用了批量大小=32。为了与Segformer进行公平的比较，TopFormer使用了批处理规模=16。mIoU是在单尺度推理中报告的。</p>
</blockquote>
<h2 id="4-experiments"><a class="markdownIt-Anchor" href="#4-experiments"></a> 4. Experiments</h2>
<p>在这一节中，我们首先对几个公共数据集进行了实验。我们描述了实施细节，并将结果与其他作品的语义分割任务进行比较。然后我们进行消融研究，分析不同部分的有效性和效率。最后，我们报告了在物体检测任务上的表现，以显示我们方法的通用能力。</p>
<h3 id="41-semantic-segmentation"><a class="markdownIt-Anchor" href="#41-semantic-segmentation"></a> 4.1. Semantic Segmentation</h3>
<h4 id="411-datasets"><a class="markdownIt-Anchor" href="#411-datasets"></a> 4.1.1 Datasets</h4>
<p>我们在三个数据集上进行了实验，ADE20K[59]，PASCAL Context[33]和COCO-Stuff[1]。类间交集的平均值被设定为我们的评价指标。全精度TopFormer模型被转换为TNN[9]，然后在一个基于ARM的计算核心上测量延迟。ADE20K。ADE20K数据集总共包含25K图像，涵盖150个类别。所有图像被分成20K/2K/3K，用于训练、验证和测试。PASCAL Context: Pascal Context数据集有4998张用于训练的场景图像和5105张用于测试的图像。有59个语义标签和1个背景标签。COCO-Stuff。COCO-Stuff[1]数据集用像素级的东西注释来增强COCO数据集。有10000张从COCO中选出的复杂图像。训练集和测试集分别由9K和1K图像组成。</p>
<h4 id="412-implementation-details"><a class="markdownIt-Anchor" href="#412-implementation-details"></a> 4.1.2 Implementation Details</h4>
<p>我们的实现是建立在MMSegmentation[8]和Pytorch之上的。它利用ImageNet预训练的TopFormer3作为骨干。标准的BatchNorm[21]层被Synchronize BatchNorm取代，以便在训练期间收集多个GPU上的BatchNorm的平均值和标准偏差。对于ADE20K数据集，我们按照Segformer使用160K调度器，批次大小为16。COCO-Stuff和PASCAL Context的训练迭代为80K。对于所有的方法和数据集，初始学习率被设定为0.00012，权重衰减为0.01。使用系数为1.0的 &quot;聚 &quot;学习率计划。在ADE20K上，我们采用与[46]相同的数据增强策略进行公平比较。训练图像首先被随机缩放，然后从结果图像中随机裁剪出固定大小的斑块。此外，我们还应用随机调整大小、随机水平翻转、随机裁剪等。在COCO-Stuff和PASCAL Context上，我们使用[8]的默认增强策略。在训练过程中，我们将PASCAL Context的图像大小调整为480×480，COCO-Stuff为512×512。最后，我们报告了验证集上的单一比例结果，以与其他方法进行比较。在推理过程中，我们遵循常见的策略，将ADE20K和COCO-Stuff的图像短边重新缩放为训练用的裁剪尺寸。至于PASCAL Context，图像被调整为480×480，然后输入我们的网络。</p>
<h4 id="413-experiments-on-ade20k"><a class="markdownIt-Anchor" href="#413-experiments-on-ade20k"></a> 4.1.3 Experiments on ADE20K</h4>
<p>我们在表1中对我们的TopFormer和以前的方法在ADE20K验证集上进行了比较。实际的延迟是在装有单个高通骁龙865处理器的移动设备上测量的。在这里，我们选择轻量级的vision transformers（ViT）[10, 22, 30, 46]和高效的卷积神经网络（CNN）[15, 16, 36, 37]作为编码器。此外，各种解码器也包括在表1中。在表1的所有方法中，基于MobilenetV2的Deeplabv3+实现了最好的mIoU（38.1%），然而，延迟超过1000毫秒，这限制了它在移动设备上的应用。</p>
<p>在这些基于CNN的基线中，采用mobilenetV3-large[16]作为编码器，LR-ASPP作为解码器的方法，在计算（2.0 GFLOPs）和精度（33.1 mIoU）之间实现了良好的权衡。继[16]之后，我们还在最后阶段减少了所有特征层的通道数，以进一步降低计算成本，表示为MobileNetV3-Large-reduce。基于较轻的骨干网，LR-ASPP可以在较低的延迟（81毫秒）下达到32.3%的mIoU。与具有可比延迟的LR-ASPP模型相比，我们的小版本TopFormer的准确度提高了3.8%。与LR-ASPP相比，微小版本的TopFormer能以2倍的计算量（0.6G vs. 1.3G）达到相当的性能。Lite-ASPP[4]是Deeplabv3+的缩减通道版本。</p>
<p>在这些基于ViT的基线中，HR-NAS-B[10]使用搜索技术将Transformer块引入HRNet[37]的设计中，也在计算量（2.2 GFLOPs）和精度（34.9 mIoU）之间实现了良好的权衡。与HR-NAS-B模型相比，我们的小版本TopFormer在计算量较少的情况下，准确度提高了1.2%。SegFormer[46]以较少的参数（3.8M）取得了很好的性能（37.4 mIoU），<strong>尽管SegFormer采用了高效的多头自关注，但由于大量的tokens，计算量仍然很大。与SegFormer相比，我们的基本版本TopFormer可以实现相当的性能，但计算量却减少了4倍以上（1.8GFLOPs对8.4GFLOPs）。</strong></p>
<p>为了在基于ARM的移动设备上实现实时分割，我们将输入图像的大小调整为448×448，并将其送入TopFormer-tiny，推理时间减少到32ms，性能略有下降。据我们所知，这是第一个基于ViT的方法可以在基于ARM的移动设备上实现实时分割，并取得有竞争力的结果。</p>
<h4 id="414-ablation-study"><a class="markdownIt-Anchor" href="#414-ablation-study"></a> 4.1.4 Ablation Study</h4>
<p>我们首先进行消融实验，讨论不同组件的影响，包括标记金字塔、语义注入模块和分割头。在不丧失一般性的情况下，所有的结果都是通过在训练集上训练和在验证（val）集上评估得到的。</p>
<h5 id="符号金字塔的影响"><a class="markdownIt-Anchor" href="#符号金字塔的影响"></a> 符号金字塔的影响。</h5>
<p>这里，我们从两个方面来讨论Token Pyramid，即把Token Pyramid作为输入的影响和从不同尺度选择Token作为输出的影响。如表2所示，我们分别进行了将不同尺度的叠加标记作为语义提取器的输入，以及将最后一个标记作为语义提取器的输入的实验。为了进行公平的比较，我们附加了一个1×1的卷积层，将通道扩大到与堆积的标记相同。实验结果证明了使用tokens金字塔作为输入的有效性。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221126201924930.png" alt="image-20221126201924930" /></p>
<p>在获得标度感知语义后，SIM将把语义注入到本地Tokens中。为了在准确性和计算成本之间追求更好的权衡，我们尝试选择不同规模的tokens进行注入。如表3所示，使用{ 1 4, 1 8, 1 16 , 1 32 }中的代币可以在最繁重的计算中达到最佳性能。使用{ 1 16 , 1 32 }中的tokens，在最轻的计算中实现了更差的性能。为了在准确性和计算成本之间实现良好的权衡，我们在所有其他实验中选择使用{ 1 8, 1 16 , 1 32 }中的tokens。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221126201935606.png" alt="image-20221126201935606" /></p>
<h5 id="规模感知语义提取器的影响"><a class="markdownIt-Anchor" href="#规模感知语义提取器的影响"></a> 规模感知语义提取器的影响。</h5>
<p>在这里，我们在Topformer-T上进行了实验来检验SASE。结果显示在表4中。这里，我们使用没有SASE的Topformer作为基线。添加SASE带来了大约10%的mIoU收益，这是一个显著的改进。为了验证Transformer块中的多头自我注意模块（MHSA），我们删除了所有的MHSA模块，并增加了更多的FFN，以进行公平的比较。结果表明，MHSA可以带来大约2.4%的mIoU收益，在精心的架构设计下，这是一个高效和有效的模块。同时，我们将SASE与流行的上下文模型，如ASPP和PPM，在TPM的基础上进行比较。如表 4 所示，&quot;+SASE &quot;比 &quot;+PSP &quot;和 &quot;+ASPP &quot;能以更少的计算成本实现更好的性能。实验结果表明，SASE更适合于在移动设备中使用。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221126202153212.png" alt="" /></p>
<h5 id="语义注入模块和分割头的影响"><a class="markdownIt-Anchor" href="#语义注入模块和分割头的影响"></a> 语义注入模块和分割头的影响。</h5>
<p>由于语义注入模块和分割头的密切关系，我们将这两者放在一起讨论。这里，我们首先讨论语义注入模块的设计。如表5所示，在一个Sigmoid层之后，将本地标记和语义相乘，表示为 “SigmoidAttn”。将语义提取器的语义添加到相应的本地标记中，表示为 “SemInfo”。与 &quot;SigmoidAttn &quot;和 &quot;SemInfo &quot;相比，同时添加 &quot;SigmoidAttn &quot;和 &quot;SemInfo &quot;可以带来相当大的改进，只需进行少量的额外计算。</p>
<p>这里，我们讨论一下分割头的设计。在将特征传入语义注入模块（Semantic Injection Module）之后，输出的分层特征既具有强大的语义，又具有丰富的空间细节。拟议的分割头只是将它们加在一起，然后使用两个1×1卷积层来预测分割图。我们还设计了另外两个分割头，如图4所示。Sum Head &quot;与SIM中只添加 &quot;SemInfo &quot;完全相同。Concat Head &quot;使用一个1×1的卷积层来减少SIM输出的通道，然后将这些特征串联起来。与 &quot;并集头 &quot;和 &quot;和集头 &quot;相比，目前的分割头可以达到更好的性能。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221126202318278.png" alt="image-20221126202318278" /></p>
<h5 id="sim中宽度的影响"><a class="markdownIt-Anchor" href="#sim中宽度的影响"></a> SIM中宽度的影响。</h5>
<p>在本文中，我们把SIM中的通道数定为M。在这里，我们研究了SIM中不同M的影响，并找到一个合适的M来实现良好的权衡。如表7所示，M=256、192、128以非常接近的计算量实现了类似的性能。因此，我们在微小模型、小模型和基本模型中分别设定M=128、192、256。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221126202349244.png" alt="image-20221126202349244" /></p>
<h5 id="输出跨度的影响"><a class="markdownIt-Anchor" href="#输出跨度的影响"></a> 输出跨度的影响。</h5>
<p>来自不同阶段的tokens被汇集到固定的分辨率，即输出跨度。表8显示了不同分辨率的结果。s32, s64, s128表示汇集的标记的分辨率为输入尺寸的1 32×32, 1 64×64, 1 128×128。考虑到计算和精度的权衡，我们选择s64作为语义提取器的输入标记的输出跨度。</p>
<h5 id="计算-参数和延迟的统计"><a class="markdownIt-Anchor" href="#计算-参数和延迟的统计"></a> 计算、参数和延迟的统计。</h5>
<p>这里，我们对所提出的TopFormer-Tiny的计算、参数和延迟进行了统计。如图3所示，虽然语义提取器的参数最多（74%），但语义提取器的FLOPs和实际延迟却相对较低（约10%）。</p>
<h4 id="415-experiments-on-pascal-context"><a class="markdownIt-Anchor" href="#415-experiments-on-pascal-context"></a> 4.1.5 Experiments on Pascal Context</h4>
<p>我们在表9中把我们的TopFormer和以前的方法在Pascal Context测试集上进行比较。我们分别评估了59个类别和60个类别（包括背景）的性能。很明显，我们的方法比之前所有基于CNNs或ViT的方法在较少的计算量下取得了更好的性能。为了更好地理解，我们分别测量了主干和头部的FLOPs。所提出的方法能以最轻的骨干和头部实现最佳性能。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221126202412885.png" alt="image-20221126202412885" /></p>
<h4 id="416-experiments-on-coco-stuff"><a class="markdownIt-Anchor" href="#416-experiments-on-coco-stuff"></a> 4.1.6 Experiments on COCO-Stuff</h4>
<p>我们在表10中对我们的TopFormer和以前的COCO-Stuff验证集的方法进行了比较。分别测量了骨干网和头部的FLOPs。可以看出，我们的方法取得了最好的性能，与计算量相当的MobileNetV3模型相比，TopFormer的基础版本的准确度提高了8%。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221126202428264.png" alt="image-20221126202428264" /></p>
<h3 id="42-object-detection"><a class="markdownIt-Anchor" href="#42-object-detection"></a> 4.2. Object Detection</h3>
<p>为了进一步证明所提出的TopFormer的泛化能力，我们对COCO数据集进行了物体检测任务。COCO由118K图像组成，用于训练，5K用于验证，20K用于测试。我们在train2017分集上训练所有模型，在val2017集上评估所有方法。我们选择RetinaNet[26]作为物体检测方法，并采用不同的骨架来产生特征金字塔。我们的实现是建立在MMdetection[3]和Pytorch之上。对于提议的TopFormer，我们用RetinaNet中的检测头代替分割头。如表11所示，基于TopFormer的RetinaNet能够以较低的计算量取得比MobileNetV3和ShuffleNet更好的性能。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221126202438751.png" alt="image-20221126202438751" /></p>
<h2 id="5-conclusion-and-limitations"><a class="markdownIt-Anchor" href="#5-conclusion-and-limitations"></a> 5. Conclusion and Limitations</h2>
<p>在本文中，我们提出了一个用于移动视觉任务的新架构。结合CNN和ViT的优势，所提出的TopFormer在准确性和计算成本之间实现了良好的权衡。TopFormer的微小版本可以在基于ARM的移动设备上产生具有竞争力的实时推理结果。实验结果证明了该方法的有效性。TopFormer的主要限制是对物体检测的微小改进。我们将继续提高物体检测的性能。此外，我们将在未来的工作中探索TopFormer在密集预测中的应用。</p>
<hr class="footnotes-sep" />
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>We use ‘features’ and ‘tokens’ interchangeably here. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Squeeze-and-Excitation-Networks/" title="Squeeze-and-Excitation Networks"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Squeeze-and-Excitation Networks</div></div><div class="info-2"><div class="info-item-1"> Squeeze-and-Excitation Networks  Abstract 卷积神经网络是建立在卷积操作之上的，它通过在局部感受野内将空间和通道信息融合在一起来提取信息特征。为了提高网络的表示能力，最近的一些方法显示了加强空间编码的好处。在这项工作中，我们专注于通道关系，并提出了一个新的结构单元，我们称之为 “挤压和激励”（SE）块，它通过明确地模拟通道之间的相互依赖性来适应性地重新校准通道方面的特征反应。我们证明，通过将这些模块堆叠在一起，我们可以构建SENet架构，该架构在具有挑战性的数据集上具有非常好的通用性。最重要的是，我们发现SE块对现有的最先进的深度架构产生了明显的性能改进，而额外的计算成本却很小。SENets构成了我们2017年ILSVRC分类提交的基础，它赢得了第一名，并将前五名的误差大大降低到2.251%，比2016年的获奖作品实现了25%的相对改进。  1. Introduction 卷积神经网络（CNN）已被证明是处理各种视觉任务的有效模型[21, 27, 33,...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Squeeze-excite-guided-few-shot-segmentation-of-volumetric-images/" title="‘Squeeze &amp; excite’ guided few-shot segmentation of volumetric images"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">‘Squeeze & excite’ guided few-shot segmentation of volumetric images</div></div><div class="info-2"><div class="info-item-1"> ‘Squeeze &amp; excite’ guided few-shot segmentation of volumetric images  文章链接   摘要 深度神经网络能够实现高度精确的图像分割，但需要大量的人工注释数据进行监督训练。小样本学习的目的是通过从小样本有注释的支持实例中学习一个新的类别来解决这个缺点。我们介绍了一个新颖的小样本框架，用于分割只有几个注释的切片的体积医学图像。与计算机视觉领域的其他相关工作相比，主要的挑战是缺乏预训练的网络和医学扫描的体积性质。我们通过提出一个新的架构来解决这些挑战，该架构用于小样本的分割，其中包括 &quot;挤压和激发 &quot;块。我们的双臂结构包括一个调节器臂，它处理注释的支持输入并产生一个特定的任务表示。这个表征被传递给分割器臂，它使用这些信息来分割新的查询图像。为了促进调节器和分割器臂之间的有效互动，我们建议使用 &quot;通道挤压和空间激励...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">CA</div></div><div class="info-2"><div class="info-item-1"> Coordinate Attention for Efficient Mobile Network Design  Abstract 最近关于移动网络设计的研究已经证明了通道注意（如SE注意力机制）对于提高模型性能的显著效果，但他们通常忽略了位置信息，而位置信息对于生成空间选择性注意图是很重要的。在本文中，我们提出了一种新的移动网络注意机制，将位置信息嵌入到通道注意中，我们称之为...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#topformer-token-pyramid-transformer-for-mobile-semantic-segmentation"><span class="toc-text"> TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-introduction"><span class="toc-text"> 1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-related-work"><span class="toc-text"> 2. Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-%E8%BD%BB%E9%87%8F-vision-transformers"><span class="toc-text"> 2.1. 轻量 Vision Transformers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-efficient-convolutional-neural-networks"><span class="toc-text"> 2.2. Efficient Convolutional Neural Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#23-mobile-semantic-segmentation"><span class="toc-text"> 2.3. Mobile Semantic Segmentation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-architecture"><span class="toc-text"> 3. Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-token-pyramid-module"><span class="toc-text"> 3.1. Token Pyramid Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-vision-transformer-as-scale-aware-semantics-extractor"><span class="toc-text"> 3.2. Vision Transformer as Scale-aware Semantics Extractor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-semantics-injection-module-and-segmentation-head"><span class="toc-text"> 3.3. Semantics Injection Module and Segmentation Head</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#34-architecture-and-variants"><span class="toc-text"> 3.4. Architecture and Variants</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-experiments"><span class="toc-text"> 4. Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-semantic-segmentation"><span class="toc-text"> 4.1. Semantic Segmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#411-datasets"><span class="toc-text"> 4.1.1 Datasets</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#412-implementation-details"><span class="toc-text"> 4.1.2 Implementation Details</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#413-experiments-on-ade20k"><span class="toc-text"> 4.1.3 Experiments on ADE20K</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#414-ablation-study"><span class="toc-text"> 4.1.4 Ablation Study</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7%E9%87%91%E5%AD%97%E5%A1%94%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text"> 符号金字塔的影响。</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A7%84%E6%A8%A1%E6%84%9F%E7%9F%A5%E8%AF%AD%E4%B9%89%E6%8F%90%E5%8F%96%E5%99%A8%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text"> 规模感知语义提取器的影响。</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E6%B3%A8%E5%85%A5%E6%A8%A1%E5%9D%97%E5%92%8C%E5%88%86%E5%89%B2%E5%A4%B4%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text"> 语义注入模块和分割头的影响。</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sim%E4%B8%AD%E5%AE%BD%E5%BA%A6%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text"> SIM中宽度的影响。</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E8%B7%A8%E5%BA%A6%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text"> 输出跨度的影响。</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97-%E5%8F%82%E6%95%B0%E5%92%8C%E5%BB%B6%E8%BF%9F%E7%9A%84%E7%BB%9F%E8%AE%A1"><span class="toc-text"> 计算、参数和延迟的统计。</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#415-experiments-on-pascal-context"><span class="toc-text"> 4.1.5 Experiments on Pascal Context</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#416-experiments-on-coco-stuff"><span class="toc-text"> 4.1.6 Experiments on COCO-Stuff</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-object-detection"><span class="toc-text"> 4.2. Object Detection</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-conclusion-and-limitations"><span class="toc-text"> 5. Conclusion and Limitations</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA">CA</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>