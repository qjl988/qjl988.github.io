<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>SegNet | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="SegNet  Abstract 我们提出了一种用于语义像素分割的新型实用深度全卷积神经网络架构，称为 SegNet。 这种可训练的核心分割引擎由一个编码器网络、一个相应的解码器网络和一个像素分类层组成。编码器网络的架构在拓扑结构上与 VGG16 网络中的 13 个卷积层相同[1]。解码器网络的作用是将低分辨率编码器特征图映射到全输入分辨率特征图，以进行像素分类。SegNet 的新颖之处在于解码">
<meta property="og:type" content="article">
<meta property="og:title" content="SegNet">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/SegNet/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="SegNet  Abstract 我们提出了一种用于语义像素分割的新型实用深度全卷积神经网络架构，称为 SegNet。 这种可训练的核心分割引擎由一个编码器网络、一个相应的解码器网络和一个像素分类层组成。编码器网络的架构在拓扑结构上与 VGG16 网络中的 13 个卷积层相同[1]。解码器网络的作用是将低分辨率编码器特征图映射到全输入分辨率特征图，以进行像素分类。SegNet 的新颖之处在于解码">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:03:44.455Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/SegNet/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'SegNet',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">SegNet</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">SegNet</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:03:44.455Z" title="更新于 2024-12-11 01:03:44">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">10.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>31分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="segnet"><a class="markdownIt-Anchor" href="#segnet"></a> SegNet</h1>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>我们提出了一种用于语义像素分割的新型实用深度全卷积神经网络架构，称为 SegNet。 这种可训练的核心分割引擎由一个编码器网络、一个相应的解码器网络和一个像素分类层组成。编码器网络的架构在拓扑结构上与 VGG16 网络中的 13 个卷积层相同[1]。解码器网络的作用是将低分辨率编码器特征图映射到全输入分辨率特征图，以进行像素分类。SegNet 的新颖之处在于解码器对低分辨率输入特征图进行上采样的方式。具体来说，解码器使用在相应编码器的最大池化步骤中计算出的池化指数来执行非线性上采样。这样就无需学习上采样。上采样图是稀疏的，然后与可训练滤波器卷积，生成密集的特征图。我们将提议的架构与广泛采用的 FCN [2] 以及著名的 DeepLab-LargeFOV [3] 和 DeconvNet [4] 架构进行了比较。这种比较揭示了实现良好分割性能所涉及的内存与精度权衡问题。</p>
<p>SegNet 主要受场景理解应用的启发。因此，它的设计在内存和推理过程中的计算时间方面都很高效。与其他同类架构相比，SegNet 的可训练参数数量要少得多，而且可以使用随机梯度下降法进行端到端训练。我们还在道路场景和 SUN RGB-D 室内场景分割任务中对 SegNet 和其他架构进行了对照基准测试。这些定量评估结果表明，与其他架构相比，SegNet 具有良好的性能，推理时间具有竞争力，推理内存效率最高。</p>
<h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1 INTRODUCTION</h2>
<p>语义分割具有广泛的应用范围，从场景理解、推断物体之间的支持关系到自动驾驶。早期依赖低级视觉线索的方法很快就被流行的机器学习算法所取代。特别是，深度学习最近在手写数字识别、语音、整个图像的分类和图像中物体的检测方面取得了巨大成功 [5]，[6]。现在，人们对语义像素标注法[7][8][9][2][4][10][11][12][13][3][14][15][16]产生了浓厚的兴趣。然而，最近的一些方法试图直接采用为类别预测而设计的深度架构来进行像素标注[7]。结果虽然非常令人鼓舞，但却显得粗糙[3]。这主要是因为最大池化和子采样降低了特征图的分辨率。我们设计 SegNet 的动机就来自于这种将低分辨率特征映射到输入分辨率以进行像素分类的需要。这种映射必须产生有助于准确边界定位的特征。</p>
<p>我们的 SegNet 架构旨在成为像素语义分割的高效架构。它主要受道路场景理解应用的启发，这些应用需要对外观（道路、建筑）、形状（汽车、行人）进行建模，并理解不同类别（如道路和人行道）之间的空间关系（上下文）。在典型的道路场景中，大部分像素都属于道路、建筑等大类，因此网络必须能产生平滑的分割。引擎还必须能够根据物体的形状来划分物体，尽管它们的尺寸很小。因此，在提取的图像表征中保留边界信息非常重要。从计算的角度来看，网络在推理过程中的内存和计算时间都必须高效。采用随机梯度下降（SGD）[17] 等有效的权重更新技术对网络中的所有权重进行联合优化，从而实现端到端训练，这也是一种额外的优势，因为它更容易重复。SegNet 的设计就是为了满足这些标准。</p>
<p>SegNet 中的编码器网络在拓扑结构上与 VGG16 [1] 中的卷积层完全相同。我们去掉了 VGG16 中的全连接层，这使得 SegNet 编码器网络比许多其他最新架构更小，更容易训练 [2]、[4]、[11]、[18]。SegNet 的关键部分是解码器网络，它由分层的解码器组成，每个编码器对应一个解码器。其中，相应的解码器使用从相应编码器接收到的最大池化指数对其输入特征图进行非线性上采样。这一想法来自为无监督特征学习设计的架构[19]。在解码过程中重复使用最大池化指数有几个实际优势：(i) 它改善了边界划分，(ii) 它减少了参数数量，实现了端到端的训练，(iii) 这种形式的上采样可以纳入任何编码器-解码器架构，如 [2]、[10]，只需稍加修改。</p>
<p>本文的主要贡献之一是分析了 SegNet 解码技术和广泛使用的全卷积网络（FCN）[2]。这是为了表达设计分割架构时的实际权衡。最近用于分割的深度架构大多采用相同的编码器网络，即 VGG16，但在解码器网络、训练和推理形式上有所不同。另一个共同特点是，它们的可训练参数高达数亿量级，因此在进行端到端训练时会遇到困难[4]。训练这些网络的困难导致了多阶段训练[2]、将网络附加到预训练架构（如 FCN）[10]、使用辅助工具（如推理的区域建议）[4]、分类和分割网络的分离训练[18]以及使用额外的训练数据进行预训练[11][20]或全面训练[10]。此外，性能提升后处理技术[3]也很流行。虽然所有这些因素都提高了在具有挑战性的基准上的性能[21]，但遗憾的是，很难从它们的定量结果中找出实现良好性能所必需的关键设计因素。因此，我们分析了其中一些方法[2]、[4]所使用的解码过程，并揭示了它们的优缺点。</p>
<p>我们评估了 SegNet 在两个场景分割任务上的性能，即 CamVid 道路场景分割 [22] 和 SUN RGB-D 室内场景分割 [23]。多年来，Pascal VOC12 [21] 一直是场景分割的基准挑战。然而，这项任务的大部分内容都是一个或两个前景类，周围是千差万别的背景。这暗中有利于用于检测的技术，正如最近关于解耦分类-分割网络的工作[18]所示，分类网络可以使用大量弱标签数据集进行训练，独立分割网络的性能也得到了提高。文献[3]中的方法还利用分类网络的特征图和独立的 CRF 后处理技术来进行分割。此外，还可以通过使用额外的推理辅助工具（如区域建议[4]、[24]）来提高性能。因此，它不同于场景理解，后者的理念是利用物体和其他空间上下文的共现进行稳健的分割。为了展示 SegNet 的功效，我们提供了一个实时在线演示，将道路场景划分为 11 个自动驾驶感兴趣的类别（见图 1 中的链接）。图 1 显示了在谷歌随机取样的道路场景图像和 SUN RGB-D 数据集[23]的室内测试场景上生成的一些测试结果示例。</p>
<p>本文的其余部分安排如下。在第 2 节中，我们回顾了近期的相关文献。第 3 节介绍 SegNet 架构及其分析。在第 4 节中，我们将评估 SegNet 在室外和室内场景数据集上的性能。随后，我们在第 5 节中对我们的方法进行了一般性讨论，并对未来的工作进行了展望。最后，我们将在第 6 节中进行总结。</p>
<h2 id="2-literature-review"><a class="markdownIt-Anchor" href="#2-literature-review"></a> 2 LITERATURE REVIEW</h2>
<p>具有挑战性的数据集[21]、[22]、[23]、[25]、[26]推动着语义像素分割成为一个活跃的研究课题。在深度网络出现之前，性能最好的方法大多依赖于人工设计的独立像素分类特征。通常情况下，将补丁输入分类器，如随机森林分类器 [27]、[28] 或提升分类器 [29]、[30] 来预测中心像素的类别概率。CamVid 道路场景理解测试 [22] 探索了基于外观 [27] 或 SfM 和外观 [28]、[29]、[30] 的特征。然后使用成对或更高阶的 CRF [29], [30]，对这些来自分类器的每像素噪声预测（通常称为单项）进行平滑处理，以提高准确性。最新的方法旨在通过尝试预测补丁中所有像素的标签，而不是只预测中心像素的标签，来生成高质量的单词。这种方法提高了基于随机森林的单体分类结果[31]，但薄结构类的分类结果较差。从 CamVid 视频中计算出的密集深度图也被用作使用随机森林分类的输入[32]。另一种方法主张结合使用流行的手工设计特征和时空超像素化来获得更高的准确率[33]。在 CamVid 测试中表现最好的技术[30]通过在 CRF 框架中将物体检测输出与分类器预测相结合，解决了标签频率不平衡的问题。所有这些技术的结果都表明，需要改进分类特征。</p>
<p>自纽约大学数据集[25]发布以来，室内 RGBD 像素语义分割也越来越受欢迎。该数据集显示了深度通道对改进分割的有用性。他们的方法使用 RGB-SIFT、深度-SIFT 和像素位置等特征作为神经网络分类器的输入，来预测像素的单值。然后使用 CRF 对有噪声的单元进行平滑处理。为了获得更高的准确率[34]，我们使用了更丰富的特征集（包括 LBP 和区域分割）进行改进，然后再使用 CRF。在最近的研究中[25]，使用基于 RGB 和深度的线索组合，同时推断类别分割和支持关系。另一种方法侧重于实时联合重建和语义分割，使用随机森林作为分类器 [35]。Gupta 等人[36]在进行类别分割之前使用了边界检测和分层分组。所有这些方法的共同点是使用人工设计的特征对 RGB 或 RGBD 图像进行分类。</p>
<p>最近，深度卷积神经网络在物体分类方面的成功使研究人员开始利用其特征学习能力来解决结构化预测问题，如分割。也有人尝试将为物体分类设计的网络应用于分割，特别是通过分块复制最深层特征来匹配图像维度 [7]、[37]、[38]、[39]。然而，这样得到的分类结果是块状的 [38]。另一种方法是使用递归神经网络 [40] 将多个低分辨率预测合并，以创建输入图像分辨率预测。这些技术已经比手工设计的特征有所改进[7]，但它们划分边界的能力较差。</p>
<p>最新的深度架构[2]、[4]、[10]、[13]、[18]特别针对分割而设计，通过学习解码或将低分辨率图像表征映射到像素级预测，推动了先进技术的发展。在所有这些架构中，产生这些低分辨率表示的编码器网络是 VGG16 分类网络[1]，它有 13 个卷积层和 3 个全连接层。该编码器网络的权重通常是在大型 ImageNet 物体分类数据集上预先训练的[41]。解码器网络在这些架构中各不相同，是负责为每个像素生成多维特征以进行分类的部分。</p>
<p>全卷积网络 (FCN) 架构 [2] 中的每个解码器都会学习对其输入特征图进行上采样，并将其与相应的编码器特征图相结合，生成下一个解码器的输入。这种架构的编码器网络有大量可训练参数（1.34 亿），但解码器网络却很小（0.5 亿）。该网络的总体规模较大，因此很难对相关任务进行端到端训练。因此，作者采用了分阶段的训练过程。在此，解码器网络中的每个解码器都被逐步添加到现有的训练网络中。该网络一直发展到无法观察到性能的进一步提高为止。这种增长在三个解码器之后停止，因此忽略高分辨率特征图肯定会导致边缘信息的丢失 [4]。除了与训练相关的问题，在解码器中重复使用编码器特征图的需要也使其在测试时占用大量内存。我们将更详细地研究这种网络，因为它是其他最新架构的核心[10]、[11]。</p>
<p>通过在 FCN 上附加递归神经网络（RNN）[10]，并在大型数据集上对其进行微调[21]、[42]，FCN 的预测性能得到了进一步提高。RNN 层模仿了 CRF 的锐利边界划分能力，同时利用了 FCN 的特征表示能力。研究结果表明，与 FCN-8 相比，RNN 有明显的改进，但同时也表明，当使用更多的训练数据来训练 FCN-8 时，这种差异会减小。当 CRF-RNN 与 FCN8 等架构联合训练时，它的主要优势就显现出来了。最近的其他研究结果[43]、[44]也显示了联合训练的优势。有趣的是，解卷积网络 [4] 的性能明显优于 FCN，但代价是更复杂的训练和推理。不过，这也提出了一个问题，即 CRF-RNN 的优势是否会随着核心前馈分割引擎的改进而减弱。无论如何，CRF-RNN 网络可以附加到包括 SegNet 在内的任何深度分割架构中。</p>
<p>多尺度深度架构也在研究之中 [13], [44]。它们有两种类型：(i) 使用几个尺度的输入图像和相应的深度特征提取网络；(ii) 结合来自单一深度架构不同层的特征图[45] [11]。它们的共同理念是使用在多个尺度上提取的特征来提供局部和全局上下文[46]，并使用早期编码层的特征图来保留更多高频细节，从而形成更清晰的类别边界。由于参数的大小，其中一些架构很难训练[13]。因此，需要采用多阶段训练过程和数据增强。此外，采用多个卷积路径进行特征提取的推理成本也很高。还有一些研究 [44] 将 CRF 附加到多尺度网络中，并对它们进行联合训练。不过，这些方法在测试时不是前馈的，需要优化才能确定 MAP 标签。</p>
<p>最近提出的几种用于分割的深度架构在推理时也不是前馈的 [4]、[3]、[18]。它们需要在 CRF 上进行 MAP 推断[44]、[43]，或使用区域建议[4]等辅助工具进行推理。我们认为，使用 CRF 可以提高性能，这是因为它们的核心前馈分割引擎缺乏良好的解码技术。另一方面，SegNet 使用解码器来获取精确像素分类的特征。</p>
<p>最近提出的解卷积网络[4]及其半监督变体解耦网络[18]使用编码器特征图的最大位置（池化指数）在解码器网络中执行非线性上采样。这些架构的作者在 SegNet（首次提交给 CVPR 2015 [12]）之外，提出了在解码器网络中解码的这一想法。然而，他们的编码器网络由 VGG-16 网络的全连接层组成，而 VGG-16 网络的参数约占整个网络参数的 90%。这使得他们的网络训练非常困难，因此需要额外的辅助工具，如使用区域建议来进行训练。此外，在推理过程中还需要使用这些建议，这大大增加了推理时间。从基准测试的角度来看，这也使得在没有其他辅助工具的情况下，很难评估其架构（编码器-解码器网络）的性能。在这项工作中，我们舍弃了 VGG16 编码器网络的全连接层，这使我们能够使用相关的训练集，通过 SGD 优化来训练网络。另一种最新方法[3]显示了在不牺牲性能、减少内存消耗和缩短推理时间的情况下大幅减少参数数量的好处。</p>
<p>我们的工作受到了 Ranzato 等人提出的无监督特征学习架构的启发[19]。关键的学习模块是一个编码器-解码器网络。编码器包括与滤波器组的卷积、元素向 tanh 非线性、最大池化和子采样，以获得特征图。对于每个样本，池化过程中计算出的最大位置的索引会被存储并传递给解码器。解码器利用存储的池化指数对特征图进行上采样。解码器使用可训练的解码器滤波器组对上采样图进行卷积，以重建输入图像。这种架构用于无监督的分类预训练。类似的解码技术也被用于可视化训练卷积网络的分类[47]。Ranzato 等人的架构主要集中在使用小的输入片段进行分层特征学习。Kavukcuoglu 等人[48] 将其扩展到接受全尺寸图像作为输入来学习分层编码器。不过，这两种方法都没有尝试使用深度编码器-解码器网络进行无监督特征训练，因为它们在每次编码器训练后都会丢弃解码器。在这里，SegNet 与这些架构不同，因为深度编码器-解码器网络是为监督学习任务而联合训练的，因此解码器在测试时是网络不可分割的一部分。</p>
<p>使用深度网络进行像素预测的其他应用包括图像超分辨率[49]和单幅图像的深度图预测[50]。作者在 [50] 中讨论了学习从低分辨率特征图进行上采样的必要性，这也是本文的核心主题。</p>
<h2 id="3-architecture"><a class="markdownIt-Anchor" href="#3-architecture"></a> 3 ARCHITECTURE</h2>
<p><img src="/img/loading.gif" data-original="images/image-20240124165810638.png" alt="image-20240124165810638" /></p>
<blockquote>
<p>图 2. SegNet 架构示意图。没有全连接层，因此只有卷积层。解码器使用编码器传输的池指数对输入进行上采样，生成稀疏的特征图。然后，解码器使用可训练的滤波器组进行卷积，使特征图更加密集。解码器输出的最终特征图被送入软最大分类器进行像素分类。</p>
</blockquote>
<p>SegNet 有一个编码器网络和一个相应的解码器网络，最后是一个像素分类层。这一架构如图 2 所示。编码器网络由 13 个卷积层组成，这些卷积层与为物体分类设计的 VGG16 网络[1]中的前 13 个卷积层相对应。因此，我们可以根据在大型数据集上进行分类时训练的权重来初始化训练过程[40]。我们还可以舍弃全连接层，在最深的编码器输出端保留分辨率更高的特征图。这也大大减少了 SegNet 编码器网络中的参数数量（从 134 个减少到 1470 万个），与其他最新的架构[2]、[4]相比（见表 6）。每个编码器层都有一个相应的解码器层，因此解码器网络有 13 层。解码器的最终输出被送入一个多类软最大分类器，为每个像素独立生成类别概率。</p>
<p>编码器网络中的每个编码器都与滤波器组进行卷积，生成一组特征图。然后对这些特征图进行批量归一化处理 [51], [52]）。然后应用元素整流线性非线性（ReLU）max(0, x)。然后，使用 2 × 2 窗口和跨距 2（非重叠窗口）进行最大池化处理，并对得到的输出进行 2 倍的子采样。子采样的结果是为特征图中的每个像素提供一个大的输入图像上下文（空间窗口）。虽然多层最大池化和子采样可以获得更多的平移不变性，从而实现稳健的分类，但相应地也会损失特征图的空间分辨率。越来越多的有损（边界细节）图像表示法不利于对边界划分至关重要的分割。因此，在进行子采样之前，有必要在编码器特征图中捕捉并存储边界信息。如果推理过程中的内存不受限制，则可以存储所有编码器特征图（子采样后）。在实际应用中通常不会出现这种情况，因此我们提出了一种更有效的方法来存储这些信息。这种方法只需存储最大池化指数，即为每个编码器特征图记忆每个池化窗口中最大特征值的位置。原则上，每个 2 × 2 汇集窗口只需使用 2 位即可完成，因此与记忆浮点精度的特征图相比，存储效率要高得多。正如我们在后面的工作中所展示的，这种较低的内存存储会导致精度略有下降，但仍然适用于实际应用。</p>
<p><img src="/img/loading.gif" data-original="images/image-20240124165621195-1706087078879-1.png" alt="image-20240124165621195" /></p>
<blockquote>
<p>图 3. a、b、c、d 对应于特征图中的值。SegNet 使用最大池化指数对特征图进行上采样（无需学习），并与可训练的解码器滤波器组进行卷积。FCN 通过学习对输入特征图进行解卷积来进行上采样，并添加相应的编码器特征图来生成解码器输出。该特征图是相应编码器中最大池化层（包括子采样）的输出。请注意，FCN 中没有可训练的解码器滤波器。</p>
</blockquote>
<p>解码器网络中的相应解码器会使用从相应编码器特征图中记忆的最大池化指数对其输入特征图进行上采样。这一步骤产生稀疏特征图。图 3 展示了这种 SegNet 解码技术。然后，这些特征图与可训练解码器滤波器组进行卷积，生成密集特征图。然后对每个特征图进行批量归一化处理。请注意，与第一个编码器（最接近输入图像）相对应的解码器生成的是多通道特征图，尽管其编码器输入只有 3 个通道（RGB）。这与网络中的其他解码器不同，它们生成的特征图的大小和通道数与其编码器输入相同。最终解码器输出端的高维特征表示被输入到一个可训练的软最大分类器中。这个软最大分类器对每个像素进行独立分类。软最大分类器的输出是一个 K 通道的概率图像，其中 K 是类别的数量。预测的分割对应于每个像素上概率最大的类别。</p>
<p>我们在此补充说明，另外两种架构，即 DeconvNet [53] 和 U-Net [16]，与 SegNet 架构相似，但有一些不同之处。DeconvNet 的参数化规模更大，需要的计算资源更多，端到端的训练难度更高（表 6），这主要是由于它使用了全连接层（尽管是卷积方式）。</p>
<p>与 SegNet 相比，U-Net [16]（针对医学成像领域提出）没有重复使用池化指数，而是将整个特征图（以增加内存为代价）传输到相应的解码器，并将它们连接到上采样（通过解卷积）解码器特征图。U-Net 中没有 VGG 网架构中的 conv5 和 max-pool 5 块。另一方面，SegNet 使用 VGG 网络中所有预训练的卷积层权重作为预训练权重。</p>
<h3 id="31-decoder-variants"><a class="markdownIt-Anchor" href="#31-decoder-variants"></a> 3.1 Decoder Variants</h3>
<p>许多分割架构 [2]、[3]、[4] 都使用相同的编码器网络，它们只是在解码器网络的形式上有所不同。我们选择将 SegNet 解码技术与广泛使用的全卷积网络（FCN）解码技术[2], [10]进行比较。</p>
<p>为了分析 SegNet 并将其性能与 FCN（解码器变体）进行比较，我们使用了一个较小版本的 SegNet，称为 SegNet-Basic 1，它有 4 个编码器和 4 个解码器。SegNet-Basic 中的所有编码器都执行最大池化和下采样，相应的解码器使用接收到的最大池化指数对输入进行上采样。编码器和解码器网络的每个卷积层之后都使用了批量归一化。卷积后不使用偏置，解码器网络中也不存在 ReLU 非线性。此外，所有编码器层和解码器层的内核大小恒定为 7 × 7，以便为平滑标记提供广泛的背景，即最深层特征图（第 4 层）中的一个像素可以追溯到输入图像中 106×106 像素的背景窗口。SegNet-Basic 的这种小尺寸允许我们探索许多不同的变体（解码器），并在合理的时间内对它们进行训练。同样，我们创建了 FCN-Basic，这是 FCN 的一个可比版本，用于我们的分析，它与 SegNet-Basic 使用相同的编码器网络，但其所有解码器都使用了 FCN 解码技术（见图 3）。</p>
<p>图 3 左侧是 SegNet（也是 SegNet-Basic）使用的解码技术，其中的上采样步骤不涉及学习。不过，上采样图会与可训练的多通道解码器滤波器进行卷积，以密集其稀疏输入。每个解码器滤波器的通道数与上采样特征图的通道数相同。一个较小的变体是解码器滤波器是单通道的，即它们只对相应的上采样特征图进行卷积。这种变体（SegNetBasic-SingleChannelDecoder）大大减少了可训练参数的数量和推理时间。</p>
<p>图 3 右侧是 FCN（也称 FCN-Basic）解码技术。FCN 模型的重要设计元素是编码器特征图的降维步骤。这可以压缩编码器特征图，然后将其用于相应的解码器。编码器特征图（例如 64 个通道）的降维是通过与 1 × 1 × 64 × K 可训练滤波器卷积来完成的，其中 K 是类别数。压缩后的 K 通道最终编码器层特征图是解码器网络的输入。在该网络的解码器中，通过使用固定或可训练的多通道上采样核进行反卷积来进行上采样。我们将核大小设置为 8 × 8。这种上采样方式也被称为解卷积。请注意，与 SegNet 相比，使用可训练解码器滤波器的多通道卷积是在上采样后进行的，以便对特征图进行致密化处理。FCN 中的上采样特征图有 K 个通道。然后将其逐个添加到相应分辨率的编码器特征图中，生成输出解码器特征图。上采样核使用双线性插值权重初始化[2]。</p>
<p>FCN 解码器模型需要在推理过程中存储编码器特征图。对于嵌入式应用来说，这可能会占用大量内存；例如，以 180 × 240 分辨率、32 位浮点精度存储 FCN-Basic 第一层的 64 个特征图需要 11MB。使用降维技术可将存储空间缩小到 11 个特征图，这需要的存储空间≈ 1.9MB。另一方面，SegNet 的池化索引所需的存储成本几乎可以忽略不计（如果每个 2 × 2 池化窗口使用 2 位存储，则存储空间为 0.17MB）。我们还可以创建 FCN-Basic 模型的变体，放弃编码器特征图添加步骤，只学习上采样核（FCN-Basic-NoAddition）。</p>
<p>除上述变体外，我们还研究了使用固定双线性插值权重的上采样，因此无需学习上采样（双线性插值）。在另一个极端，我们可以在 SegNet 解码器的相应输出特征图的每一层添加 64 个编码器特征图，从而创建一个更耗费内存的 SegNet 变体（SegNet-BasicEncoderAddition）。这里使用了用于上采样的池化指数，然后使用卷积步骤对稀疏输入进行增密。然后将其按元素顺序添加到相应的编码器特征图中，生成解码器输出。</p>
<p>另一种更耗费内存的 FCN-Basic 变体（FCN-Basic-NoDimReduction）不对编码器特征图进行降维处理。这意味着与 FCN-Basic 不同的是，最终的编码器特征图在传递给解码器网络之前不会被压缩到 K 个通道。因此，每个解码器末端的通道数与相应编码器的通道数相同（即 64 个）。</p>
<p>我们还尝试了其他通用变体，即通过复制[7]对特征图进行简单的上采样，或使用固定（稀疏）的索引数组进行上采样。与上述变体相比，这些变体的性能相当差。没有最大池化和编码器网络子采样的变体（解码器是冗余的）会消耗更多内存，收敛时间更长，性能也更差。最后，请注意，为了鼓励复制我们的结果，我们发布了所有变体的 Caffe 实现 2。</p>
<h3 id="32-training"><a class="markdownIt-Anchor" href="#32-training"></a> 3.2 Training</h3>
<p>我们使用 CamVid 道路场景数据集来衡量解码器变体的性能。这个数据集很小，由 367 张训练用 RGB 图像和 233 张测试用 RGB 图像（白天和黄昏场景）组成，分辨率为 360 × 480。挑战在于分割 11 个类别，如道路、建筑、汽车、行人、标志、电线杆、人行道等。我们对 RGB 输入进行了局部对比度归一化处理 [54]。</p>
<p>编码器和解码器权重均采用 He 等人[55]中描述的技术进行初始化。为了训练所有变体，我们使用随机梯度下降法（SGD），学习率固定为 0.1，动量为 0.9 [17]，并使用我们的 SegNet-Basic Caffe 实现 [56]。我们对变体进行训练，直到训练损失收敛为止。每次训练前，我们都会对训练集进行洗牌，然后按顺序挑选每个迷你批次（12 幅图像），从而确保每幅图像在一次训练中只使用一次。我们选择在验证数据集上表现最好的模型。</p>
<p>我们使用交叉熵损失 [2] 作为训练网络的目标函数。损失是一个迷你批次中所有像素的总和。当训练集中每个类别的像素数量差异较大时（例如，CamVid 数据集中主要是道路、天空和建筑像素），就需要根据真实类别对损失进行不同的加权。这就是所谓的类别平衡。我们使用频率中值平衡法[13]，在损失函数中分配给一个类别的权重是在整个训练集上计算的类别频率中值除以类别频率的比值。这意味着训练集中较大类的权重小于 1，而最小类的权重最大。我们还尝试了在不进行类平衡或等效使用自然频率平衡的情况下训练不同的变体。</p>
<h3 id="33-analysis"><a class="markdownIt-Anchor" href="#33-analysis"></a> 3.3 Analysis</h3>
<p>为了比较不同解码器变体的定量性能，我们使用了三种常用的性能指标：全局准确度 (G)，用于衡量数据集中正确分类像素的百分比；类平均准确度 ©，即所有类的预测准确度的平均值；以及所有类的平均交集大于联合 (mIoU)，如 Pascal VOC12 挑战赛[21]中使用的指标。mIoU 指标比类平均准确率更严格，因为它惩罚假阳性预测。不过，mIoU 指标不能直接通过类平衡交叉熵损失进行优化。</p>
<p>mIoU 指标又称 Jacard 指数，是基准测试中最常用的指标。然而，Csurka 等人[57]指出，这一指标并不总是与人类对优质分割的定性判断（等级）相对应。他们通过实例表明，mIoU 更倾向于区域的平滑度，而不能评估边界的准确性，FCN [58] 的作者最近也提到了这一点。因此，他们建议使用基于伯克利轮廓匹配得分的边界测量方法来补充 mIoU 指标，该方法通常用于评估无监督图像分割质量[59]。Csurka 等人[57] 简单地将其扩展到了语义分割，并表明与 mIoU 指标结合使用的语义轮廓准确度指标更符合人类对分割输出结果的排序。</p>
<p>计算语义轮廓分数的关键思路是评估 F1 测量[59]，这涉及到在给定像素容差距离的情况下，计算预测类别边界和地面真实类别边界之间的精确度和召回值。我们使用图像对角线的 0.75% 作为容差距离。对地面实况测试图像中出现的每个类别的 F1measure 值进行平均，得出图像的 F1measure 值。然后，我们通过平均图像 F1 测量值来计算整个测试集的平均值，即边界 F1 测量值（BF）。</p>
<p>在 CamVid 验证集上每迭代 1000 次优化后，我们会测试每个架构变体，直到训练损失收敛为止。由于训练小批量的大小为 12，这相当于大约每 33 个历时（遍数）通过训练集进行一次测试。我们在验证集的评估中选择全局准确率最高的迭代。此时，我们将在保留的 CamVid 测试集上报告所有三项性能指标。虽然我们在训练变体时使用了类平衡，但实现较高的全局准确率对于实现整体平滑的分割仍然非常重要。另一个原因是，分割对自动驾驶的贡献主要在于划分道路、建筑物、人行道、天空等类别。这些类别占据了图像中的大部分像素，而高的全局精确度则对应于对这些重要类别的良好分割。我们还观察到，当类别平均值最高时，报告的数值性能往往与低总体准确率相对应，表明分割输出的感知噪声较大。</p>
<p>在表 1 中，我们报告了分析的数值结果。我们还显示了可训练参数和最高分辨率特征图或池化指数存储内存的大小，即最大池化和子采样后第一层特征图的大小。我们展示的是 Caffe 实现一次前向传递的平均时间，该时间是在配备 cuDNN v3 加速的英伟达 Titan GPU 上使用 360 × 480 输入进行的 50 次测量的平均值。我们注意到，SegNet 变体中的上采样层并未使用 cuDNN 加速进行优化。我们显示了所有变体在选定迭代时的测试和训练结果。此外，我们还列出了不进行类平衡（固有频率）时的训练和测试精确度结果。下面我们将分析类平衡的结果。</p>
<p>从表 1 中我们可以看出，基于双线性插值法的上采样方法在没有任何学习的情况下，在所有精度指标上表现最差。所有其他方法，无论是使用学习进行上采样（FCN-Basic 及其变体），还是在上采样后学习解码器滤波器（SegNet-Basic 及其变体），其性能都要好得多。这强调了学习解码器进行分割的必要性。其他作者在比较 FCN 和 SegNet 类型解码技术时收集的实验证据也证明了这一点[4]。</p>
<p>当我们比较 SegNet-Basic 和 FCN-Basic 时，我们发现两者在这项测试中的所有准确度指标上都表现相当。两者的区别在于，SegNet 在推理过程中使用的内存更少，因为它只存储最大池化索引。另一方面，FCN-Basic 会完整地存储编码器特征图，这会消耗更多内存（11 倍）。SegNet-Basic 有一个解码器，每个解码器层有 64 个特征图。相比之下，FCN-Basic 采用了降维技术，每个解码器层中的特征图数量更少（11 个）。这减少了解码器网络中的卷积次数，因此 FCN-Basic 在推理（前向传递）过程中速度更快。从另一个角度看，SegNet-Basic 中的解码器网络使其整体规模大于 FCN-Basic。这赋予了它更大的灵活性，因此在迭代次数相同的情况下，它比 FCN-Basic 获得了更高的训练精度。总之，我们看到，在推理时间内存受限，但推理时间在一定程度上可以妥协的情况下，SegNet-Basic 比 FCN-Basic 更有优势。</p>
<p>就解码器而言，SegNet-Basic 与 FCN-Basic-NoAddition 最为相似，尽管 SegNet 的解码器更大。两者都通过学习生成密集的特征图，要么像 FCN-Basic-NoAddition 那样直接学习进行解卷积，要么先进行上采样，然后用训练有素的解码器滤波器进行卷积。SegNet-Basic 的性能更优越，部分原因是其解码器规模更大。FCN-Basic-NoAddition 的准确度也低于 FCN-Basic。这表明，为了获得更好的性能，捕捉编码器特征图中的信息至关重要。特别要注意的是，这两个变体之间的 BF 测量值下降幅度很大。这也是 SegNet-Basic 性能优于 FCN-BasicNoAddition 的部分原因。</p>
<p>FCN-Basic-NoAddition-NoDimReduction 模型的大小略大于 SegNet-Basic，因为最终编码器特征图并没有压缩到与类 K 的数量相匹配。在测试中，该 FCN 变体的性能比 SegNet-Basic 差，但在相同的训练历元数下，其训练精度也更低。这表明，仅使用较大的解码器是不够的，还必须捕捉编码器特征图信息，特别是细粒度轮廓信息，以便更好地学习（注意 BF 测量值的下降）。同样有趣的是，与 FCNBasic-NoDimReduction 等更大的模型相比，SegNet-Basic 的训练准确率更有竞争力。</p>
<p>FCN-BasicNoAddition 和 SegNet-Basic-SingleChannelDecoder 之间另一个有趣的比较表明，使用最大池化指数进行上采样和使用更大的解码器会带来更好的性能。这也证明 SegNet 是一种很好的分割架构，特别是当需要在存储成本、准确性和推理时间之间找到折衷方案时。在内存和推理时间都不受限制的最佳情况下，FCN-Basic-NoDimReduction 和 SegNet-EncoderAddition 等较大的模型都比其他变体更准确。特别是，在 FCN-Basic 模型中摒弃降维导致 FCN-Basic 变体中性能最佳，BF 分数较高。这再次强调了分割架构在内存和准确性之间的权衡。</p>
<p>表 1 的最后两列显示了不使用类平衡（固有频率）时的结果。在这里，我们可以看到，在不加权的情况下，所有变体的结果都较差，尤其是类平均准确率和 mIoU 指标。在没有加权的情况下，全局精度是最高的，因为大部分场景都是由天空、道路和建筑物像素构成的。除此之外，变体比较分析的所有推论也适用于自然频率平衡，包括 BF 指标的趋势。SegNetBasic 和 FCN-Basic 的表现一样好，而且优于更大的 FCN-Basic-NoAddition-NoDimReduction。规模较大但效率较低的 FCN-Basic-NoDimReduction 和 SegNetEncoderAddition 的表现优于其他变体。</p>
<p>现在，我们可以用以下几点概括上述分析。</p>
<p>当编码器特征图全部存储时，性能最佳。这一点在语义轮廓划分指标（BF）中体现得最为明显。</p>
<p>当推理过程中内存受限时，可以存储编码器特征图的压缩形式（降维、最大池化指数），并与适当的解码器（如 SegNet 类型）配合使用，以提高性能。</p>
<p>对于给定的编码器网络，较大的解码器可提高性能。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/SAN/" title="SAN"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">SAN</div></div><div class="info-2"><div class="info-item-1"> SAN  Abstract 本文提出了一种利用预先训练好的视觉语言模型（名为 “侧适配器网络”（SAN））进行开放词汇语义分割的新框架。我们的方法将语义分割任务建模为区域识别问题。一个侧边网络连接到一个冻结的 CLIP 模型，该模型有两个分支：一个用于预测mask建议，另一个用于预测注意力偏差，该注意力偏差应用于 CLIP 模型，以识别mask类别。这种解耦设计有利于 CLIP 识别mask建议类别。由于附加侧网络可以重复使用 CLIP 特征，因此可以非常轻便。此外，整个网络可以进行端到端训练，使侧网络适应冻结的 CLIP 模型，从而使预测的mask建议具有 CLIP 感知。我们的方法快速、准确，而且只增加了几个额外的可训练参数。我们在多个语义分割基准上评估了我们的方法。我们的方法明显优于其他同类方法，可训练参数减少了 18 倍，推理速度提高了 19 倍。图 1 显示了 ImageNet 上的一些可视化结果。我们希望我们的方法能成为一个坚实的基线，并为未来的开放词汇语义分割研究提供帮助。              图 1.ImageNet...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Squeeze-and-Excitation-Networks/" title="Squeeze-and-Excitation Networks"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Squeeze-and-Excitation Networks</div></div><div class="info-2"><div class="info-item-1"> Squeeze-and-Excitation Networks  Abstract 卷积神经网络是建立在卷积操作之上的，它通过在局部感受野内将空间和通道信息融合在一起来提取信息特征。为了提高网络的表示能力，最近的一些方法显示了加强空间编码的好处。在这项工作中，我们专注于通道关系，并提出了一个新的结构单元，我们称之为 “挤压和激励”（SE）块，它通过明确地模拟通道之间的相互依赖性来适应性地重新校准通道方面的特征反应。我们证明，通过将这些模块堆叠在一起，我们可以构建SENet架构，该架构在具有挑战性的数据集上具有非常好的通用性。最重要的是，我们发现SE块对现有的最先进的深度架构产生了明显的性能改进，而额外的计算成本却很小。SENets构成了我们2017年ILSVRC分类提交的基础，它赢得了第一名，并将前五名的误差大大降低到2.251%，比2016年的获奖作品实现了25%的相对改进。  1. Introduction 卷积神经网络（CNN）已被证明是处理各种视觉任务的有效模型[21, 27, 33,...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BAM</div></div><div class="info-2"><div class="info-item-1"> BAM  Abstract 近来，小样本分割分割技术（FSS）得到了广泛的发展。然而，训练出来的模型偏向于所看到的类别，而不是理想的类别无关性，从而阻碍了对新内容的识别。本文提出了一种新颖而直接的见解来缓解这一问题。具体来说，我们在传统的 FSS 模型（元学习器）上增加了一个分支（基础学习器），以明确识别基础类别的目标，即不需要分割的区域。然后，对这两个学习器并行输出的粗略结果进行自适应整合，从而得出精确的分割预测结果。考虑到元学习器的敏感性，我们进一步引入了一个调整因子来估计输入图像对之间的场景差异，以促进模型的集合预测。在 PASCAL-5i 和 COCO-20i 上取得的显著性能提升验证了这一方法的有效性，而且令人惊讶的是，即使使用两个普通学习器，我们的多功能方案也创造了新的一流水平。此外，鉴于所提方法的独特性，我们还将其扩展到了更现实但更具挑战性的环境中，即广义 FSS，在这种环境中，基础类和新类别的像素都需要确定。  1. Introduction   得益于成熟的大规模数据集 [8, 9,...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#segnet"><span class="toc-text"> SegNet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-introduction"><span class="toc-text"> 1 INTRODUCTION</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-literature-review"><span class="toc-text"> 2 LITERATURE REVIEW</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-architecture"><span class="toc-text"> 3 ARCHITECTURE</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-decoder-variants"><span class="toc-text"> 3.1 Decoder Variants</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-training"><span class="toc-text"> 3.2 Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-analysis"><span class="toc-text"> 3.3 Analysis</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/11/%E5%B0%8F%E7%B1%B3/syzkaller/syzkaller01-%E9%85%8D%E7%BD%AE/" title="syzkaller01-配置">syzkaller01-配置</a><time datetime="2024-12-10T17:10:45.000Z" title="发表于 2024-12-11 01:10:45">2024-12-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM">BAM</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>