<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>CANet | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning (Chi Zhang 等。, 2019)  摘要 深度卷积神经网络和大规模标记的图像数据集推动了语义分割的最新进展。然而，用于像素分割的数据标签是繁琐而昂贵的。此外，一个训练有素的模型只能在一组">
<meta property="og:type" content="article">
<meta property="og:title" content="CANet">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CANet/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning (Chi Zhang 等。, 2019)  摘要 深度卷积神经网络和大规模标记的图像数据集推动了语义分割的最新进展。然而，用于像素分割的数据标签是繁琐而昂贵的。此外，一个训练有素的模型只能在一组">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:04:03.363Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CANet/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CANet',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">CANet</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">CANet</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:04:03.363Z" title="更新于 2024-12-11 01:04:03">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">7.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>24分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning"><a class="markdownIt-Anchor" href="#canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning"></a> CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning</h1>
<p>(<a href="zotero://select/library/items/3AR39Q4N">Chi Zhang 等。, 2019</a>)</p>
<h2 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h2>
<p>深度卷积神经网络和大规模标记的图像数据集推动了语义分割的最新进展。然而，用于像素分割的数据标签是繁琐而昂贵的。此外，一个训练有素的模型只能在一组预先定义的类别中进行预测。在本文中，我们提出了CANet，一个与类别无关的分割网络，在只有小样本注释图像的情况下对新的类别进行小样本的分割。我们的网络由一个双分支密集比较模块和一个迭代优化模块组成，前者在支持图像和查询图像之间进行多层次的特征比较，后者对预测结果进行迭代改进。此外，我们还引入了一种注意力机制，在k-shot学习的设定下，有效地融合来自多个支持实例的信息。在PASCAL VOC 2012上的实验表明，我们的方法在1张照片分割和5张照片分割中分别取得了55.4%和57.1%的平均交叉-联合得分，比最先进的方法分别高出14.6%和13.2%的幅度。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221024162858165.png" alt="img" /></p>
<blockquote>
<p>图1：我们提出的用于单张照片分割的网络概述。我们的框架包括一个密集比较模块（DCM）和一个迭代优化模块（IOM）。只需给定一张有注释的训练图像，我们的网络就能用新的类别对测试图像进行分割，并迭代优化结果。</p>
</blockquote>
<h2 id="1-介绍"><a class="markdownIt-Anchor" href="#1-介绍"></a> 1、介绍</h2>
<p>深度卷积神经网络在许多视觉理解任务中取得了重大突破，包括图像分类[13, 9, 30]，物体检测[27, 8, 26]，和语义分割[16, 2, 20]。一个关键的原因是大规模数据集的可用性，如ImageNet[4]，使深度模型的训练成为可能。然而，数据标记是昂贵的，特别是对于密集的预测任务，如语义分割和实例分割。与机器学习算法相比，人类在只看到几个例子时就能轻易地从图像中分割出一个新概念。人类和机器学习算法之间的差距促使人们研究小样本的学习，其目的是学习一个模型，该模型可以用稀少的标记训练数据很好地归纳到新的类别。</p>
<p>==在本文中，我们承担了小样本语义分割的任务，即只使用少数有注释的训练图像来对新类别进行分割。以前关于这个任务的工作[29, 24, 5]遵循双分支结构的设计，包括一个支持分支和一个查询分支。支持分支旨在从支持集中提取信息以指导查询分支的分割工作。在我们的框架中，我们也采用了双分支的设计，以解决少数片段分割的问题。==</p>
<p>==我们的网络包括一个双分支的密集比较模块，其中一个共享的特征提取器从查询集和支持集中提取表征用于比较。密集比较模块的设计从图像分类任务的度量学习[37, 31]中得到启发，其中距离函数评估了图像之间的相似度。然而，与每个图像都有一个标签的图像分类不同，图像分割需要对具有结构化表示的数据进行预测。直接将公因子学习应用于密集的预测问题是很困难的。为了解决这个问题，一个直接的方法是在所有的像素对之间进行比较。然而，一幅图像中有数百万个像素，对所有像素对进行比较需要巨大的计算成本。相反，我们的目标是从支持图像中获得一个全局的表示，以便进行比较。在这里，为了只关注指定的类别，我们使用前景区域的全局平均池化来过滤掉不相关的信息。然后将全局特征与查询分支中的每个位置进行比较，这可以被看作是一种密集形式的度量学习方法。==</p>
<p>在小样本的设置下，网络应该能够处理在训练期间从未见过的新类别。因此，我们旨在从CNN中挖掘可转移的表征进行比较。正如在特征可视化文献[39, 38]中所观察到的，较低层的特征与低层次的线索有关，例如边缘和颜色，而较高层的特征则与物体层次的概念有关，例如类别。我们关注的是中间层的特征，这些特征可能构成未见过的类别所共享的对象部分。例如，如果CNN在对汽车类进行训练时学习了与车轮有关的特征，那么这种特征也可能对新的车辆类（如卡车和公共汽车）的特征比较有用。我们在CNN中提取多层次的表征来进行密集的比较。</p>
<p>由于同一类别中存在着外观上的差异，同一类别的物体可能只具有少数相似的特征。密集的特征比较并不足以指导整个物体区域的分割。尽管如此，这也提供了一个重要的线索，说明物体的位置。在半自动分割的文献中，给出了对类别无关的分割的弱注释，例如，用点击或涂鸦注释的交互式分割[36，14]和用边界盒或极点先验的实例分割[10，21]。定位物体区域的可转移知识是在训练过程中学习的。==受半自动分割任务的启发，我们希望通过密集的比较结果作为先验因素，逐步将物体与背景区分开来。我们提出了一个<strong>迭代优化模块</strong>（IOM），它可以学习迭代地完善预测结果。完善是以循环的形式进行的，密集比较的结果和预测的掩码被发送到一个IOM进行优化，输出结果被循环地发送到下一个IOM。经过几次迭代完善，我们的密集比较模块能够生成细粒度的分割图。在每个IOM内部，我们采用剩余连接来有效地将预测的掩码纳入最后的迭代步骤中。图1显示了我们的一次性分割网络的概况。==</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210241500066.png" alt="图1" /></p>
<p>以前的k-shot分割方法是基于1-shot模型的。他们使用不可学习的融合方法来融合单个的1张照片的结果，例如，平均1张照片的预测或中间特征。相反，我们采用一种注意力机制来有效地融合来自多个支持实例的信息。</p>
<p>为了进一步减少小样本分割的标注工作，我们探索了一个新的测试环境：我们的模型使用边界盒注释的支持集来执行查询图像的分割。我们在PASCAL VOC 2012数据集和COCO数据集上进行了综合实验，以验证我们网络的有效性。本文的主要贡献总结如下。</p>
<ul>
<li>我们开发了一个新颖的双分支密集比较模块，它有效地利用了CNN的多层次特征表示，进行密集的特征比较。</li>
<li>我们提出了一个迭代优化模块，以迭代的方式完善预测结果。迭代细化的能力可以被推广到未见过的类中，用小样本的学习来生成精细的图。</li>
<li><strong>我们采用了一种注意力机制，在kshot设置中有效地融合了来自多个支持实例的信息，其性能优于1-shot结果的不可学习的融合方法。</strong></li>
<li>我们证明了使用弱注释的支持集，即边界框，我们的模型仍然可以达到与昂贵的像素级注释支持集的结果相媲美的性能，这进一步大大减少了对小样本分割的新类的标注工作。</li>
<li>在PASCAL VOC 2012数据集上的实验表明，我们的方法在1张照片分割和5张照片分割上分别取得了55.4%和57.1%的miou联合得分，分别比最先进的结果高出14.6%和13.2%。</li>
</ul>
<h2 id="2-相关工作"><a class="markdownIt-Anchor" href="#2-相关工作"></a> 2、相关工作</h2>
<p><strong>小样本学习</strong></p>
<p>小样本学习的目的是学习可转移的知识，这些知识可以用稀少的标记训练数据概括到新的类别。目前有许多关于小样本分类的表述，包括带记忆的递归神经网络[28, 23]，学习微调模型[6, 25]，网络参数预测[1, 35]，以及公因子学习[31, 37, 11]。基于度量学习的方法在少许分类任务中取得了最先进的性能，它们具有快速和以前馈方式进行预测的特点。我们的工作与关系网络[37]最为相关。关系网络元学习了一个深度距离指标来比较图像，并计算出分类的相似度分数。该网络由一个生成图像表示的嵌入模块和一个比较嵌入并输出相似度分数的关系模块组成。这两个模块都是卷积运算的形式。我们网络中的密集比较模块可以被看作是关系网络在密集形式下的扩展，以解决分割的任务。</p>
<p><strong>小样本语义分割</strong></p>
<p>完全监督下的语义分割是将图像中的每个像素分类到一组预先定义的类别中的任务[16, 2, 20, 15, 17]。另一方面，小样本语义分割的目的是将分割能力推广到任何新的类别，只用少数注释的例子。以前关于小样本语义分割的工作采用了双分支结构。Shaban等人[29]首先在语义分割上采用了小样本学习。支持分支直接预测查询分支中最后一层的权重来进行分割。在[24]中，支持分支产生了一个嵌入，它被融合到查询分支中作为额外的特征。我们的网络也采用了双分支设计。然而，与以往工作中两个分支有不同的结构不同，我们网络中的两个分支共享同一个骨干网络。以前的方法中的模型集中在1-shot设置上，当把1-shot扩展到k-shot时，他们对每个支持实例独立应用1-shot方法，并使用不可学习的融合方法在图像层面或特征层面融合单个预测结果。例如，Shaban等人[29]提出使用逻辑OR操作来融合单个预测掩码，Rakelly等人[24]对不同支持例产生的支持分支中的嵌入进行平均。相反，我们采用了一种通过注意力机制的可学习方法来有效地融合来自多个支持实例的信息。</p>
<h2 id="3-任务描述"><a class="markdownIt-Anchor" href="#3-任务描述"></a> 3、任务描述</h2>
<p>==假设我们的模型是在一个有类集Ctrain的数据集上训练的，我们的目标是用训练好的模型在一个有新类Ctest的不同数据集上进行预测，在这个数据集上只有少数有注释的例子。直观地说，我们训练模型的能力是，对于一个新的类c 6∈Ctrain，我们的模型能够在只看到该类的几张图片时将该类从图片中分割出来。一旦模型训练完成，参数就固定了，在新的数据集上测试时不需要优化。==</p>
<p>我们将训练和测试与e范式[33]相统一，以处理小样本的情况。具体来说，给定一个k-shot学习任务，每个情节都是通过采样来构建的：1）支持（训练）集S = {(xis, yi s©)}ik =1，其中xis∈RHi×Wi×3是一个RGB图像，yi s© ∈RHi×Wi是支持图像中c类的二进制掩码。2）查询（测试）集Q = {xq, yq©}，其中xq是查询图像，yq©是查询图像中c类的基础真实掩码。该模型的输入是支持集S和查询图像xq，输出是查询图像中c类的预测掩码ˆyq©。由于一个查询图像xq中可能有多个类别，当分配了不同的标签c时，地面真相查询掩码是不同的。图1显示了k=1时的任务说明。</p>
<h2 id="4-方法"><a class="markdownIt-Anchor" href="#4-方法"></a> 4、方法</h2>
<p>==我们提出了一个新的框架，以解决小样本的语义分割问题。我们首先在不失一般性的前提下，在单次拍摄的环境下说明我们的模型。我们的网络由两个模块组成：密集比较模块（DCM）和迭代优化模块（IOM）。DCM在支持的例子和查询的例子之间进行密集的特征比较，而IOM则对预测结果进行迭代细化。图2（a）显示了我们框架的概况。为了将我们的网络从单次学习推广到k次学习，我们采用了一种注意力机制来融合来自不同支持实例的信息。此外，我们提出了一个新的测试环境，即使用带有边界框注释的支持图像来进行小样本的分割，这一点将在后面描述。==</p>
<img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210241509212.png" style="zoom:150%;" />
<h3 id="41-密集比较模块"><a class="markdownIt-Anchor" href="#41-密集比较模块"></a> 4.1、密集比较模块</h3>
<p>我们开发了一个双分支密集比较模块，将查询图像中的每个位置与支持实例进行密集比较，如图2（b）所示。该模块由两个子模块组成：<strong>一个是提取表征的特征提取器，一个是进行特征比较的比较模块。</strong></p>
<p><strong>特征提取</strong></p>
<p>特征提取器旨在从CNN中获取不同层次的表征，用于特征匹配。我们使用ResNet-50[9]作为特征提取器的骨干。如同以前的小样本分割工作一样，骨干模型是在Imagenet[4]上预训练的。正如在CNN特征可视化文献[39, 38]中所观察到的，较低层的特征往往与低层次的线索有关，例如边缘和颜色，而较高层的特征则与对象层次的概念有关，例如对象类别。在小样本的情况下，我们的模型应该适应任何未见过的类别。<strong>因此，我们不能假设在训练过程中学习了与未见过的类别相对应的特征。相反，我们关注的是可能构成未见过的类别所共享的物体部分的中间层特征。ResNet中的各层根据空间分辨率分为4个块，自然对应于4个不同的表示层次。我们选择由块2和块3产生的特征进行特征比较，并在块3之后放弃各层。我们在块2之后的层中使用扩张卷积[2]来保持特征图的空间分辨率。块2之后的所有特征图都有一个固定的尺寸，为输入图像的1/8。块2和块3之后的特征被串联起来，并通过3×3卷积编码为256维。我们在第5.1.3节中研究了用于比较的特征的选择。支持分支和查询分支都使用相同的特征提取器。在训练过程中，我们保持ResNet中的权重固定。</strong></p>
<p><strong>密集比较</strong></p>
<p>由于支持图像中可能有多个物体类别和杂乱的背景，我们希望获得一个只对应于目标类别的嵌入来进行比较。在这里，我们使用前景区域的全局平均池化化，将特征图挤压成一个特征向量。全局图像特征在分割任务中变得非常有用[19, 40, 3]，这可以通过全局平均池化化轻松实现。在我们的网络中，我们只对前景区域的特征进行平均，以过滤掉不相关的区域。在我们从支持集获得全局特征向量后，我们将该向量与查询分支生成的特征图中的所有空间位置连接起来。这一操作的目的是将查询分支中的所有空间位置与支持分支中的全局特征向量进行比较。然后，串联的特征图再经过另一个卷积块与256个3×3卷积滤波器进行比较。</p>
<p>为了有效实施，我们首先对二元支持掩码进行双线性下采样，使其与特征图的空间大小相同，然后对特征图进行逐元乘法。因此，属于背景区域的特征层为零。然后，我们采用全局池化，将得到的向量除以前景区域，得到平均特征向量。我们将该向量向上取样到与查询特征相同的空间大小，并将它们连接起来进行密集比较。</p>
<h3 id="42-迭代优化模块"><a class="markdownIt-Anchor" href="#42-迭代优化模块"></a> 4.2. 迭代优化模块</h3>
<p>由于同一类别中存在着外观上的差异，密集比较只能匹配物体的一部分，这可能不足以准确地分割图像中的整个物体。我们观察到，初始预测是关于物体粗略位置的一个重要线索。我们提出一个迭代优化模块，对预测结果进行迭代优化。其结构如图2（c）<strong>所示。该模块的输入是由密集比较模块生成的特征图和上一次迭代的预测掩码。直接将带有预测掩码的特征图作为附加通道连接起来会导致特征分布的不匹配，因为在第一次前向传递中没有预测掩码。相反，我们建议将预测的掩码以残差形式纳入。</strong></p>
<p>其中x是密集比较模块的输出特征；yt-1是上一个迭代步骤的预测掩码，Mt是残差块的输出。函数F（-）是特征x和预测掩码yt-1的连接，然后是两个3×3的卷积块，有256个滤波器。然后，我们添加两个具有相同数量卷积滤波器的虚化残差块。在此基础上，我们使用Deeplab V3[3]中提出的Atrous Spatial Pyramid Pooling模块（ASPP）来捕捉多尺度信息。该模块由四个平行分支组成，包括三个3×3的卷积，其阿特拉斯率分别为6、12和18，以及一个1×1的卷积。1×1卷积是在图像级别的特征上操作的，这是通过全局平均池化化实现的。然后将得到的向量双线性上采样到原始空间大小。来自4个分支的输出特征被串联起来，并通过另一个带有256个过滤器的1×1卷积进行融合。最后，我们使用1×1卷积来生成最终的掩码，包括一个背景掩码和一个前景掩码。我们使用softmax函数对每个位置的分数进行归一化处理，从而输出前景和背景的置信度图。然后，这些置信图被送入下一个IOM进行优化。我们的最终结果是通过对置信度图进行双线性上样，使其与查询图像的空间大小相同，并根据置信度图对每个位置进行分类。在训练时，为了避免迭代优化模块过度拟合预测mask，我们交替使用上一个epoch的预测mask和空mask作为IOM的输入。预测掩码yt-1被重置为空掩码，其概率为pr。这可以看作是整个掩码的丢弃，是标准丢弃的延伸[32]。与以往分割文献中的迭代细化方法相比[14, 34, 22]，我们的方法将细化方案整合到具有剩余连接的模型中，这样整个模型可以以前馈的方式运行，并且是端到端的训练。</p>
<h3 id="43-k-shot-分割的注意机制"><a class="markdownIt-Anchor" href="#43-k-shot-分割的注意机制"></a> 4.3. k-shot 分割的注意机制</h3>
<p>为了在k-shot设置中有效地合并信息，我们使用一个注意力机制来融合不同支持实例产生的比较结果。具体来说，我们在DCM中增加了一个与密集比较卷积平行的注意模块（见图3）。注意力分支由两个卷积块组成。第一个有256个3×3滤波器，然后是3×3最大集合。第二个有一个3×3卷积，然后是全局平均池化。注意力分支的结果作为权重λ。然后，所有支持实例的权重通过softmax函数进行归一化。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210241557121.png" alt="" /></p>
<p>最终的输出是由不同支持样本产生的特征的加权和。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210241539954.png" alt="" /></p>
<h3 id="44-bbox注释"><a class="markdownIt-Anchor" href="#44-bbox注释"></a> 4.4. bbox注释</h3>
<p>由于我们的密集比较模块的本质是将查询图像中的每个位置与支持实例提供的全局表示进行密集比较，我们探索了一种新的支持集注释形式，即使用边界盒。与像素级注释相比，边界盒注释使用一个矩形框来表示物体区域，这在物体检测任务中经常使用。标注边界盒比像素化标注要便宜得多。我们通过将整个边框区域视为前景来放松支持集。我们在这种设置下测试我们的模型，以评估我们框架的能力。两种测试设置的比较如图4所示。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210241554933.png" alt="图4" /></p>
<h2 id="5-实验"><a class="markdownIt-Anchor" href="#5-实验"></a> 5、实验</h2>
<p>为了评估我们提出的方法的性能，我们在PASCAL VOC 2012数据集和COCO数据集上进行了广泛的实验。我们的网络是端到端的训练。损失函数是输出地图中所有空间位置上交叉熵损失的平均值。我们的网络是用SGD在Nvidia Tesla P100 GPU上用PyTorch库训练了200个epochs。我们将学习率设置为0.0025，将概率pr设置为0.7。我们在PASCAL-5i上使用4个事件的迷你批处理进行训练，在COCO上使用8个事件。在推理时间，我们在初始预测后对预测结果进行了4次迭代优化。</p>
<p><strong>评价指标</strong></p>
<p>在以前的工作中，评价指标有一个微小的差异。Shaban等人[29]测量每个类别的前景交叉-联合（IoU）并使用所有类别的平均IoU（meanIoU）来报告结果。而在[24，5]中，他们忽略了图像类别，并计算所有测试图像的前景IoU和背景IoU的平均值（FB-IoU）。由于以下原因，我们在分析实验中选择了MeanIoU评价指标。</p>
<p>1）不同类别的测试样本的数量并不平衡（例如，49个羊类与378个人类）。忽略图像类别可能会导致对有更多图像的类别产生偏颇的结果。另外，我们可以通过meanIoU评价指标观察我们的模型在不同类别中的有效性。</p>
<p>2）由于大多数物体相对于整个图像来说是很小的，即使模型未能分割任何物体，背景IU仍然可以很高，因此不能反映模型的能力。</p>
<p>3）在二元分割的文献中（如视频分割和交互式分割），前景IU更经常被使用。尽管如此，我们仍然在这两个评价指标下将我们的结果与以前的工作进行比较。</p>
<h3 id="51-pascal-5i"><a class="markdownIt-Anchor" href="#51-pascal-5i"></a> 5.1. PASCAL-5i</h3>
<p>PASCAL-5i是在[29]中提出的一个用于小样本语义分割的数据集。它是建立在PASCAL VOC 2012的图像和SDS[7]的额外注释上。来自PASCAL VOC的20个物体类别被均匀地分为4个部分，其中3个部分用于训练，1个部分用于测试。在测试时，在测试部分随机抽出1000个支持-查询对。关于PASCAL-5i的更多细节可以在[29]中找到。</p>
<h4 id="511-comparison-with-the-state-of-the-art-methods"><a class="markdownIt-Anchor" href="#511-comparison-with-the-state-of-the-art-methods"></a> 5.1.1 Comparison with the State-of-the-art Methods</h4>
<p>我们在表1中把我们的模型与最先进的方法进行了比较。表1（a）显示了在meanIoU评估指标下的评估结果，表1（b）显示了在FB-IoU指标下的结果。对于[29]在FB-IoU指标下的表现，我们引用[24]中转载的结果。我们的模型在两个评价指标下都明显优于现有的方法。特别是，我们的平均IoU得分在1次拍摄任务中比最先进的结果高出14.6%，在5次拍摄任务中高出13.2%。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210241603785.png" alt="" /></p>
<blockquote>
<p>Table 1 – Results on the PASCAL-5i dataset. Our proposed method outperforms all previous methods under both evaluation metrics and sets a new state-of-the-art performance (bold).</p>
</blockquote>
<p><strong>定性结果</strong></p>
<p>图5显示了我们分割结果的一些定性的例子。请注意，在给定相同的查询图像时，我们的模型能够在不同的支持例子中划分出不同的类别（见图5中的第5和第6个例子）。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210241607257.png" alt="" /></p>
<blockquote>
<p>Figure 5 – Qualitative examples of 1-shot segmentation on the PASCAL-5i dataset. The first row is query images and support images (right bottom) with ground-truth annotations. The second row is our predicted results. Note that the 5th and the 6th examples have the same query images and our model is able to segment different classes when different support examples are presented.</p>
</blockquote>
<p><strong>“5.1.2 Experiments on Bounding Box Annotations”</strong> (<a href="zotero://select/library/items/3AR39Q4N">Chi Zhang 等。, 2019, p. 5222</a>) (<a href="zotero://open-pdf/library/items/BDTNTTDB?page=6">pdf</a>)</p>
<p>我们在测试时用边界盒注释的支持集来评估CANet。我们从PASCAL VOC 2012数据集和SDS[7]中获取边界盒注释。支持掩码是指支持图像中一个实例的边界框内的区域，而不是所有实例。该实例是随机选择的。如表2所示，使用边界框注释支持集的性能与使用昂贵的像素级注释支持集的结果相当，这意味着我们的密集比较模块能够抵御边界框内背景区域引入的噪声。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210241609499.png" alt="" /></p>
<p><strong>“5.1.3 Ablation Study”</strong> (<a href="zotero://select/library/items/3AR39Q4N">Chi Zhang 等。, 2019, p. 5222</a>) (<a href="zotero://open-pdf/library/items/BDTNTTDB?page=6">pdf</a>)</p>
<p>我们在PASCAL-5i数据集上进行了广泛的消融实验，以检查我们网络中不同组件的有效性。所有的结果都是在PASCAL-5i数据集上进行的4次分割的平均IoU。</p>
<p><strong>特征比较</strong></p>
<p>在表3中，我们比较了在ResNet-50中使用不同级别特征进行特征比较的模型变体。在所有情况下，我们在比较前将特征编码为256维，并且不采用迭代优化。我们用单块和多块进行特征比较实验。当使用单块进行比较时，块3表现最好。当使用多个块进行比较时，块2和块3的组合取得了最好的结果。原因是区块2对应的是相对低级的线索，单靠它不足以匹配物体的部分。而block4对应的是高层次的特征，例如类别，并且包含了大量的参数（2048个通道），这使得它很难在几率设置下进行优化。块2和块3的组合对于匹配类别无关的对象部分是最好的。我们还用VGG16作为特征提取器来实施实验。我们选择阶段2、3和4（共5个）的特征。以VGG为骨干的最终多尺度测试结果是54.3%。与ResNet50版本（55.4%）相比，性能只下降了1.1%，仍然明显优于最先进的结果。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210241610586.png" alt="" /></p>
<p><strong>迭代优化模块。</strong></p>
<p>为了验证我们提出的迭代优化模块的有效性，我们将我们的网络与不采用额外的IOM进行优化的基线模型，即来自CANet(CANet-Init)的初始预测进行比较。我们还将我们的迭代优化方案与DenseCRF[12]进行了比较，DenseCRF是一种在分割文献中广泛使用的后处理方法，用于细化分割图。表4显示了不同模型变体的结果。如图所示，迭代优化比初始预测产生了2.8%的改进。DenseCRF并没有明显改善小样本的分割预测。我们将结果可视化，发现对于成功定位大部分物体区域的预测面具，DenseCRF可以有效地改善分割结果，特别是在物体边界区域。然而，对于失败的面具，例如错误的物体定位，DenseCRF会扩大假阳性区域，从而恶化了IoU得分。另一方面，我们的IOM可以有效地填充物体区域，并以可学习的方式去除不相关的区域。我们在图6中可视化了我们迭代优化过程的中间结果。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210241608574.png" alt="" /></p>
<p><strong>注意力vs.特征融合vs.掩码融合。</strong></p>
<p>在k-shot设置中，我们将我们的注意力机制与以前工作中的几种解决方案进行比较。</p>
<ol>
<li>特征级平均融合。我们实验了[24]中的方法，也就是对不同支持实例产生的特征进行平均。</li>
</ol>
<ol start="2">
<li>掩码的逻辑OR融合。Shaban等人[29]使用1-shot模型对每个支持实例进行预测，并使用逻辑OR操作来融合各个预测的掩码。逻辑OR操作意味着，如果有任何支持实例将其预测为前景，则该位置将被预测为前景。</li>
<li>mask的平均融合。此外，我们还尝试用平均操作来融合单个1次预测的置信度图。我们在表5中报告了CANet与不同融合方案的结果。我们的注意力机制表现最好，并且比1次拍摄的基线带来了最大的增量。这表明，在融合来自不同支持实例的信息时，学习到的注意力模块比特征层面或图像层面的非学习性融合方法更有效。使用逻辑OR操作来融合预测的掩码并没有显示出比1次拍摄结果的改进。</li>
</ol>
<p><strong>多规模评价。</strong></p>
<p>我们还实验了多尺度评估，这在分割文献中是很常见的。具体来说，我们通过[0.7, 1, 1.3]对查询图像进行重新缩放，并对其预测结果进行平均。在1次拍摄和5次拍摄的设置中，多尺度评估分别带来1.4%和1.3%的平均IoU改进。</p>
<h2 id="6-结论"><a class="markdownIt-Anchor" href="#6-结论"></a> 6、结论</h2>
<p>我们提出了CANet，一个新颖的具有少许学习功能的类诊断分割网络。密集比较模块利用CNN中的多层次特征来进行密集特征比较，迭代优化模块学习迭代完善预测结果。我们解决k-shot问题的注意机制证明比不可学习的方法更有效。综合实验表明我们的框架是有效的，其性能明显优于以前的所有工作。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">BAM</div></div><div class="info-2"><div class="info-item-1"> BAM  Abstract 近来，小样本分割分割技术（FSS）得到了广泛的发展。然而，训练出来的模型偏向于所看到的类别，而不是理想的类别无关性，从而阻碍了对新内容的识别。本文提出了一种新颖而直接的见解来缓解这一问题。具体来说，我们在传统的 FSS 模型（元学习器）上增加了一个分支（基础学习器），以明确识别基础类别的目标，即不需要分割的区域。然后，对这两个学习器并行输出的粗略结果进行自适应整合，从而得出精确的分割预测结果。考虑到元学习器的敏感性，我们进一步引入了一个调整因子来估计输入图像对之间的场景差异，以促进模型的集合预测。在 PASCAL-5i 和 COCO-20i 上取得的显著性能提升验证了这一方法的有效性，而且令人惊讶的是，即使使用两个普通学习器，我们的多功能方案也创造了新的一流水平。此外，鉴于所提方法的独特性，我们还将其扩展到了更现实但更具挑战性的环境中，即广义 FSS，在这种环境中，基础类和新类别的像素都需要确定。  1. Introduction   得益于成熟的大规模数据集 [8, 9,...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CBL/" title="CBL"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">CBL</div></div><div class="info-2"><div class="info-item-1"> 3. Effective Number of Samples We formulate the data sampling process as a simplified version of random covering. The key idea is to associate each sample with a small neighboring region instead of a single point. We present our theoretical framework and the formulation of calculating effective number of samples.  3.1. Data Sampling as Random Covering Given a class, denote the set of all possible data in the feature space of this class as S. We assume the volume of S is N and N ≥ 1. Denote...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">CA</div></div><div class="info-2"><div class="info-item-1"> Coordinate Attention for Efficient Mobile Network Design  Abstract 最近关于移动网络设计的研究已经证明了通道注意（如SE注意力机制）对于提高模型性能的显著效果，但他们通常忽略了位置信息，而位置信息对于生成空间选择性注意图是很重要的。在本文中，我们提出了一种新的移动网络注意机制，将位置信息嵌入到通道注意中，我们称之为...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning"><span class="toc-text"> CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text"> 摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-text"> 1、介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-text"> 2、相关工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E4%BB%BB%E5%8A%A1%E6%8F%8F%E8%BF%B0"><span class="toc-text"> 3、任务描述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%96%B9%E6%B3%95"><span class="toc-text"> 4、方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-%E5%AF%86%E9%9B%86%E6%AF%94%E8%BE%83%E6%A8%A1%E5%9D%97"><span class="toc-text"> 4.1、密集比较模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-%E8%BF%AD%E4%BB%A3%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9D%97"><span class="toc-text"> 4.2. 迭代优化模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-k-shot-%E5%88%86%E5%89%B2%E7%9A%84%E6%B3%A8%E6%84%8F%E6%9C%BA%E5%88%B6"><span class="toc-text"> 4.3. k-shot 分割的注意机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#44-bbox%E6%B3%A8%E9%87%8A"><span class="toc-text"> 4.4. bbox注释</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%AE%9E%E9%AA%8C"><span class="toc-text"> 5、实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#51-pascal-5i"><span class="toc-text"> 5.1. PASCAL-5i</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#511-comparison-with-the-state-of-the-art-methods"><span class="toc-text"> 5.1.1 Comparison with the State-of-the-art Methods</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E7%BB%93%E8%AE%BA"><span class="toc-text"> 6、结论</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA">CA</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>