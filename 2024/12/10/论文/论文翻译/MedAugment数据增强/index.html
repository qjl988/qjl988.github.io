<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>MedAugment | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="MedAugment: Universal Automatic Data Augmentation Plug-in for Medical Image Analysis  Abstract 在计算机视觉领域，数据增强（DA）被广泛用于缓解数据短缺问题，而医学图像分析（MIA）中的数据增强则面临多重挑战。医学图像分析中流行的数据增强方法包括传统数据增强、合成数据增强和自动数据增强。然而，这些方法的">
<meta property="og:type" content="article">
<meta property="og:title" content="MedAugment">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/MedAugment%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="MedAugment: Universal Automatic Data Augmentation Plug-in for Medical Image Analysis  Abstract 在计算机视觉领域，数据增强（DA）被广泛用于缓解数据短缺问题，而医学图像分析（MIA）中的数据增强则面临多重挑战。医学图像分析中流行的数据增强方法包括传统数据增强、合成数据增强和自动数据增强。然而，这些方法的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T16:39:12.973Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/MedAugment%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'MedAugment',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">MedAugment</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">MedAugment</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T16:39:12.973Z" title="更新于 2024-12-11 00:39:12">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>25分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="medaugment-universal-automatic-data-augmentation-plug-in-for-medical-image-analysis"><a class="markdownIt-Anchor" href="#medaugment-universal-automatic-data-augmentation-plug-in-for-medical-image-analysis"></a> MedAugment: Universal Automatic Data Augmentation Plug-in for Medical Image Analysis</h1>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>在计算机视觉领域，数据增强（DA）被广泛用于缓解数据短缺问题，而医学图像分析（MIA）中的数据增强则面临多重挑战。医学图像分析中流行的数据增强方法包括传统数据增强、合成数据增强和自动数据增强。然而，这些方法的使用带来了各种挑战，如经验驱动的设计和密集的计算成本。在此，我们提出了一种高效的自动图像增强方法–MedAugment。我们提出了像素增强空间和空间增强空间，并排除了可能破坏医学图像细节和特征的操作。此外，我们还提出了一种新颖的采样策略，即从这两个空间中采样有限数量的操作。此外，我们还提出了一种超参数映射关系，以产生合理的增强级别，并使 MedAugment 可通过单个超参数进行完全控制。这些修订解决了自然图像和医学图像之间的差异。在四个分类和三个分割数据集上的大量实验结果证明了 MedAugment 的优越性。我们认为，即插即用、无需训练的 MedAugment 有可能为医学领域做出宝贵贡献，尤其是对缺乏深度学习基础专业知识的医学专家而言。</p>
<h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<ul>
<li>
<p>医学影像分析（MIA）采用各种成像模式来创建人体内部的可视化表示，并协助进一步的医学诊断。目前，医学影像分析主要由医学专家进行，这一耗时耗力的过程可能会导致解释和准确性的差异。为此，深度学习（DL）被引入 MIA 领域，以协助 MIA，尤其是主流的分类和分割任务。虽然基于深度学习的 MIA 取得了可喜的成果[1-4]，但在数据稀缺的情况下确保深度学习模型的性能仍具有挑战性。与自然图像[5]不同，医疗图像分析中的数据稀缺主要归因于两个因素。首先，收集医学图像需要专业设备，并需要专家注释。其次，收集到的图像的分发受到病人隐私问题的限制[6]。在这种情况下，人们提出了各种技术来缓解数据短缺问题，而数据增强（DA）是最普遍、最有效的一种技术[7]。数据扩增通过增强数据的多样性和丰富性来提高模型的性能和泛化能力，其在 MIA 领域的普遍应用包括传统数据扩增、合成数据扩增和自动数据扩增。</p>
<p>传统 DA 方法利用各种 DA 操作（如旋转、翻转和平移[7]）来组成不同的 DA 管道。这种方法虽然在 MIA 中直接有效 [8-12]，但也带来了明显的挑战。操作选择、顺序调整和幅度确定等管道设计在很大程度上依赖于经验。这可能不适合没有扎实 DL 基础的人员，并可能导致次优的增强多样性。利用生成式对抗网络（GAN）[13]来执行 DA 是最流行的合成方法之一。生成式对抗网络包括生成器和判别器进行对抗游戏，生成器学习合成逼真的人工图像，判别器则努力区分真实图像和合成图像。然而，在 MIA 中使用 GAN [14-23] 面临着各种挑战，包括耗时且耗费数据的训练 [24, 25]，以及合成质量的参差不齐。除了利用 GAN 外，一些研究人员还利用扩散模型 [26] 来增强医学图像 [27-30]。然而，这种方法面临采样速度低、计算成本高等问题 [29]。</p>
<p>自动诊断[31-36]由一个包含各种常规操作的增强空间组成，通过从空间中采样的操作对输入进行增强。这可以为输入产生不同的增强，从而提高数据多样性和模型泛化能力。虽然已有多种自动数据分析方法[31-40]适用于 MIA，但这些方法可能不适合医学图像，而且计算量很大。无处不在的数据短缺问题[20, 41, 42]，再加上遇到的扩增挑战，在很大程度上影响了模型在 MIA 中的表现。为了解决数据短缺问题和所面临的挑战，我们提出了一种高效的自动增强方法 MedAugment。与现有的单一增强空间方法相比，我们提出了两个增强空间，即像素增强空间 Ap 和空间增强空间 As，并排除了可能破坏医学图像细节和特征的操作。此外，我们还提出了一种新颖的操作采样策略，即限制从两个空间采样的操作数量。此外，我们还提出了一种超参数映射关系，以产生合理的增强水平，并使 MedAugment 只需一个超参数即可完全控制。这些修改有效地解决了自然图像和医学图像之间的差异。在四个分类和三个分割数据集上的大量实验结果证明了所提出的 MedAugment 的领先性。即插即用、无需训练的 MedAugment 可使 MIA 社区受益，尤其是那些没有坚实 DL 基础的医学专家。总之，我们的主要贡献是</p>
</li>
<li>
<p>我们提出了一种高效的自动数据扩增方法，称为 MedAugment。MedAugment 即插即用，无需训练，可同时执行医学图像分类和分割任务。</p>
</li>
<li>
<p>我们提出了像素增强空间和空间增强空间，以及新颖的采样策略和超参数映射关系，以有效解决自然图像和医学图像之间的差异。</p>
</li>
<li>
<p>我们在四个分类和三个分割数据集上进行了广泛的实验，结果证明了所提出的 MedAugment 的优越性。</p>
</li>
</ul>
<p>本文接下来的内容安排如下。第 2 节 &quot;相关工作 &quot;介绍了自动数据分析和 MIA 中数据分析的最新进展。第 3 节 &quot;方法 &quot;讨论了所建议的 MedAugment 的详细方法。第 4 节 &quot;实验与结果 &quot;展示了数据集、实验设置和主要结果。我们还进行了详细的分析和消融实验。我们在第 5 节 &quot;结论 &quot;中总结了我们的工作并指出了未来的展望。</p>
<h2 id="related-work"><a class="markdownIt-Anchor" href="#related-work"></a> Related Work</h2>
<h3 id="21-automatic-data-augmentation"><a class="markdownIt-Anchor" href="#21-automatic-data-augmentation"></a> 2.1. Automatic Data Augmentation</h3>
<p>人们开发了许多自动 DA 方法，将常规操作结合起来以提高模型性能。2019 年，Cubuk 等人[31] 开发了一种 AutoAugment，其中搜索空间中的一个策略由多个子策略组成，每个子策略针对每幅图像随机选择。每个子策略由从 16 个中选出的两个 DA 操作组成。虽然 AutoAugment 取得了可喜的性能，但 DA 策略是使用强化学习方法搜索的，因此计算成本较高。为此，Lim 及其同事[32]提出了 “快速自动增强”（Fast AutoAugment），通过在配对训练数据集上进行密度匹配来确定增强策略。Fast AutoAugment 基于贝叶斯 DA [43]，可以在策略搜索阶段通过贝叶斯优化恢复额外的缺失数据点。此外，Ho 等人[33] 提出了一种基于群体的增强方法，以产生非平稳策略而非固定策略。这些方法虽然降低了搜索成本，但并没有消除单独的搜索阶段。因此，Cubuk 及其同事开发了一种 RandAugment [34] 方法，在这种方法中，具有相同增强水平的多个 DA 运算被依次利用。RandAugment 的增强空间包括 14 个操作。与 RandAugment 类似的工作包括 TrivialAugment [35]，该方法利用单一操作，并对每幅图像的增强级别重新采样。此外，UniformAugment [36] 将操作次数固定为两次，并以 p = 0.5 的概率放弃每次操作。除了连续利用 DA 操作外，另一种方法是并行组合这些操作。例如，AugMix[44] 从九个操作中随机抽样各种操作，组成一个增强链。虽然这些方法有效且计算量低，但它们的使用在 MIA 领域提出了各种挑战。首先，反转、均衡和日晒等操作会破坏医学图像中错综复杂的细节和特征。其次，采样策略往往会忽略医学图像对亮度、对比度和海报化等操作的高度敏感性。最后，亮度和对比度等操作会破坏掩膜中存储的灰阶类别信息，从而限制模型在图像分割方面的性能。</p>
<h3 id="22-data-augmentation-in-mia"><a class="markdownIt-Anchor" href="#22-data-augmentation-in-mia"></a> 2.2. Data Augmentation in MIA</h3>
<p>大多数研究人员在 MIA 中采用 DA 方法[7]，普遍使用的方法有三个方面。首先，大多数研究都利用了传统的 DA。例如，Kaushik 等人[11] 利用平移、旋转、缩放、翻转等方法增强眼底图像，用于糖尿病视网膜病变诊断。Khened 及其同事[10]利用旋转、平移、缩放、高斯噪声等方法增强数据集，用于心脏分割。此外，Isensee 等人开发的 nnU-Net [12]利用预设的 DA 管道，依次进行旋转、缩放、高斯噪声、高斯模糊等不同操作。虽然传统的检测方法简单有效，但管道设计严重依赖经验，可能会导致次优的增强多样性。其次，很大一部分研究人员采用 GAN 等合成模型来合成人工图像。例如，Beers 及其同事[22] 证明了利用 PGGAN[45] 合成眼底和胶质瘤图像的可行性。Calimeri 等人[46] 采用 LAPGAN[46] 生成脑磁共振成像图像。此外，2022 年开发的 DPGAN [14] 由三个变异自动编码器 GAN 组成，用于合成人工图像和标签。利用基于 GAN 的方法会面临各种挑战，如训练耗时、数据量大、合成质量参差不齐等。一些研究集中于采用扩散模型来执行合成 DA 作为替代方法。例如，Moghadam 等人[27] 利用扩散模型、颜色归一化和优先形态加权生成组织病理学图像。Pinaya 及其同事[28] 利用潜在扩散模型[47] 合成了三维人工大脑图像。此外，2023 年开发的 MGCC [30] 在进行半监督分割时，将潜在扩散模型 [47] 生成的图像用作非标记数据。不过，这类方法可能会受到采样速度低和计算成本高的限制。第三，一些研究人员利用了自动 DA 方法。例如，Qin 等人[37] 开发了一种联合学习策略，将分割模块和 Dueling DQN[48] 结合起来，以寻求最大的性能改进。Xu 及其同事[38] 提出了一种使用随机松弛和蒙特卡罗方法更新参数的可微分方法。2022 年开发的 AADG 框架[39] 包含一个新的代理任务，利用 Sinkhorn 距离最大化各种增强域之间的多样性。此外，Yang 等人[40] 利用验证精度来更新递归神经网络的控制器。这些方法都面临计算成本的挑战。</p>
<h2 id="methods"><a class="markdownIt-Anchor" href="#methods"></a> Methods</h2>
<h3 id="31-medaugment"><a class="markdownIt-Anchor" href="#31-medaugment"></a> 3.1. MedAugment</h3>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003418306.png" alt="image-20231122003240037" /></p>
<p>我们在图 1 中展示了拟议的 MedAugment 的实现过程。MedAugment 包含 N = 4 个增强分支和一个用于保留输入特征的独立分支。我们设计了两个增强空间 Ap 和 As，并排除了可能破坏医学图像细节和特征的操作。Ap 和 As 分别包含 6 个和 8 个 DA 操作。此外，我们还开发了一种操作抽样策略 Π 来限制从两个空间中抽样的操作数量，从而在每个增强分支中产生 M = {2, 3} 个连续的 DA 操作。此外，我们还提出了一种新颖的映射关系，以产生合理的增强水平，并肯定了在单一增强水平 l = 5 的情况下，每个操作的最大幅度和概率都是可控的。这些修正能有效处理自然图像和医学图像之间的差异。值得注意的是，有些操作（如水平翻转）不具备幅度。总之，MedAugment 从三个方面引入了随机性，包括策略层面、操作层面和幅度层面。对于每个输入图像，MedAugment 都会使用采样策略对每个分支的操作进行采样。然后对采样的操作进行洗牌。最后，用洗牌后的操作对输入图像进行增强，其中操作幅度在最大幅度内均匀采样，概率根据 l 确定。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003240037.png" alt="image-20231122003303028" /></p>
<blockquote>
<p>图 1：MedAugment 的实现。MedAugment 由 N = 4 个增强分支和一个用于保留输入特征的独立分支组成。对于每个分支，使用采样策略 Π 从像素增强空间 Ap 和空间增强空间 As 对 M = {2, 3} DA 操作进行采样。</p>
</blockquote>
<h3 id="32-augmentation-spaces"><a class="markdownIt-Anchor" href="#32-augmentation-spaces"></a> 3.2. Augmentation Spaces</h3>
<p>我们将检测操作分为像素级检测操作和空间级检测操作，并用像素级检测操作和空间级检测操作构建了两个增强空间，分别称为 Ap 和 As。为了确保提议的 MedAugment 适用于医学图像，我们排除了会破坏医学图像细节和特征的操作，如反转、均衡和日晒。这就产生了 Ap = {亮度、对比度、海报化、锐度、高斯模糊、高斯噪声}，以及 As = {旋转、水平翻转、垂直翻转、缩放、平移 x、平移 y、剪切 x、剪切 y}。为了防止 Ap 中的操作影响遮罩中的灰度级信息，我们只使用 As 中的操作来增强遮罩。为了确保兼容性和可扩展性，我们利用成熟的增强框架 Albumentations [49] 来执行常规操作，因为它提供了卓越的多样性 [50-52]。</p>
<h3 id="33-sampling-strategy"><a class="markdownIt-Anchor" href="#33-sampling-strategy"></a> 3.3. Sampling Strategy</h3>
<p>在对 Ap 和 As 进行采样操作时，我们提出了一种有效的采样策略 Π，这是从两个方面考虑的。首先，医学图像对亮度等属性非常敏感。其次，大量的连续操作会导致输出图像不真实，与原始图像相差甚远[44]。为此，我们规定从 Ap 中采样的最大操作次数和总操作次数分别等于 1 次和 3 次。此外，我们认为将操作总数设为 1 并不重要，因为它会退化为没有组合的单一操作。考虑到这些因素，我们确定连续操作的次数为 M = {2，3}。在这种设置下，会产生四种采样组合 Π = {π1, π2, π3, π4}，其中从两个空间采样的操作次数分别等于 1 + 2, 0 + 3, 1 + 1 和 0 + 2。这个组合数决定了增强分支的数量。出于可扩展性的考虑，我们将 MedAugment 中的 N 设为可扩展，并将采样改为替换采样。此外，还可以屏蔽单独的分支。通过设置 N = 1 并屏蔽独立分支，MedAugment 可以执行一对一的增强。在图 2 中，我们比较了不同方法生成的增强图像实例。通过详细观察，我们发现 MedAugment 能生成最逼真的增强图像，防止破坏医学图像中的细节和特征。其余方法生成的增强图像可能可以通过 DL 模型识别，但可能缺乏临床相关性或可解释性。</p>
<h3 id="34-hyperparameter-mapping"><a class="markdownIt-Anchor" href="#34-hyperparameter-mapping"></a> 3.4. Hyperparameter Mapping</h3>
<p>我们提出了一种超参数映射关系，以产生合理的增强水平，并使用变量 l 完全控制每个操作的最大幅度（MA）和概率（PA）。我们观察到，医学图像对某些操作（如 Posterize）的幅度非常敏感。当剩余位数减少时，增强图像的质量会大幅下降。因此，我们基于广泛的实验，精心设计了这些类型操作的幅度，以确保结果增强的图像保持其重要性。我们在表1中展示了 l 和不同操作的 MA 之间的映射关系。值得注意的是，没有幅度的操作表示为“-”。出于可扩展性考虑，l 可以设置为 {1, 2, 3, 4}，较高的值对应更大的增强。函数 F 根据输入的 l 返回一个奇数，其公式为：</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003645598.png" alt="image-20231122003418306" /></p>
<p>其中，⌈⌉ 表示四舍五入。从 Ap 和 As 中采样的操作概率采用相同的公式，其中 PA = 0.2l。</p>
<h2 id="experiments-and-results"><a class="markdownIt-Anchor" href="#experiments-and-results"></a> Experiments and Results</h2>
<h3 id="41-datasets"><a class="markdownIt-Anchor" href="#41-datasets"></a> 4.1. Datasets</h3>
<p>我们利用四个数据集进行分类性能评估。乳腺超声数据集（BUSI）[53] 收集自 600 名 25 至 75 岁的女性患者。该数据集包含 780 幅图像，其中良性、恶性和正常图像分别为 437 幅、210 幅和 133 幅。BUSI 的平均图像分辨率约为 500 × 500。肺部疾病 X 射线数据集（LUNG）[54] 由卡塔尔大学和达卡大学的研究人员收集。它包括三个类别，分别对应 COVID-19、严重急性呼吸系统综合征和中东呼吸系统综合征，每个类别分别包含 423、134 和 144 幅图像。脑肿瘤磁共振成像数据集（BTMRI）[55] 由四个类别组成，分别对应胶质瘤、脑膜瘤、正常和垂体。每个类别包含 1321、1339、1595 和 1457 幅训练图像，以及 300、306、405 和 300 幅测试图像。白内障眼部相机数据集（CATAR）[56] 包括白内障和正常类别，其中 245 和 246 幅图像用于训练，61 和 60 幅图像用于测试。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003503461.png" alt="image-20231122003503461" /></p>
<blockquote>
<p>表 1：Ap 和 As 的运算 MA。⌊⌋ 表示向下舍入。函数 F 返回整数奇数。不带幅度的运算用 - 表示。</p>
</blockquote>
<p>我们使用三个数据集来比较分割性能，包括 LUNG 数据集，在该数据集中，不同类别的图像和掩码被合并。内窥镜结肠镜检查数据集（CVC）[57] 作为国际医学影像计算和计算机辅助干预会议训练阶段的官方数据库，包含从结肠镜视频中提取的 1224 帧息肉图像和掩码。结肠镜图像数据集（Kvasir）[58] 包含 1000 张胃肠道息肉图像和掩码，分辨率从 332 × 487 到 1920 × 1072 不等。</p>
<h3 id="42-experimental-setup"><a class="markdownIt-Anchor" href="#42-experimental-setup"></a> 4.2. Experimental Setup</h3>
<p>在模型训练前，我们对数据进行了预处理。我们将数据集划分为训练集、验证集和测试集，比例为 6:2:2 或在测试数据单独提供的情况下为 8:2。对于分类数据集，不同子集中每个类别的比例等于原始数据的比例。类平衡划分可以防止潜在的类别不平衡。在执行增强之前，图像和掩码被预处理为 224 × 224 的分辨率，增强仅在训练子集上应用。按照 MedAugment 的一对五增强方式，所有方法的增强训练子集大小是原始训练子集的五倍。</p>
<p>在分类方面，我们使用 Adam 作为优化器，衰减因子为 0.01。我们使用交叉熵作为损失函数。初始学习率为 0.002，每 20 个周期以 0.9 的因子逐步衰减。总周期为 40，使用了 8 次耐心的早停技术。批量大小设置为 128。我们使用 VGGNet [59]、ResNeXt [60] 和 ConvNeXt [61] 进行训练。模型基于准确度（ACC）、负预测值（NPV）、阳性预测值（PPV）、敏感性（SEN）、特异性（SPE）和 F1 分数（FOS）等指标进行评估。我们将 MedAugment 与基于 SOTA GAN 的方法 StyleGAN2-ADA [62] 以及包括 AutoAugment、AugMix、RandAugment 和 TrivialAugment 在内的基础和 SOTA 自动 DA 方法进行比较。报告的结果是所有类别的平均值（如果适用）。</p>
<p>在分割方面，我们使用骰子损失（dice loss），其余超参数遵循分类设置。对于模型训练，我们使用 UNet++ [63]、FPN [64] 和 DeepLabV3 [65]，ResNet-18 [66] 作为编码器 [67]。我们使用骰子分数（DS）、交并比（IoU）和像素准确度（PA）来评估性能。由于主流 SOTA 自动 DA 方法 [34–36] 是为分类设计的，我们提出传统流程作为性能比较的 SOTA 方法。根据报告 [7]，水平翻转、旋转和垂直翻转是 MIA 领域最常用的操作，我们提出了 OneAugment、TwoAugment 和 ThreeAugment。OneAugment 仅包括水平翻转，而 TwoAugment 和 ThreeAugment 分别由水平翻转、旋转和水平翻转、旋转、垂直翻转组成。每个 DA 操作的概率 p = 0.5。</p>
<h3 id="43-classification-results"><a class="markdownIt-Anchor" href="#43-classification-results"></a> 4.3. Classification Results</h3>
<p>表 2、表 3 和表 4 分别展示了 VGGNet、ResNeXt 和 ConvNeXt 的分类结果。可以看出，在不同的模型中，建议的 MedAugment 都优于 SOTA 方法。关于 VGGNet，MedAugment 在 24 个指标中的 19 个指标上排名第一，其中 5 个指标是联合指标。在 BUSI、LUNG、BTMRI 和 CATAR 数据集上，MedAugment 的准确率最高，分别达到 83.4%、85.1%、89.3% 和 95.9%。在 BTMRI 数据集上，StyleGAN2-ADA 显示出卓越的性能，而在 LUNG 和 CATAR 数据集上，TrivialAugmet 则取得了理想的结果。对于 ResNeXt，MedAugment 在 16 个指标中排名第一，其中 3 个指标是联合指标。在四个数据集上，MedAugmet 的准确率最高，分别为 79.0%、85.8%、87.2% 和 95.9%。在 BUSI 数据集中，StyleGAN2-ADA 表现领先；在 CATAR 数据集中，AugMix 的表现优于各种方法。此外，TrivialAugment 在 BUSI 和 LUNG 数据集上也取得了理想的结果。至于 ConvNeXt，MedAugment 在 23 项指标中均名列第一，只有在 LUNG 数据集上的 PPV 值例外。在不同的数据集上，MedAugment 的准确率最高，分别为 78.3%、85.8%、86.3% 和 96.7%。值得注意的是，CATAR 数据集的测试子集中包含的图像很少，因此可能会产生相同的结果。此外，在 BUSI、LUNG 和 BTMRI 数据集上观察到的 SEN 值相对较低，这表明模型可能无法正确分类真正的阳性样本。这可能是由于很难识别医学图像中的细微异常，尤其是当异常处于早期阶段时。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003303028.png" alt="image-20231122003556630" /></p>
<blockquote>
<p>表 2：使用 VGGNet 对不同数据集进行分类的结果。†: CATAR 持有一个小型测试子集，可能会产生相同的性能。结果以百分比表示，最佳结果以粗体表示。</p>
</blockquote>
<p>我们在图 3 中展示了不同自动 DA 方法的类激活图[68]。我们利用 BUSI 数据集，因为其肿瘤区域在背景中清晰可辨。这样就能直观地区分类激活图与图像之间的叠加。我们采用 VGGNet，因为它的性能优于其他模型。通过观察可以发现，MedAugment 增强了模型准确识别正确区域的能力。虽然大多数方法都能正确地将注意力集中在感兴趣的区域，但 MedAugment 实现了更全面的覆盖和更合适的轮廓。与其他方法相比，AutoAugment 的性能不够理想，注意力可能会集中在不正确的区域。我们还举例说明了一个例子，在这个例子中，无论使用哪种自动调整方法，模型都无法捕捉到肿瘤区域。这是合理的，因为与背景相比，肿瘤区域的差异微乎其微，因此难以捕捉。</p>
<p>在图 4 中，我们利用 t-SNE，直观地展示了不同 DA 方法在使用 VGGNet 的 BUSI 数据集上的表现。我们从第二线性层和 RELU 激活函数后面的分类头中提取特征。通过详细观察可以发现，MedAugent 在每个类别中都能获得最接近的点簇，尤其是在恶性类别中。此外，与其他方法相比，不同类别之间的距离较长。与其他方法相比，RandAugment 的表现相对较差，正常类别与良性和恶性类别有很大程度的重叠。此外，AutoAugment 没有在不同类别之间预设明显的界限。密集的类内分布和稀疏的类间分布可以证明 MedAugment 所带来的性能提升。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003556630.png" alt="image-20231122003631302" /></p>
<blockquote>
<p>图 3：使用 VGGNet 在 BUSI 数据集上比较不同 DA 方法的类激活图。</p>
</blockquote>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122010420122.png" alt="image-20231122003645598" /></p>
<blockquote>
<p>图 4：使用 VGGNet 在 BUSI 数据集上比较不同 DA 方法的 t-SNE 可视化效果。</p>
</blockquote>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003659911.png" alt="image-20231122003659911" /></p>
<blockquote>
<p>表 3：使用 ResNeXt 对不同数据集进行分类的结果。</p>
</blockquote>
<h3 id="44-segmentation-results"><a class="markdownIt-Anchor" href="#44-segmentation-results"></a> 4.4. Segmentation Results</h3>
<p>表 5 展示了 UNet++、FPN 和 DeepLabV3 的分割结果。可以看出，与其他方法相比，MedAugment 的性能最高。对于 UNet++，MedAugment 在 9 个指标中的 8 个指标上都取得了最高的结果。在 LUNG、CVC 和 Kvasir 数据集上，MedAugment 的 DS 分别达到最高的 91.8%、69.1% 和 68.1%。在 CVC 数据集上，OneAugment 的 PA 最高。在 FPN 方面，MedAugment 在 8 个指标上取得了最高的联合性能。在三个数据集上，MedAugment 的 DS 分别达到了 94.7%、77.3% 和 72.3%。TwoAugment 在 CVC 数据集上表现出色。关于 DeepLabV3，MedAugment 在所有指标上都取得了最高的联合性能。在不同的数据集上，MedAugment 的 DS 分别达到了 93.3%、80.0% 和 72.3%。TwoAugment 在 Kvasir 数据集上实现了相同的 PA。值得注意的是，与 DS 和 IoU 相比，我们观察到的 PA 明显更高。虽然高 PA 可能表明模型性能优越，但预测的像素可能比较分散，因此并不理想。</p>
<p>我们在图 5 中展示了不同方法的预测掩码和地面实况掩码的比较。我们利用 LUNG 数据集，因为它的肺部对象具有规则形状的形态，因此有助于清晰辨别不同方法之间的细微差别。由于 FPN 性能优越，我们采用了它。通过详细观察，我们可以发现，与地面实况相比，用 MedAugment 训练的模型能生成最准确的掩膜，其性能提升主要体现在两个方面。首先，它减少了被错误预测的像素数量。其次，它提高了模型预测更平滑边缘曲线和更精确轮廓的能力。与其他方法相比，我们可以发现 ThreeAugment 和 OneAugment 可以生成不规则的凸起，而 TwoAugment 和 ThreeAugment 则可以预测出丑陋和不规则的轮廓。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003831780.png" alt="image-20231122010420122" /></p>
<blockquote>
<p>表 5：使用各种模型对不同数据集进行分割的结果。</p>
</blockquote>
<p>在图 6 中，我们使用 FPN 比较了 LUNG 数据集上不同 DA 方法的 Bland-Altman 图，以说明每张测试图像的预测掩膜与地面实况掩膜之间的关系。我们计算预测掩码中的对象像素数量，将结果除以总像素数量，然后将输出结果与地面实况掩码进行比较。从结果可以看出，MedAugment 的平均值和标准偏差最低，分别为 0.19% 和 1.96%。与 SOTA 方法相比，ThreeAugment 的性能相对较差，平均值和标准偏差分别为 4.53% 和 6.87%。此外，大多数数据点都位于 95% 的一致性范围内，这表明预测掩膜和地面实况掩膜之间具有极佳的一致性。此外，数据没有显示出一致的趋势差异，表明不存在显著的系统性差异。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003817852.png" alt="image-20231122003817852" /></p>
<blockquote>
<p>图 6：在使用 FPN 的 LUNG 数据集上，不同 DA 方法的 Bland-Altman 图的 Cpmarsion。</p>
</blockquote>
<h3 id="45-ablation-study"><a class="markdownIt-Anchor" href="#45-ablation-study"></a> 4.5. Ablation Study</h3>
<p>我们设计了一种具有一个增强空间和随机抽样的消融方法，以验证所提出的增强空间和抽样策略的有效性。在此设置下，增强空间由 14 个操作组成，每次随机抽样 M = {2, 3} 个操作。我们利用 LUNG 数据集，因为在分类和分割任务中使用一致的数据集能产生更有说服力的结果。图 7 和图 8 分别展示了消融方法和 MedAugment 设置在 LUNG 数据集上的分类和分割任务性能比较。在分类方面，可以看到大多数指标都呈现出下降趋势，只有少数指标有所上升。此外，与上升相比，下降的幅度更为明显。在分割方面，我们观察到所有指标的下降幅度都高于分类指标。这一观察结果符合我们的预期，因为在像素增强空间 Ap 中进行随机抽样可能会产生不切实际的增强图像，如图 2 所示，尽管程度低于上述方法生成的图像。此外，像素级分割更容易产生不真实的图像。</p>
<p>我们通过验证 MedAugment 在 l = {1, 2, 3, 4} 条件下的性能来研究增强级别的影响。在图 9 和图 10 中，我们分别比较了 LUNG 数据集上不同 l 的分类和分割任务性能。虽然 l = 5 在大多数情况下性能领先，但部分指标（如 ConvNeXt 中的 SEN）可能相对较低。在分割方面，与其余值相比，l = 5 实现了更高的性能领先性，同时确保了最高的 ACC、NPV 和 PPV。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003846364.png" alt="image-20231122003831780" /></p>
<blockquote>
<p>图 7：在 LUNG 数据集上利用一个增强空间和 MedAugment 的分类结果比较。</p>
</blockquote>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003901091.png" alt="image-20231122003846364" /></p>
<blockquote>
<p>图 8：在 LUNG 数据集上利用一个增强空间和 MedAugment 的分割结果比较。</p>
</blockquote>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003913839.png" alt="image-20231122003901091" /></p>
<blockquote>
<p>图 9：LUNG 数据集上不同 l 的分类指标比较。</p>
</blockquote>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231122003631302.png" alt="image-20231122003913839" /></p>
<blockquote>
<p>图 10：LUNG 数据集上不同 l 的分割指标比较。</p>
</blockquote>
<h2 id="conclusions"><a class="markdownIt-Anchor" href="#conclusions"></a> Conclusions</h2>
<p>在本文中，我们提出了一种高效、有效的自动增量方法，即 MedAugment。我们开发了像素增强空间和空间增强空间，并排除了可能破坏医学图像细节和特征的操作。此外，我们还提出了一种采样策略，即限制从建议空间中采样的操作数量。此外，我们还提出了一种超参数映射关系，以产生合理的增强水平，并确保所提出的方法只需一个超参数即可完全控制。这些修订可以解决自然图像和医学图像之间的差异。在七个公共医疗数据集上的大量实验结果证明了所提出的 MedAugment 的有效性。我们相信，即插即用、无需训练的 MedAugment 可能会让 MIA 社区受益，尤其是那些没有坚实 DL 基础的医学专家。尽管 MedAugment 性能优越，但仍存在一些挑战。首先，必须采用有效的指标平衡方法，因为不同指标之间的性能差距很大，而且 SEN 等几个指标的结果并不理想。具体来说，可以采用不同的超参数，通过在训练过程中更新超参数来平衡不同的指标。其次，处理小尺寸物体所面临的挑战需要进一步研究，以减少将错误分类归入背景的可能性。这就引出了强调小物体的方法，例如根据物体的大小利用不同类型和级别的操作。显然，与较大物体的图像相比，较小物体的图像在增强过程中被放大的可能性更大。</p>
<h2 id="code"><a class="markdownIt-Anchor" href="#code"></a> Code</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python utils\medaugment.py --dataset tooth --train_type segmentation --level 5 --number_branch 4 --seed 8</span><br></pre></td></tr></table></figure>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/PHTrans/" title="PHTrans"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">PHTrans</div></div><div class="info-2"><div class="info-item-1"> PHTrans  Abstract. Transformer在计算机视觉领域的成功引起了医学影像界越来越多的关注。特别是在医学图像分割方面，许多基于卷积神经网络（CNN）和Transformer的优秀混合架构已被提出，并取得了令人印象深刻的性能。然而，这些将模块化的Transformer嵌入到CNN中的方法，大多数都难以发挥其全部潜力。在本文中，我们提出了一种用于医学图像分割的新型混合架构，称为PHTrans，它将Transformer和CNN并行地混合在主要构件中，从全局和局部特征中产生层次化的表征，并自适应地聚合它们，旨在充分利用它们的优势来获得更好的分割性能。具体来说，PHTrans遵循U形编码器-解码器的设计，并在深层阶段引入了并行的hybird模块，其中卷积块和修改后的3D Swin Transformer分别学习局部特征和全局依赖，然后通过序列到体积的操作来统一输出的维度，实现特征聚合。在颅顶以外的多图集标签和自动心脏诊断挑战者数据集上的大量实验结果证实了其有效性，一直优于最先进的方法。  1...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ParcNet/" title="ParC-Net"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">ParC-Net</div></div><div class="info-2"><div class="info-item-1"> ParC-Net ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">CA</div></div><div class="info-2"><div class="info-item-1"> Coordinate Attention for Efficient Mobile Network Design  Abstract 最近关于移动网络设计的研究已经证明了通道注意（如SE注意力机制）对于提高模型性能的显著效果，但他们通常忽略了位置信息，而位置信息对于生成空间选择性注意图是很重要的。在本文中，我们提出了一种新的移动网络注意机制，将位置信息嵌入到通道注意中，我们称之为...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#medaugment-universal-automatic-data-augmentation-plug-in-for-medical-image-analysis"><span class="toc-text"> MedAugment: Universal Automatic Data Augmentation Plug-in for Medical Image Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-text"> Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#related-work"><span class="toc-text"> Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-automatic-data-augmentation"><span class="toc-text"> 2.1. Automatic Data Augmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-data-augmentation-in-mia"><span class="toc-text"> 2.2. Data Augmentation in MIA</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#methods"><span class="toc-text"> Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-medaugment"><span class="toc-text"> 3.1. MedAugment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-augmentation-spaces"><span class="toc-text"> 3.2. Augmentation Spaces</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-sampling-strategy"><span class="toc-text"> 3.3. Sampling Strategy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#34-hyperparameter-mapping"><span class="toc-text"> 3.4. Hyperparameter Mapping</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experiments-and-results"><span class="toc-text"> Experiments and Results</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-datasets"><span class="toc-text"> 4.1. Datasets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-experimental-setup"><span class="toc-text"> 4.2. Experimental Setup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-classification-results"><span class="toc-text"> 4.3. Classification Results</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#44-segmentation-results"><span class="toc-text"> 4.4. Segmentation Results</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#45-ablation-study"><span class="toc-text"> 4.5. Ablation Study</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusions"><span class="toc-text"> Conclusions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#code"><span class="toc-text"> Code</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA">CA</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>