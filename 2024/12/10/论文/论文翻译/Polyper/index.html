<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Polyper | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Polyper  Abstract 我们提出了一种新的息肉分割边界敏感框架，称为 Polyper。我们的方法源于一种临床方法，即经验丰富的医生经常利用息肉内部区域的固有特征来解决边界模糊的问题。受此启发，我们提出明确利用息肉区域来增强模型的边界辨别能力，同时最大限度地减少计算量。我们的方法首先通过形态学算子从初始分割图中提取边界和息肉区域。然后，我们设计了对边界敏感的注意力，利用内部息肉区域的特">
<meta property="og:type" content="article">
<meta property="og:title" content="Polyper">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Polyper/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="Polyper  Abstract 我们提出了一种新的息肉分割边界敏感框架，称为 Polyper。我们的方法源于一种临床方法，即经验丰富的医生经常利用息肉内部区域的固有特征来解决边界模糊的问题。受此启发，我们提出明确利用息肉区域来增强模型的边界辨别能力，同时最大限度地减少计算量。我们的方法首先通过形态学算子从初始分割图中提取边界和息肉区域。然后，我们设计了对边界敏感的注意力，利用内部息肉区域的特">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:03:57.058Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Polyper/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Polyper',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">Polyper</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Polyper</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:03:57.058Z" title="更新于 2024-12-11 01:03:57">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">6.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="polyper"><a class="markdownIt-Anchor" href="#polyper"></a> Polyper</h1>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>我们提出了一种新的息肉分割边界敏感框架，称为 Polyper。我们的方法源于一种临床方法，即经验丰富的医生经常利用息肉内部区域的固有特征来解决边界模糊的问题。受此启发，我们提出明确利用息肉区域来增强模型的边界辨别能力，同时最大限度地减少计算量。我们的方法首先通过形态学算子从初始分割图中提取边界和息肉区域。然后，我们设计了对边界敏感的注意力，利用内部息肉区域的特征集中增强边界区域附近的特征，从而生成良好的分割结果。我们提出的方法可与 ResNet-50、MiT-B1 和 Swin Transformer 等经典编码器网络无缝集成。为了评估 Polyper 的有效性，我们在五个公开的挑战性数据集上进行了实验，并在所有数据集上都获得了一流的性能。</p>
<h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<p>结肠息肉是结肠粘膜内的突起物，在形状、质地和颜色上有很大差异（Pooler 等人，2023 年）。重要的是，结肠息肉被认为是与结肠癌发展密切相关的癌前病变（Djinbachian 等人，2020 年）。因此，迫切需要提高早期检测的效率和息肉轮廓分割的准确性。由于息肉的边界不明显且对比度低，因此在结肠镜检查中给诊断带来了挑战。在初期阶段，息肉通常表现为较小的尺寸，导致边缘不清晰，从而增加了检测难度。</p>
<p>为应对这一挑战，当前的研究趋势之一是最大限度地整合不同尺度的特征，以保留尽可能多的边界细节。典型的例子是，Wu 等人（Wu et al. 2021）引入了语义校准和细化技术，以弥合不同层次特征图之间的语义差距，从而获得良好的息肉分割图。类似地，SwinE-Net（Park 和 Lee，2022 年）利用多编译卷积和多特征聚合块，完善了从 CNN 和 Swin Transformer 架构中提取的多层次特征。虽然这种方法可以处理边缘清晰的病变中后期息肉，但却难以有效处理边缘对比度较低的早期息肉。</p>
<p>另一种流行的策略是生成息肉掩膜来粗略定位息肉，然后通过增强潜在边界周围的语义特征来提高分割图的质量。作为早期的尝试，SegT（Chen、Ma 和 Zhang，2023 年）通过评估前景和背景区域之间的差异来突出息肉区域的边界。然而，由于息肉边界与周围粘膜混合造成的模糊性，准确捕捉息肉边界是一项挑战。仅仅依靠前景和背景信息之间的协同效应可能无法实现准确的息肉分割。CaraNet（Lou 等人，2022 年）利用轴向注意力的远距离交互作用，从全局角度计算成对亲和度。粗略估计的息肉区域会从深层特征中移除，然后利用不同尺度的特征来补充边界细节，以产生良好的 arXiv:2312.08735v1 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>] 14 Dec 202 分割结果。然而，轴向关注可能无法始终使网络受益，因为它有可能无意中排除关键的边界信息（Thanh Duc 等，2022 年）。因此，上述方法无法充分解决边界模糊的问题。</p>
<p>在内窥镜筛查中，技术娴熟的医生通常会利用非边界区域的息肉特征来解决边界模糊的难题。受此启发，我们的方法首先生成初始分割图，并利用形态学算子将息肉分割为边界区域和非边界区域。然后，我们利用从非边界区域提取的语义特征，通过一个新颖的边界敏感注意模块来完善边界区域，该模块可以利用全局和局部特征来识别真正的息肉边界。</p>
<p align="center">     <img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202312222018439.png" alt=" " />    <p align="left">   图 1：在五个流行数据集上与最先进方法的比较。   	</p> </p>
<p>我们的方法被称为 Polyper，它简单易学，适用于实际医疗场景。我们在五个广泛使用的数据集上进行了一系列实验来评估 Polyper。如图 1 所示，在所有数据集上，Polyper 的 mDice 和 mIoU 分数都优于之前的方法。通常情况下，由于息肉特征的异质性，以往的研究大多对生长早期的小息肉效果不佳。在随后的消融研究中，我们验证了我们的方法在小息肉上的良好表现。</p>
<p>我们的主要贡献可归纳如下：</p>
<ul>
<li>
<p>我们提出了一个名为 &quot;边界敏感注意 &quot;的新模块，<strong>它可以模拟息肉的边界区域和内部区域之间的关系，利用内部区域的固有特征来增强边界区域附近的特征</strong>。</p>
</li>
<li>
<p>我们设计了一种用于息肉分割的新型解码器，由两个不同的阶段组成：<strong>潜在边界提取和边界敏感细化</strong>。这种解码器能帮助我们有效识别息肉的真实边界，解决内窥镜检查中边界模糊的难题。</p>
</li>
<li>
<p>我们在五个流行的息肉分割数据集上对所提出的 Polyper 进行了评估，结果几乎在所有基准上都创下了新纪录。</p>
</li>
</ul>
<h2 id="related-work"><a class="markdownIt-Anchor" href="#related-work"></a> Related Work</h2>
<p>**医学图像分割架构。**CNN 是医学图像分割领域应用最广泛的深度神经网络架构之一。最经典的网络之一 U-Net（Ronneberger、Fischer 和 Brox，2015 年）就是一个典型的例子。Attention U-Net（Oktay 等人，2018 年）引入了一种新颖的注意力门机制，使模型能够选择性地关注不同形状和大小的目标。Res-UNet（Xiao 等人，2018 年）采用了加权注意力机制来提高分割性能。R2U-Net （Alom 等人，2018 年）巧妙地融合了残差网络（He 等人，2016 年）和 U-Net 的优势。KiU-Net（Valanarasu 等人，2020 年）提出了一种创新结构，利用欠完整和超完整特征来增强对具有小解剖结构的病变区域的分割。AttResDUNet（Khan 等人，2023 年）在卷积块中的跳过连接和残余连接上加入了注意门。最近，人们对利用 Transformers（Huang 等人，2022 年）进行息肉分割产生了浓厚的兴趣。SwinMM （Wang 等人，2023 年）开发了一种跨视点解码器，通过交叉注意块聚合多视点信息。MPU-Net（Yu 和 Han，2023 年）旨在通过将图像序列化与位置注意力模块相结合实现精确定位，使模型能够有效理解更深层的上下文依赖关系。Segtran（Li 等人，2021 年）提出了一种新颖的 &quot;挤压-扩展 &quot;转换器。</p>
<p>**细化方法。**细化的一个途径是最大限度地利用不同尺度的特征。Wu 等人（Wu et al. 2021 年）采用语义校准和细化技术来弥合不同层次特征映射之间的语义差距。SwinE-Net（Park 和 Lee，2022 年）通过多压缩卷积和多特征聚合块来完善和增强从 CNN 和 Swin Transformer 提取的多级特征。FTMFNet （Liu 等人，2023 年）提出了一种用于分割小息肉物体的傅立叶变换多尺度特征融合网络。另一种方法是针对特定区域进行细化。PraNet（Fan等人，2020年）和CaraNet（Lou等人，2022年）都集成了反向关注模块（Chen等人，2018年），这是一个专门的组件，可以突出息肉及其周围环境之间的边界。Xie 等人（Xie et al. 2020）引入了辅助边界监督作为完善玻璃分割的指导机制，帮助预测边界周围的不确定区域。与直接增强边界特征不同，He 等人（He et al. 2021）主张通过残差方法对非边缘部分进行监督，以获得更精细的边缘。张等人（Zhang et al. 2020）提出了本地上下文关注模块，将本地上下文特征从编码器层传递到解码器层，增强了对难点区域的关注。RFENet （Fan 等人，2023 年）引入了结构焦点细化模块，以方便对边界周围的模糊点进行细粒度特征细化。EAMNet（Sun、Jiang 和 Qi，2023 年）将边缘检测和伪装物体分割视为一个相互关联的交叉细化过程。</p>
<h2 id="method"><a class="markdownIt-Anchor" href="#method"></a> Method</h2>
<p align="center">     <img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202312222016676.png" alt=" " />    <p align="left">    图 2：Polyper 的整体架构。我们使用 Swin Transformer 的 Swin-T 作为编码器。解码器分为两个主要阶段。第一个潜在边界提取（PBE）阶段旨在从编码器中捕捉多尺度特征，然后将这些特征汇总，生成初始分割结果。接下来，我们使用形态学算子提取预测息肉的潜在边界和内部区域。在第二个边界敏感细化（BSR）阶段，我们对潜在边界和内部区域之间的关系进行建模，以生成更好的分割结果。  	</p> </p>
<p>由于息肉与周围组织的对比度较低，因此从周围粘膜中准确识别息肉边界具有挑战性。为了解决这个问题，一种普遍的策略是通过完善潜在边界附近的语义特征来提高分割图的质量。然而，息肉边界与周围组织混合的模糊性质往往会阻碍对息肉边界区域的准确预测。在内窥镜筛查中，经验丰富的医疗从业人员通常会利用非边界区域内息肉的固有特征来有效解决边界模糊的问题。受这种临床方法的启发，我们提出了一种新的边界敏感框架，称为 Polyper。</p>
<p>图 2 是 Polyper 的概览。与之前大多数息肉分割工作一样，我们采用了经典的编码器-解码器架构。编码器在提取不同尺度和层次的特征方面起着至关重要的作用，使模型能够捕捉到粗略和精细的细节。我们使用 Swin Transformer（Liu 等人，2021 年）中的 Swin-T 作为编码器。解码器包括两个不同的阶段：潜在边界提取和边界敏感细化。在潜在边界提取阶段，编码器的多尺度特征汇总生成一个初始预测，用于提取预测息肉的潜在边界和内部区域。边界敏感阶段利用内部区域的独特特征，通过对潜在边界区域和内部区域之间的关系建模来提高模型的精度。下面我们将详细介绍这两个阶段。</p>
<h3 id="potential-boundary-extraction"><a class="markdownIt-Anchor" href="#potential-boundary-extraction"></a> Potential Boundary Extraction</h3>
<p>图 2 展示了潜在边界提取阶段的概况。我们使用 1 × 1 卷积预测分割图，并使用形态学算子从初始分割结果中提取边界和内部息肉区域。这一阶段可分为特征聚合和区域分离。</p>
<p>特征聚合。在特征聚合部分，我们使用 1 × 1 卷积和连接操作来聚合不同尺度的特征。给定来自编码器四个阶段的特征（表示为 E0、E1、E2 和 E31），我们首先按照（Lin 等人，2017 年）的方法构建一个特征金字塔。具体来说，我们通过线性插值调整特征图 E1、E2 和 E3 的大小，以确保它们与 E0 大小相同，从而得到 E′ 1、E′ 2 和 E′3。特征聚合过程各阶段中间层特征图 Di 的计算公式为</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231222201318437.png" alt="image-20231222201318437" /></p>
<p>其中 i∈{0, 1, 2, 3}，[- - - ] 表示连接操作。这里，D3 相当于 E′ 3，E′ 0 相当于 E0。Conv1×1 表示 1 × 1 卷积。</p>
<p>区域分离。我们引入了一个区域分离模块，用于从初始分割图中分离边界和内部息肉区域。具体来说，给定最后阶段特征聚合的输出 D0，通过 1 × 1 卷积得到初始分割掩膜 fm。然后，我们利用侵蚀算子 (E) 和扩张算子 (D)2 从初始分割掩膜中分离出边界和内部区域。每次迭代，掩膜边缘都会侵蚀或扩张一个像素。这些区域为后续的细化过程提供了重要的指导。分离过程可以写成</p>
<p><img src="/img/loading.gif" data-original="images/image-20231222201341078.png" alt="image-20231222201341078" /></p>
<p>其中 PCR 表示内部区域，PBR 表示潜在边界区域。值得注意的是，运算符 D 和运算符 E 的总运算次数是相同的。</p>
<h3 id="boundary-sensitive-refinement"><a class="markdownIt-Anchor" href="#boundary-sensitive-refinement"></a> Boundary Sensitive Refinement</h3>
<p align="center">     <img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202312222019455.png" alt=" " />    <p align="left">    图 3：边界敏感注意（BSA）模块的详细结构。这一过程分为两个并行分支，它们系统地利用了息肉在不同生长阶段的空间和通道特性。B "和 "M "表示输入 H × W 尺寸和 C 信道中息肉边界和内部区域的像素数量。  	</p> </p>
<p>如导言部分所述，不同生长阶段的息肉具有不同的特征，包括形状、纹理和颜色，这给息肉分割方法的鲁棒性带来了巨大挑战。为了解决这个问题，我们提出了边界敏感细化阶段，以细化基于 PCR 和 PBR 的边界区域特征。</p>
<p>边界敏感细化阶段如图 2 右侧所示，包括边界敏感注意模块和全阶段敏感策略。我们首先分别从边界、内部多边形和背景区域提取特征。然后，我们利用交叉注意机制来模拟边界区域和内部息肉区域之间的关系，以及内部息肉区域和背景区域之间的关系。这样就能同时对全局特征和局部特征进行编码，从而提高分割结果的质量，并获得准确的边界。完成上述过程后，不同区域对应的特征将恢复到初始位置。这样做的目的是确保有效利用硬件资源。为了实现上述过程，我们引入了一个新颖的边界敏感注意模块。此外，深层特征擅长捕捉和传递语义信息，而低层特征则善于表现复杂的几何细节。我们引入了全阶段灵敏度策略，巧妙地利用了深层和浅层特征的优势。</p>
<p><strong>边界敏感注意</strong> 边界敏感注意模块的结构如图 3 所示。它由两个分支组成，通过建立空间和通道注意力来深入探索息肉的固有特征。我们首先介绍边界敏感注意模块在空间信息编码方面的工作机制。</p>
<p>给定输入 Di、边界区域掩码 PBR 和内部息肉区域掩码 PCR，我们首先在 Di 上对 PBR 和 PCR 执行元素乘积运算，以获得与边界区域和内部息肉区域相对应的特征，分别称为 FBR 和 FCR。为了更好地发现真正的息肉边界，我们不考虑背景区域。我们将 FBR 视为查询矩阵，FCR 视为关键矩阵和值矩阵，并计算它们之间的交叉关注度如下：</p>
<p><img src="/img/loading.gif" data-original="images/image-20231222201434214.png" alt="image-20231222201434214" /></p>
<p>其中 MHCAS(-, -, -) 表示沿空间维度的多头交叉关注（Vaswani 等人，2017 年）。这一操作的目的是根据本节开头提到的观察结果，利用内部息肉区域的先验值，更准确地挖掘出真正的息肉区域。此外，由于交叉关注的计算只涉及 FBR 和 FCR，而不涉及背景区域，因此计算成本也较低。然后将结果放回 Di 的相应位置。</p>
<p>我们还考虑使用背景区域来捕捉背景与边界区域之间以及背景与内部息肉区域之间的变化和相关性。我们用 F ′ BR 和 F ′ CR 分别表示包含背景信息的边界区域特征和内部息肉区域特征。我们将 F ′ BR 视为查询矩阵，F ′ CR 视为关键矩阵和值矩阵，并计算它们之间的交叉关注度如下：</p>
<p><img src="/img/loading.gif" data-original="images/image-20231222201505589.png" alt="image-20231222201505589" /></p>
<p>其中，MHCAC(-, -, -) 表示沿通道维度的多头交叉关注，遵循（Yin 等人，2022 年；Zamir 等人，2022 年）以节省计算量。这一操作的目的是从全局角度捕捉不同区域之间的一致性和相关性。这有助于更好地理解息肉的整体结构。</p>
<p>拟议的边界敏感注意模块的输出可表述为</p>
<p><img src="/img/loading.gif" data-original="images/image-20231222201530471.png" alt="image-20231222201530471" /></p>
<p>其中 Conv1×1 是 1 × 1 卷积。</p>
<p>全阶段敏感策略。如图 2 右侧所示，边界敏感注意模块可有效利用不同尺度的特征，逐步细化边界区域。这种全阶段敏感策略的表述如下。在最深阶段，细化过程的计算方法如下：</p>
<p><img src="/img/loading.gif" data-original="images/image-20231222201555039.png" alt="image-20231222201555039" /></p>
<p>其中，BSA(-, -, -) 表示边界敏感注意模块。此外，后续阶段的细化过程可定义为</p>
<p><img src="/img/loading.gif" data-original="images/image-20231222201624907.png" alt="image-20231222201624907" /></p>
<p>最后，我们对 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">F_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 进行 1 × 1 卷积，生成最终预测结果。从以上描述可以看出，我们的解码器主要包含 1 × 1 卷积和简单的交叉注意。这使得我们的方法非常高效。</p>
<h2 id="experiments"><a class="markdownIt-Anchor" href="#experiments"></a> Experiments</h2>
<h3 id="dataset"><a class="markdownIt-Anchor" href="#dataset"></a> Dataset</h3>
<p>与之前的大多数研究一样，这些结果是在 Pranet（Fan 等人，2020 年）中使用的相同数据集上报告的，其中包括五个常用数据集：Kvasir-SEG、CVCClincDB、CVC-ColonDB、EndoScene 和 ETIS。具体来说，训练集包括来自 KvasirSEG 的 900 幅图像和来自 ClinicDB 的 550 幅图像。测试集包括 Kvasir-SEG 的 100 幅图像、CVCClincDB 的 62 幅图像、CVC-ColonDB 的 380 幅图像、EndoScene 的 60 幅图像和 ETIS 的 196 幅图像。</p>
<h3 id="implementation-details"><a class="markdownIt-Anchor" href="#implementation-details"></a> Implementation Details</h3>
<p>我们在所有实验中都使用 PyTorch（Paszke 等人，2019 年）和 mmsegmentation（Contributors，2020 年）来实现 Polyper。训练期间的输入分辨率设置为 224 × 224，批量大小设置为 6。训练期间的迭代次数为 80k。我们使用 AdamW 优化器进行训练，初始学习率为 0.0002，动量为 0.9，权重衰减为 1e-4。所有实验均在一个英伟达 RTX 3090 GPU 上进行。按照先前研究（Lin 等，2022 年）的配置，采用了 mIoU 和 mDice 评估指标。实验中的翻转数和参数计算基于 512 × 512 的输入大小，计算方法来自 mmsegmentation 项目。</p>
<h3 id="analysis-of-experimental-results"><a class="markdownIt-Anchor" href="#analysis-of-experimental-results"></a> Analysis of Experimental Results</h3>
<p align="left">表 1：与最先进方法的比较。Polyper w/o RS "表示在提取潜在边界时不使用区域分割，而是细化整个前景区域而不是边界区域。Polyper w/o BSR "表示不细化初始分割结果。
<p align="center">     <img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202312222027559.png" alt=" " />          	</p> </p>
<p>与最先进方法的比较。我们将 Polyper 的分割性能与其他最先进的模型进行了比较。表 1 列出了与基于 CNN 的方法的综合比较，包括 UNet（Ronneberger、Fischer 和 Brox，2015 年）、UNet++（Zhou 等，2018 年）、SFA（Fang 等，2019 年）、ACSNet（Zhang 等，2020 年）、PraNet（Fan 等，2020 年）和 SANet（Wei 等，2021 年）。此外，我们还比较了基于Transformer的方法，包括 TransFuse（Zhang、Liu 和 Hu，2021 年）、HarDNet-MSEG（Huang、Wu 和 Lin，2021 年）、DSTransUNet（Lin 等，2022 年）、Polyp-PVT（Dong 等，2021 年）和 SwinENet。2021 年）、SwinE-Net（Park 和 Lee 2022 年），以及基于细化的方法，如 CaraNet（Lou 等 2022 年）、ColonFormer（Thanh Duc 等 2022 年）和 SegT（Chen、Ma 和 Zhang 2023 年）。如表 1 所示，我们提出的边界敏感方法 Polyper 明显优于其他列出的方法。可视化结果如图 5 所示，从中可以看出，我们提出的方法在边界处理和小息肉处理方面优于之前的方法。</p>
<p>小息肉分析。我们还对小息肉的性能进行了评估。这类息肉往往出现在疾病的初期，对比度较低（Antonelli 等人，2021 年）。具体来说，我们采用 CaraNet（Lou 等人，2022 年）中提出的方法，重点评估占整个图像 6% 以下的息肉。在该实验中，我们与基于 CNN 的方法 PraNet（Fan 等人，2020 年）、基于Transformer的方法 DSTransUNet（Lin 等人，2022 年）和基于细化的方法 CaraNet（Lou 等人，2022 年）进行了比较。结果如图 4 所示。可以看出，由于采用了边界敏感策略，Polyper 在小息肉中的表现优于其他方法。值得注意的是，Polyper 甚至优于专门为小息肉目标设计的方法 CaraNet。</p>
<p align="center">     <img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202312222030955.png" alt=" " />    <p align="left">    图 4：五个数据集上小息肉的性能。比例大小 "是指息肉的大小与整个图像的比例。  	</p> </p>
<p align="center">     <img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202312222030460.png" alt=" " />    <p align="left">    图 5：不同方法在 Kvasir、ClinicDB 和 ColonDB 数据集上的分割结果。  	</p> </p>
<h3 id="ablation-study"><a class="markdownIt-Anchor" href="#ablation-study"></a> Ablation Study</h3>
<p align="left"> 表 2：不同编码器和特征聚合方法的消融情况。Polyper w/o BSR "表示不细化初始分割结果
<p align="center">     <img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202312222026113.png" alt=" " />          	</p> </p>
<p>我们在 Kvasir 数据集上进行了大量消融实验，以分析 Polyper。</p>
<p>对编码器的消融。首先，我们进行实验来评估不同编码器的影响。我们选择常用的 ResNet-50（He 等人，2016 年）和 MiTB1（Xie 等人，2021 年）作为评估编码器。如表 2 所示，当使用我们的解码器和 ResNet-50 作为编码器时，与不改进初始分割结果相比，mIoU 和 mDice 分别显著提高了 2.22 和 1.16。此外，使用 MiT-B1 作为编码器也能带来显著提升。这表明 Polyper 可广泛应用于各种编码器。</p>
<p>消融对潜在边界提取的影响。我们首先评估了不同特征聚合方法的影响。为此，我们考虑了非局部块（Wang 等人，2018 年）和 Hamburger（Geng 等人，2021 年）。如表 2 所示，我们提出的边界敏感方法 Polyper 兼容不同的特征聚合方法，这反映了我们方法的泛化能力。此外，值得注意的是，当使用非本地块和 Hamburger 进行特征聚合时，其性能并没有超过我们提出的特征聚合方法。我们认为这是由于这两种方法最初是为自然图像的语义分割而设计的。鉴于可用于分割的医疗数据量有限，以及由此带来的网络融合挑战，这两种方法在医疗分割中的表现仍不能令人满意。</p>
<p align="left">表 3：区域分割的消蚀。其中，"迭代次数 "表示侵蚀算子的迭代次数。
<p align="center">     <img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202312222028422.png" alt=" " />          	</p> </p>
<p>此外，我们还通过实验分析了区域分割（RS）的有效性。在没有 RS 的情况下，随后的细化阶段会使用完整的掩码来涵盖整个前景区域。如表 3 所示，与使用完整掩膜的细化相比，使用 RS 的细化在 mIoU 和 mDice 上分别提高了 4.40 和 2.59。这是因为初始分割结果的边界区域包含了不可靠的低置信度特征。当细化整个前景时，会受到这些不可靠特征的影响，从而降低分割结果的质量。</p>
<p>最后，我们对边界区域的宽度进行了实验，结果如表 3 所示。边界区域的宽度是通过在应用侵蚀算子后的掩膜和应用扩张算子后的掩膜之间进行减法运算来确定的，计算方法遵循（Zhu, Qiao, and Yang 2023）。在本实验中，我们重点评估了侵蚀算子迭代次数的影响。从表中可以看出，侵蚀算子的最佳迭代次数为 4。</p>
<p><img src="/img/loading.gif" data-original="images/image-20231222202910561.png" alt="image-20231222202910561" /></p>
<p>烧蚀对边界敏感的细化。在这里，我们通过实验来验证边界敏感细化阶段的重要性。首先，我们评估了空间注意力和通道注意力这两个分支的重要性。表 4 清楚地说明了空间注意力和通道注意力的贡献。这说明了充分利用边界区域与内部息肉区域之间的关系以及内部息肉区域与背景区域之间的关系来生成具有精确边界的息肉区域的有效性。</p>
<p>然后，我们评估全阶段敏感策略的重要性。该实验的目的是验证是否有必要在特征聚合部分细化所有特征。表 5 说明了充分利用不同层次的特征来提高具有准确边界的分割结果质量的有效性。我们可以看到，逐渐加入更多低层次的特征可以不断提高模型的性能。</p>
<p align="center">     <img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202312222031760.png" alt=" " />
<p align="left" >    图 6：不同版本 Polyper 生成的特征图的视觉分析。所有图像均选自 Kvasir 数据集。(a) 无边界敏感度细化；(b) 无区域分割；(c) 无边界敏感度关注；(d) 无通道关注的 Polyper；(e) 无空间关注；(f) Polyper；(g) 无边界敏感度的分割结果；(h) 无区域分割的分割结果；(i) 使用 Polyper 的分割结果；(j) 地面实况注释。  	</p> </p>
<p>可视化分析。为了更全面地了解我们提出的 Polyper 中每个组件的功能，我们利用（Komodakis 和 Zagoruyko，2017 年）中提出的可视化方法对不同版本 Polyper 生成的输出特征图进行了可视化。可视化结果如图 6 所示。从图 6(b)可以看出，当从全局角度完善初始结果时，这种方法效果有限，无法解决由于低置信度预测的边界区域存在干扰而导致的边缘模糊问题。相反，从图 6(f)中可以看出，如果对内部息肉区域和边界区域之间的关系以及内部息肉区域和背景区域之间的关系进行建模，以区分病变区域的边界，我们的方法就会非常有效。</p>
<p>Polyper 的局限性。我们在图 7 中展示了 Polyper 的一些失败案例。首先，我们假设息肉定位是准确的，因此无法很好地处理假阳性或假阴性。其次，我们采用固定宽度法来定义边界宽度和提取边界区域。这可能无法解释息肉特征的多样性。未来，我们将探索自适应边界宽度方法。</p>
<h2 id="conclusion"><a class="markdownIt-Anchor" href="#conclusion"></a> Conclusion</h2>
<p>我们提出了一种新颖的息肉分割方法 Polyper。我们利用形态学算子从初始分割结果中划分出息肉的边界和内部区域。然后，我们利用息肉内部区域的特征来增强边界区域的特征。我们在五个数据集上的实验证明了 Polyper 的卓越性能。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ParcNet/" title="ParC-Net"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">ParC-Net</div></div><div class="info-2"><div class="info-item-1"> ParC-Net ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/PIDNet/" title="PIDNet A Real-time Semantic Segmentation Network Inspired by PID Controllers"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">PIDNet A Real-time Semantic Segmentation Network Inspired by PID Controllers</div></div><div class="info-2"><div class="info-item-1"> PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers  Abstract  1. Introduction Proportional-Integral-Derivative (PID) Controller is a classic concept that has been widely applied in modern dynamic systems and processes such as robotic manipulation [3], chemical processes [24], and power systems [25]. Even though many advanced control strategies with better control performance have been developed in recent years, PID controller is still the go-to choice for most...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">CA</div></div><div class="info-2"><div class="info-item-1"> Coordinate Attention for Efficient Mobile Network Design  Abstract 最近关于移动网络设计的研究已经证明了通道注意（如SE注意力机制）对于提高模型性能的显著效果，但他们通常忽略了位置信息，而位置信息对于生成空间选择性注意图是很重要的。在本文中，我们提出了一种新的移动网络注意机制，将位置信息嵌入到通道注意中，我们称之为...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#polyper"><span class="toc-text"> Polyper</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-text"> Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#related-work"><span class="toc-text"> Related Work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#method"><span class="toc-text"> Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#potential-boundary-extraction"><span class="toc-text"> Potential Boundary Extraction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#boundary-sensitive-refinement"><span class="toc-text"> Boundary Sensitive Refinement</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experiments"><span class="toc-text"> Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#dataset"><span class="toc-text"> Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#implementation-details"><span class="toc-text"> Implementation Details</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#analysis-of-experimental-results"><span class="toc-text"> Analysis of Experimental Results</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ablation-study"><span class="toc-text"> Ablation Study</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusion"><span class="toc-text"> Conclusion</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA">CA</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>