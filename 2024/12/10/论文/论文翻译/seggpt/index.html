<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>SegGPT | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="SegGPT  Abstract 我们提出的 SegGPT 是一种用于在上下文中分割一切的通用模型。我们将各种分割任务统一到一个通用的上下文学习框架中，该框架通过将不同种类的分割数据转换为相同格式的图像，来适应不同种类的分割数据。SegGPT 的训练被表述为对每个数据样本进行随机颜色映射的上下文着色问题。目的是根据上下文完成不同的任务，而不是依赖特定的颜色。经过训练后，SegGPT 可以通过上下">
<meta property="og:type" content="article">
<meta property="og:title" content="SegGPT">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/seggpt/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="SegGPT  Abstract 我们提出的 SegGPT 是一种用于在上下文中分割一切的通用模型。我们将各种分割任务统一到一个通用的上下文学习框架中，该框架通过将不同种类的分割数据转换为相同格式的图像，来适应不同种类的分割数据。SegGPT 的训练被表述为对每个数据样本进行随机颜色映射的上下文着色问题。目的是根据上下文完成不同的任务，而不是依赖特定的颜色。经过训练后，SegGPT 可以通过上下">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:03:46.247Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/seggpt/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'SegGPT',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">SegGPT</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">SegGPT</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:03:46.247Z" title="更新于 2024-12-11 01:03:46">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">6.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>19分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="seggpt"><a class="markdownIt-Anchor" href="#seggpt"></a> SegGPT</h1>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>我们提出的 SegGPT 是一种用于在上下文中分割一切的通用模型。我们将各种分割任务统一到一个通用的上下文学习框架中，该框架通过将不同种类的分割数据转换为相同格式的图像，来适应不同种类的分割数据。SegGPT 的训练被表述为对每个数据样本进行随机颜色映射的上下文着色问题。目的是根据上下文完成不同的任务，而不是依赖特定的颜色。经过训练后，SegGPT 可以通过上下文推理执行图像或视频中的任意分割任务，如对象实例、东西、部分、轮廓和文本。SegGPT 在广泛的任务中进行了评估，包括小样本语义分割、视频对象分割、语义分割和全景分割。结果表明，无论是定性还是定量分析，SegGPT 都能很好地分割域内和域外目标。</p>
<h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1. Introduction</h2>
<p>分割是计算机视觉领域最基本的问题之一，其目的是在像素级别定位和重新组织有意义的概念，例如前景、类别、对象实例等。近年来，我们在为各种分割任务开发更准确、更快速的算法方面取得了巨大进步，例如前景分割 [41]、交互式分割 [51, 34]、语义分割 [32, 28, 54, 39]、实例分割 [18, 11, 2, 48] 和全景分割 [23, 5, 8]。</p>
<p>然而，这些专业分割模型仅限于特定的任务、类别、粒度和数据类型等。如果要适应不同的环境，例如分割一个新概念，或分割视频而不是图像中的物体，就必须训练一个新的模型。这需要昂贵的标注工作，而且对于大量的分割任务来说是不可持续的。</p>
<p>在这项工作中，我们的目标是训练一个单一的模型，使其能够解决多样化、无限制的分割任务。主要的挑战有两个方面：（1）在训练中纳入那些非常不同的数据类型，例如，部件、语义、实例、全景、人物、医疗图像、航空图像等；（2）设计一种不同于传统多任务学习的通用训练方案，这种方案在任务定义上非常灵活，能够处理域外任务。</p>
<p>为了应对这些挑战，我们提出了 SegGPT，这是一种用于分割上下文中所有内容的通用模型。我们将分割视为视觉感知的一种通用格式，并将不同的分割任务统一到一个通用的上下文学习框架中[46]。该框架通过将不同类型的分割数据转换为相同格式的图像，从而将它们容纳进来。SegGPT 的训练被表述为对每个数据样本进行随机颜色映射的上下文着色问题。其目标是仅根据上下文为相应区域（如类、对象实例、部件等）着色。通过使用随机着色方案，模型不得不参考上下文信息来完成指定任务，而不是依赖特定的颜色。这使得训练方法更加灵活和通用。训练的其余部分与 [46] 相同，使用 vanilla ViT [42] 和简单的 smooth-ℓ1 [17] loss。</p>
<p>经过训练后，SegGPT 能够通过上下文推理，在给定几个例子的情况下执行图像或视频中的各种分割任务，如物体实例、东西、部分、轮廓、文本等。为了有效地集合多个上下文实例，我们提出了一种简单而有效的上下文集合策略–特征集合，它可以帮助模型从多实例提示设置中获益。此外，SegGPT 还可以在不更新模型参数的情况下，通过调整特定的提示，方便地充当专业模型，用于专门的使用案例，例如域内 ADE20K 语义分割。</p>
<p>我们的主要贡献如下：</p>
<p>(1) 我们首次展示了一个能够自动执行各种分割任务的通用模型。</p>
<p>(2) 我们在广泛的任务中直接评估了预训练的 SegGPT，即无需微调，包括小样本语义分割、视频对象分割、语义分割和全视角分割。</p>
<p>(3) 我们的研究结果表明，无论是从定性还是定量的角度来看，我们都具有很强的分割域内和域外目标的能力。</p>
<p>不过，这项工作的目的并不在于宣称取得了新的最先进成果，或在所有基准上都优于现有的专业方法，因为我们认为这可能不是通用模型的职责所在。</p>
<h2 id="2-related-work"><a class="markdownIt-Anchor" href="#2-related-work"></a> 2. Related Work</h2>
<h3 id="21-visual-segmentation"><a class="markdownIt-Anchor" href="#21-visual-segmentation"></a> 2.1. Visual Segmentation</h3>
<p>分割是计算机视觉中的一个基本问题，涉及在像素级别定位和组织有意义的概念。分割任务的类型因前景、类别或对象实例等概念的定义而异。例如，语义分割 [55] 涉及图像的像素级语义分类，而实例分割 [30] 则旨在识别不同的物体实例及其类别。视频对象分割[52, 37, 12]是指在整个视频序列中，仅根据第一帧的对象掩码对特定对象进行分割。</p>
<p>以往的分割方法[32, 28, 54, 39, 18, 11, 2, 48, 23, 5, 8]都是专门针对某些任务而设计的，无法通用于切换任务或改变类别。本文介绍了一种通用界面，该界面可兼容所有分割任务，只要有适当的训练方案，一个通用模型就能在域内和域外分割任务中取得良好的定性或定量性能。</p>
<h3 id="22-vision-generalist"><a class="markdownIt-Anchor" href="#22-vision-generalist"></a> 2.2. Vision Generalist</h3>
<p>近年来，人们一直在努力使用基于transformer的模型来统一视觉领域的不同任务，从而产生了一些视觉通用头[6, 7, 56, 33, 24]。DETR [5] 是最早采用 Transformer [42] 作为物体检测任务专用头的软件之一。Pix2Seq 系列 [6, 7] 将视觉任务的输出空间定义为离散空间，并以自动回归的方式执行物体检测、实例分割、关键点估算和图像字幕等任务。Unified-IO [33] 和 OFA [45] 以序列到序列的方式对视觉、视觉与语言和 NLP 任务进行联合建模，即输入和输出都定义为离散标记序列。UViM[24]将像素标记任务统一在一起，如全景分割、深度估计和着色，但对每个任务都训练单独的模型。</p>
<p>虽然这些研究表面上看都是将不同的任务统一到相似的空间中，但实际上它们都是通过某种形式的硬指标（如特殊标记）来完成每项任务的，因此很难推广到新的任务中。与此相反，这项研究采用了一种上下文框架，保持了任务定义的灵活性，并利用随机着色方案防止模型坍缩为多任务学习解决方案，而是迫使它通过参考上下文信息来完成分配的任务。另一个不同之处在于任务的范围。这项工作主要侧重于视觉感知中的一个重要类别，即图像分割。</p>
<h3 id="23-in-context-visual-learning"><a class="markdownIt-Anchor" href="#23-in-context-visual-learning"></a> 2.3. In-Context Visual Learning</h3>
<p>GPT-3 [3]将上下文学习的概念引入了深度学习，它允许将一系列 NLP 任务表述为给定提示和示例的文本完成问题。在计算机视觉领域，[1] 首次提出了一种上下文训练框架，利用离散标记对视觉文章中的数字和信息图进行内绘，展示了该框架在前景分割、单个物体检测和着色方面的能力。Painter[46]在连续像素上采用遮蔽图像建模，利用有监督数据集对七种不同的、具有挑战性的视觉任务进行上下文训练，在这些任务上取得了极具竞争力的结果。</p>
<p>我们的工作以 Painter 框架为基础，但特别关注分割任务，因为它在视觉感知中起着核心作用。因此，这项工作统一了不同的分割数据，包括语义分割、实例分割、部件分割，甚至还有航空图像等特殊场景的分割数据。此外，我们还设计了一种随机着色方案，迫使模型参考上下文信息来完成指定任务，但又不会塌陷到多任务解决方案中。与深度/姿态估计相比，分割任务和数据集的可变性较小，因此更容易共享内部结构，从而对域内任务进行有效训练，同时保持对域外分割任务的泛化能力。</p>
<h2 id="3-approach"><a class="markdownIt-Anchor" href="#3-approach"></a> 3. Approach</h2>
<p>SegGPT 是 Painter [46] 框架的一个特殊版本，它可以用通用的 Painter 对万物进行分割，因此我们的模型被命名为 SegGPT。该训练框架将视觉任务的输出空间重新定义为 “图像”，并将不同任务统一为同一个图像涂色问题，即随机屏蔽任务输出图像并重建缺失像素。为了保持简单性和通用性，我们没有修改架构和损失函数，即 vanilla ViT [13] 和简单的 smooth-ℓ1 [17] 损失，但在上下文训练中设计了一种新的随机着色方案，以获得更好的泛化能力。</p>
<h3 id="31-in-context-coloring"><a class="markdownIt-Anchor" href="#31-in-context-coloring"></a> 3.1. In-Context Coloring</h3>
<p>在传统的 Painter 框架中，每项任务的色彩空间都是预先定义好的，这就导致解决方案坍缩为多任务学习。例如，在语义分割中，预先定义了一组颜色，并为每个语义类别分配了固定的颜色。同样，在实例分割中，实例对象的颜色是根据其位置类别分配的，即颜色的数量等于空间位置的数量，这导致模型只能依靠颜色本身来确定任务，而不是利用分割之间的关系。</p>
<p>为了解决这一局限性，我们提出了一种用于上下文着色的随机着色方案。首先，我们随机抽样另一张与输入图像具有相似上下文的图像，如相同的语义类别或对象实例。接着，我们从目标图像中随机取样一组颜色，并将每种颜色映射到一个随机颜色上。这将导致相应像素的重新着色。这样，我们就得到了两对图像，这两对图像被定义为 “上下文对”。此外，我们还引入了混合上下文训练法，使用混合示例来训练模型。这包括将具有相同颜色映射的多幅图像拼接在一起。然后随机裁剪和调整生成的图像，形成混合上下文训练样本。通过这种方法，模型可以学会关注图像的上下文信息，而不是仅仅依靠特定的颜色信息来确定任务。</p>
<p>这种统一使我们能够以一致的方式利用所有分割数据集，只需根据具体任务改变数据采样策略即可。我们根据不同的数据类型定义不同的情境。对于语义分割，我们对类别进行随机抽样。对于实例分割，对象实例以随机数取样。同一图像的不同视图（例如，通过一组增强功能转换的图像）被视为上下文中的图像。在实施过程中，采样都是以颜色为单位，例如，相同的颜色指的是相同的类别或相同的实例。</p>
<h3 id="32-context-ensemble"><a class="markdownIt-Anchor" href="#32-context-ensemble"></a> 3.2. Context Ensemble</h3>
<p>一旦完成训练，它就能在推理过程中释放出全部能量。SegGPT 可在上下文中进行任意分割，例如，以单幅图像及其目标图像为例。目标图像可以是单一颜色（不包括背景），也可以是多种颜色，例如，在一个镜头中分割多个类别或感兴趣的对象。具体来说，在给定待测输入图像后，我们将其与示例图像拼接，并输入 SegGPT 以获得相应的上下文预测。</p>
<p>为了提供更准确、更具体的上下文，可以使用多个示例。例如，可以使用同一语义类别的多个示例或视频中的前几帧。为了在 SegGPT 模型中有效利用多个示例，我们提出了两种上下文集合方法。一种称为 “空间集合”（Spatial Ensemble），即将多个示例按 n × n 网格串联起来，然后再进行子采样，使其大小与单个示例相同。这种方法与上下文着色的直觉相吻合，多个示例的语义信息可以在几乎不增加额外成本的情况下进行上下文提取。另一种方法是特征集合。多个示例在批量维度上进行组合，除了在每个注意层之后对查询图像的特征进行平均之外，其他计算都是独立的。这样，查询图像就能在推理过程中收集多个示例的信息。</p>
<h3 id="33-in-context-tuning"><a class="markdownIt-Anchor" href="#33-in-context-tuning"></a> 3.3. In-Context Tuning</h3>
<p>SegGPT 能够在不更新模型参数的情况下适应独特的使用情况。我们冻结整个模型，并初始化一个可学习的图像张量作为输入上下文。在训练过程中，只更新这个可学习的图像张量。训练的其他部分保持不变，例如相同的损失函数。调整完成后，我们将学习到的图像张量取出，并将其作为特定应用的即插即用密钥。例如，给定一个具有固定对象类别集（如 ADE20K）的数据集，我们可以为该数据集训练一个定制的提示，同时不会损害模型的通用性。或者，我们可以针对特定场景（如您的公寓）或特定人物（如伯特的脸）优化提示图像。这就为广泛的应用提供了机会。</p>
<h2 id="4-experiment"><a class="markdownIt-Anchor" href="#4-experiment"></a> 4. Experiment</h2>
<h3 id="41-training-data"><a class="markdownIt-Anchor" href="#41-training-data"></a> 4.1. Training Data</h3>
<p>我们的方法使用不同的分割数据集，包括部件、语义、实例、全景、人物、视网膜血管和航空图像分割。与以往依赖手工标签合并来组合不同类型的分割数据集的方法不同，我们的方法提供了一个统一的视角，无需对数据集进行额外的工作或调整。特别是，在添加额外数据集时，我们的方法无需对架构或训练管道进行任何修改。</p>
<p><strong>ADE20K</strong>[55]为150个语义类别提供了分割标签，共有25K张图像，包括20K张训练图像、2K张验证图像和3K张测试图像。</p>
<p><strong>COCO</strong>[30]是一个广泛使用的视觉感知数据集，支持实例分割、语义分割和泛视分割。它包含 118K 张训练图像和 5K 张验证图像，其中有 80 个 &quot;事物 &quot;类别和 53 个 &quot;物品 &quot;类别。</p>
<p><strong>PASCAL VOC</strong> [14] 是一个经典的物体识别数据集。我们使用的是增强分割版本，它在 10582 张训练图像上提供了 20 个类别的注释。</p>
<p><strong>Cityscapes</strong> [10] 侧重于街景的场景理解。我们使用的是 2954 幅训练图像，其中包含 19 个类别的语义分割注释。</p>
<p><strong>LIP</strong> [26] 侧重于对人物的语义理解。我们使用了 30385 张带有 19 个人体部位类别分割标签的训练图像。</p>
<p><strong>PACO</strong> [38] 是一个新发布的数据集，提供常见物体的部分和属性注释。我们处理并使用了带有部件注释的 41807 张训练图像。</p>
<p><strong>CHASE DB1</strong> [16]、<strong>DRIVE</strong> [40]、<strong>HRF</strong> [4] 和 <strong>STARE</strong> [20] 为视网膜血管分割提供了注释。我们对高分辨率原始图像进行了随机裁剪。</p>
<p><strong>SAID</strong> [49] 和 <strong>loveDA</strong> [44] 专注于航空图像的语义理解，分别为 15 和 6 个语义类别提供了 23262 和 2520 幅训练图像。</p>
<h3 id="42-one-shot-training-details"><a class="markdownIt-Anchor" href="#42-one-shot-training-details"></a> 4.2. One-Shot Training Details</h3>
<p>我们的分割任务方法采用通用界面，我们强调只用混合数据集训练一个通用模型，并在不同基准上对该模型进行评估。根据文献[46]，我们使用视觉transformer（ViT-L）编码器[13]，该编码器有 3.07 亿个参数。我们使用 [46] 中预先训练好的检查点作为初始化。我们使用 AdamW 优化器 [22] 和余弦学习率调度器，基本学习率为 1e-4。权重衰减设置为 0.05。批次大小为 2048。我们进行了 9K 次迭代训练，热身期为 1.8K 次迭代。我们使用了一组数据增强，包括随机大小裁剪、颜色抖动和随机水平翻转。单张输入图像的大小为 448 × 448。</p>
<h3 id="43-qualitative-results"><a class="markdownIt-Anchor" href="#43-qualitative-results"></a> 4.3. Qualitative Results</h3>
<p>为了从直观的角度展示 SegGPT 的能力，我们将所选图像的任务输出与专门的任务提示进行了可视化，如图 1 和图 5 所示。这两幅图包含了多种分割任务，例如具有不同粒度的任意部分/对象分割、文本分割、无视频训练的视频对象分割，以及具有可学习提示调整功能的近集实例/语义分割。图 6 展示了 YouTube-VOS 2018 数据集视频对象分割的更多可视化效果。从这些可视化效果来看，SegGPT 展示了在各种任务中进行高精度预测的能力，同时在任务定义中保持了超强的灵活性。</p>
<h3 id="44-comparison-with-specialist-methods"><a class="markdownIt-Anchor" href="#44-comparison-with-specialist-methods"></a> 4.4. Comparison with Specialist Methods</h3>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231224211249990.png" alt="image-20231224211249990" /></p>
<blockquote>
<p>表 1：COCO-20i 和 PASCAL-5i 基于实例的语义分割的量化结果。* 表示训练中的类别涵盖了测试中的类别。</p>
</blockquote>
<p>**小样本语义分割。**我们对 SegGPT 的性能进行了评估，评估采用了两种小样本语义分割设置：COCO-20i/PASCAL-5i 上的域内语义分割和 FSS-1000 上的域外语义分割。表 1 显示了 COCO20i/PASCAL-5i 上基于示例的语义分割结果。为了进行公平比较，我们还对标有 * 的域内类别的专家模型进行了评估。我们的结果表明，在这两个基准上，SegGPT 的性能可与最近发布的最先进的专家模型相媲美，甚至明显优于后者。需要注意的是，现有技术中的 FPTrans 使用不同的镜头分别训练不同的模型。此外，SegGPT 还大大超过了通用的 Painter [46]。</p>
<p>视频对象分割 视频对象分割（VOS）是一项在视频帧中分割特定对象的任务。在这项工作中，我们将重点放在半监督 VOS 设置上，并在三个数据集的验证分割上评估我们提出的方法 SegGPT： 这三个数据集是：YouTube-VOS 2018 [52]、DAVIS 2017 [37] 和最近发布的具有挑战性的基准 MOSE [12]。我们使用 VOS 中常用的两个指标进行评估：J score 和 F score，并通过官方评估服务器或工具对我们的结果进行评估。</p>
<p>SegGPT 通过将第一帧及其对象掩码转换为上下文着色示例来执行视频对象分割。在测试当前帧时，我们使用其前 K 帧（如果有）来构建多个示例。这些帧的对象掩码已被预测并存储在 FIFO 队列中。构建多个示例后，将应用特征集合（见第 3.2 节），并将预测结果存储到下一帧。我们在多个基准测试中评估了我们的模型，结果如表 3 所示。尽管我们的方法不是专门针对该任务训练的，但与在这些数据集上训练的专业模型相比，我们的方法取得了具有竞争力的结果。例如，在 YouTube-VOS 2018 [52] 上，我们的方法明显优于特定任务方法 AGAME [21] 和 AGSS [29]。在具有挑战性的 MOSE 基准（侧重于复杂场景）上，SegGPT 的表现甚至可与最先进的方法 RDE 相媲美。</p>
<h3 id="45-ablation-study"><a class="markdownIt-Anchor" href="#45-ablation-study"></a> 4.5. Ablation Study</h3>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231224211906709.png" alt="image-20231224211408843" /></p>
<blockquote>
<p>表 4：在上下文推理中对集合策略（a）和帧数（b）的消减研究。空间集合方法在 FSS-1000 数据集上表现良好，但在 DAVIS 2017 数据集上性能下降。由于没有子采样，特征集合取得了更好的结果。</p>
</blockquote>
<p>在这里，我们删除了两种上下文集合策略，即空间集合和特征集合。结果如表 4a 所示。我们的研究结果表明，空间集合方法在 FSS-1000 数据集上表现良好，但在 DAVIS 2017 数据集上性能下降。我们将其归因于空间集合对示例进行了子采样。值得注意的是，与高分辨率的 DAVIS 数据集（640×480）相比，FSS-1000 数据集的图像分辨率（224×224）较低，因此，子采样不会导致 FSS-1000 数据集的重大信息损失。而我们观察到，特征集合可以减少子采样时的这种信息损失，并在 DAVIS 2017 上取得明显更好的性能。</p>
<p>我们还消减了 DAVIS 2017 中的帧数，如表 4b 所示。随着帧数的增加，性能在达到收益递减点之前最初会有所改善。特别是，我们观察到，当使用 8 个帧时，性能达到最佳。</p>
<h3 id="46-in-context-tuning"><a class="markdownIt-Anchor" href="#46-in-context-tuning"></a> 4.6. In-Context Tuning</h3>
<p>通过上下文调整，可以使用一组数据样本定制独特的应用程序。例如，针对特定的数据集、场景甚至人物调整提示。具体来说，我们将任务提示定义为可学习张量，冻结整个模型，然后使用相同的训练损失来优化任务提示。在此，我们对具有挑战性的 ADE20K 语义分割和 COCO 全景分割进行了上下文调整。我们在相应的基准测试中评估了带有可学习提示的 SegGPT。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231224211823599.png" alt="image-20231224211823599" /></p>
<blockquote>
<p>表 5：ADE20K 语义分割结果</p>
</blockquote>
<p>ADE20K 语义分割的结果如表 5 所示。我们的模型 SegGPT 的性能与 RefineNet 等专业模型相比具有竞争力。不过，与通用模型 Painter 相比，我们的方法在 mIoU 方面下降了 10.3 个百分点。这一现象的原因在于我们引入了随机颜色方案，这使得模型将颜色作为域内任务的简单指标变得更具挑战性。相反，模型需要依靠上下文示例来确定任务，这就大大增加了优化的难度。同样，表 6 显示了我们的 SegGPT 模型在 COCO 全景分割上的结果。在这里，我们再次观察到 PQ 与通用 Painter 相比下降了 9.0 个百分点。在特定基准测试中超越所有专业方法并不是这项工作的目的，我们相信未来还有很大的改进空间。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20231224211408843.png" alt="image-20231224211906709" /></p>
<blockquote>
<p>Table 6: Results on COCO panoptic segmentation.</p>
</blockquote>
<h2 id="5-discussion-and-conclusion"><a class="markdownIt-Anchor" href="#5-discussion-and-conclusion"></a> 5. Discussion and Conclusion</h2>
<p>在这项工作中，我们提出了一种通用分割模型，展示了如何设计适当的训练策略，以充分利用上下文视觉学习的灵活性。我们的模型在处理域内和域外分割任务方面都表现出强大的能力，包括物体实例、东西、部件、轮廓、文本分割等。</p>
<p>这项工作并非没有缺点。虽然我们的工作引入了一种新的随机着色机制，以提高上下文训练的泛化能力，但它也增加了训练任务的内在难度，这可能是在训练数据充足的域内任务中表现较差的原因，例如 ADE20K 上的语义分割和 COCO 上的全视角分割。</p>
<p>展望未来，我们相信，通过利用任务定义的灵活性和上下文推理，我们的方法有可能成为图像/视频分割领域实现更多样化应用的有力工具。扩大模型规模是我们计划进一步提高性能的途径之一。有了更大的模型，就能捕捉到数据中更复杂的模式，从而获得更好的分割结果。不过，这也带来了寻找更多数据的挑战。一个潜在的解决方案是探索自监督学习技术。我们希望我们的工作能激励社区继续探索计算机视觉中上下文学习的潜力。我们仍然乐观地认为，GPT-3 在视觉领域的最佳时刻还未到来。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/%E6%95%B0%E6%8D%AE%E9%9B%86%E7%B1%BB%E5%88%AB%E7%BC%96%E5%8F%B7%E5%92%8C%E6%95%B0%E9%87%8F/" title="采样方式"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">采样方式</div></div><div class="info-2"><div class="info-item-1"> 数据集类别数量 总：   166 红：1 155 绿：2 136 黄：3 58 ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/seaformer/" title="SEAFORMER SQUEEZE-ENHANCED AXIAL TRANSFORMER FOR MOBILE SEMANTIC SEGMENTATION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">SEAFORMER SQUEEZE-ENHANCED AXIAL TRANSFORMER FOR MOBILE SEMANTIC SEGMENTATION</div></div><div class="info-2"><div class="info-item-1"> SEAFORMER: SQUEEZE-ENHANCED AXIAL TRANSFORMER FOR MOBILE SEMANTIC SEGMENTATION 文章链接  ABSTRACT 自从引入transformer后，许多计算机视觉任务（如语义分割）的格局，最近被CNN压倒性地主宰，发生了显著的变化。然而，计算成本和内存要求使得这些方法不适合在移动设备上使用，特别是对于高分辨率的每像素语义分割任务。在本文中，我们介绍了一种用于移动语义分割的新方法挤压增强轴向变换器（SeaFormer）。具体来说，我们设计了一个通用的注意力块，其特点是制定挤压轴和细节增强。它可以进一步用于创建一个具有卓越成本效益的骨干架构系列。与轻型分割头相结合，我们在ADE20K和Cityscapes数据集上基于ARM的移动设备上实现了分割精度和延迟之间的最佳权衡。最关键的是，我们以更好的性能和更低的延迟击败了对移动设备友好的对手和基于transformer的同行，而没有任何花哨的东西。除了语义分割之外，我们还将提出的SeaFormer架构应用于图像分类问题，展示了其作为多功能移动友好骨干的潜力。  1...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BAM</div></div><div class="info-2"><div class="info-item-1"> BAM  Abstract 近来，小样本分割分割技术（FSS）得到了广泛的发展。然而，训练出来的模型偏向于所看到的类别，而不是理想的类别无关性，从而阻碍了对新内容的识别。本文提出了一种新颖而直接的见解来缓解这一问题。具体来说，我们在传统的 FSS 模型（元学习器）上增加了一个分支（基础学习器），以明确识别基础类别的目标，即不需要分割的区域。然后，对这两个学习器并行输出的粗略结果进行自适应整合，从而得出精确的分割预测结果。考虑到元学习器的敏感性，我们进一步引入了一个调整因子来估计输入图像对之间的场景差异，以促进模型的集合预测。在 PASCAL-5i 和 COCO-20i 上取得的显著性能提升验证了这一方法的有效性，而且令人惊讶的是，即使使用两个普通学习器，我们的多功能方案也创造了新的一流水平。此外，鉴于所提方法的独特性，我们还将其扩展到了更现实但更具挑战性的环境中，即广义 FSS，在这种环境中，基础类和新类别的像素都需要确定。  1. Introduction   得益于成熟的大规模数据集 [8, 9,...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#seggpt"><span class="toc-text"> SegGPT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-introduction"><span class="toc-text"> 1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-related-work"><span class="toc-text"> 2. Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-visual-segmentation"><span class="toc-text"> 2.1. Visual Segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-vision-generalist"><span class="toc-text"> 2.2. Vision Generalist</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#23-in-context-visual-learning"><span class="toc-text"> 2.3. In-Context Visual Learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-approach"><span class="toc-text"> 3. Approach</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-in-context-coloring"><span class="toc-text"> 3.1. In-Context Coloring</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-context-ensemble"><span class="toc-text"> 3.2. Context Ensemble</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-in-context-tuning"><span class="toc-text"> 3.3. In-Context Tuning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-experiment"><span class="toc-text"> 4. Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-training-data"><span class="toc-text"> 4.1. Training Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-one-shot-training-details"><span class="toc-text"> 4.2. One-Shot Training Details</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-qualitative-results"><span class="toc-text"> 4.3. Qualitative Results</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#44-comparison-with-specialist-methods"><span class="toc-text"> 4.4. Comparison with Specialist Methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#45-ablation-study"><span class="toc-text"> 4.5. Ablation Study</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#46-in-context-tuning"><span class="toc-text"> 4.6. In-Context Tuning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-discussion-and-conclusion"><span class="toc-text"> 5. Discussion and Conclusion</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/11/%E5%B0%8F%E7%B1%B3/syzkaller/syzkaller01-%E9%85%8D%E7%BD%AE/" title="syzkaller01-配置">syzkaller01-配置</a><time datetime="2024-12-10T17:10:45.000Z" title="发表于 2024-12-11 01:10:45">2024-12-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM">BAM</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>