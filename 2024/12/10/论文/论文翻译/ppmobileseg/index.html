<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>PP-MobileSeg Explore the Fast and Accurate Semantic Segmentation Model on Mobile Devices | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="PP-MobileSeg: Explore the Fast and Accurate Semantic Segmentation Model on Mobile Devices 文章链接  1. Abstract transformer在计算机视觉中的成功导致了一些尝试，以使其适用于移动设备，但其性能在一些实际应用中仍然不令人满意。为了解决这个问题，我们提出了PP-MobileSeg，一个在移">
<meta property="og:type" content="article">
<meta property="og:title" content="PP-MobileSeg Explore the Fast and Accurate Semantic Segmentation Model on Mobile Devices">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ppmobileseg/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="PP-MobileSeg: Explore the Fast and Accurate Semantic Segmentation Model on Mobile Devices 文章链接  1. Abstract transformer在计算机视觉中的成功导致了一些尝试，以使其适用于移动设备，但其性能在一些实际应用中仍然不令人满意。为了解决这个问题，我们提出了PP-MobileSeg，一个在移">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:03:56.538Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ppmobileseg/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'PP-MobileSeg Explore the Fast and Accurate Semantic Segmentation Model on Mobile Devices',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">PP-MobileSeg Explore the Fast and Accurate Semantic Segmentation Model on Mobile Devices</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">PP-MobileSeg Explore the Fast and Accurate Semantic Segmentation Model on Mobile Devices</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:03:56.538Z" title="更新于 2024-12-11 01:03:56">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">5.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="pp-mobileseg-explore-the-fast-and-accurate-semantic-segmentation-model-on-mobile-devices"><a class="markdownIt-Anchor" href="#pp-mobileseg-explore-the-fast-and-accurate-semantic-segmentation-model-on-mobile-devices"></a> PP-MobileSeg: Explore the Fast and Accurate Semantic Segmentation Model on Mobile Devices</h1>
<p><a href="zotero://open-pdf/library/items/RR79NDET">文章链接</a></p>
<h2 id="1-abstract"><a class="markdownIt-Anchor" href="#1-abstract"></a> 1. Abstract</h2>
<p>transformer在计算机视觉中的成功导致了一些尝试，以使其适用于移动设备，但其性能在一些实际应用中仍然不令人满意。为了解决这个问题，我们提出了PP-MobileSeg，一个在移动设备上实现最先进性能的语义分割模型。PP-MobileSeg包括三个新的部分：<strong>StrideFormer主干</strong>、<strong>聚合注意力模块（AAM）<strong>和</strong>有效插值模块（VIM）</strong>。<strong>四个阶段的StrideFormer主干是用MV3块和strided SEA注意力构建的，它能够以最小的参数开销提取丰富的语义和细节特征</strong>。<strong>AAM首先通过语义特征集合投票过滤细节特征，然后将其与语义特征相结合，以增强语义信息</strong>。此外，我们提出了VIM，将下采样的特征上采样到输入图像的分辨率。它只对最终预测中存在的类进行插值，从而大大降低了模型的延迟，而插值是导致整体模型延迟的最主要因素。广泛的实验表明，与其他方法相比，PP-MobileSeg在准确性、模型大小和延迟之间实现了卓越的权衡。在ADE20K数据集上，PP-MobileSeg比SeaFormer-Base在高通骁龙855上以32.9%的参数和42.3%的加速实现了1.57%的mIoU准确性。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230509151508213.png" alt="image-20230509151508213" /></p>
<blockquote>
<p>图1.我们展示了我们提出的PP-MobileSeg模型在ADE20K验证集上的准确性-延迟-参数分析。权衡分析以气泡图表示，其中x轴表示延迟，y轴表示mIoU。相同颜色的模型来自同一个模型系列。我们的模型实现了更好的准确性-延迟-参数的权衡。请注意，延迟是在高通骁龙855 CPU上使用PaddleLite，以单线程和512x512作为输入形状，用最终的ArgMax算子测试的。</p>
</blockquote>
<h2 id="2-introduction"><a class="markdownIt-Anchor" href="#2-introduction"></a> 2. Introduction</h2>
<ul>
<li>
<p>与图像分类[18]或物体检测[37]等其他计算机视觉任务相比，语义分割是一项计算成本很高的任务，因为它涉及预测每个像素的类别。虽然在GPU设备上的语义分割已经取得了重大进展，但很少有研究解决移动语义分割的挑战[12, 27, 33]。这种研究的缺乏阻碍了语义分割在移动应用中的实际应用。</p>
<p>最近，transformer（ViTs）[10]的兴起证明了基于transformer的cnn在语义分割上的良好表现[5, 24, 31, 35]。多项工作提出了用于轻量级cnn设计的transformer-cnn混合架构，如MobileViT[21]、MobileFormer[4]和EdgeNext[20]。这种混合架构以最低的成本结合了cnn中的全局和局部信息。然而，多头自我注意（MHSA）的计算复杂性使得这些网络很难被部署在移动设备上。尽管已经做出了一些努力来降低时间的复杂性，包括移窗注意[16]、有效注意[23]、外部注意[29]、轴向注意[28]、SEA注意[27]等。但是这些技术中有许多需要复杂的索引操作，而ARM CPU无法支持[27]。除了延迟和准确性，内存存储也是移动应用的一个关键因素，因为移动设备的内存存储是有限的。因此，基本问题出现了：我们能否为移动设备设计一个混合网络，在参数、延迟和精度之间有一个卓越的权衡？</p>
<p>在这项工作中，我们通过探索模型大小和速度限制下的移动分割架构来解决上述问题，以实现性能的飞跃。在广泛的搜索下，我们设法提出了三个新颖的设计模块：四级骨干StrideFormer、特征融合块AAM和上采样模块VIM，如图2所示。通过结合这些模块，我们提出了一个名为PP-MobileSeg的SOTA移动语义分割网络系列，它非常适合移动设备，具有很好的参数、延迟和准确性平衡。我们改进的网络设计使PPMobileSeg-Base的推理速度提高了40%，模型大小比SeaFormer少了34.9%，同时保持了具有竞争力的1.37高的mIoU（表1）。与MobileSeg-MV3相比，PP-MobileSeg-Tiny实现了3.13个更高的mIoU，同时速度快45%，体积小49.5%（表1）。我们还评估了PP-MobileSeg在Cityscapes数据集[6]上的表现（表2），这显示了它在高分辨率输入上的模型性能优势。虽然PP-MobileSeg-Base的延迟时间稍长，但它保持了模型大小的优势，同时在mIoU上比SeaFormer[27]在城市景观数据集[6]上高出1.96。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230509151602110.png" alt="image-20230509151602110" /></p>
<blockquote>
<p>图2.PP-MobileSeg网络的结构。AAM的结构在图的右上方。普通插值模块和VIM的区别显示在图的右下方。通过只选择存在于最终预测中的类别，VIM通过对少数通道进行上采样，大大降低了延迟。</p>
</blockquote>
<p><img src="/img/loading.gif" data-original="images/image-20230509152849090.png" alt="image-20230509152849090" /></p>
<blockquote>
<p>Table 1. Results on ADE20K validation set. Latency is measured with PaddleLite with the final ArgMax operator on Qualcomm Snapdragon 855 CPU) and 512x512 as the input shape. All result is evaluated with a single thread. The mIoU is reported with single-scale inference.</p>
</blockquote>
<p><img src="/img/loading.gif" data-original="images/image-20230509153004354.png" alt="image-20230509153004354" /></p>
<blockquote>
<p>表2.Cityscapes验证集的结果。我们使用PaddleLite与最后的ArgMax算子在一个基于ARM的设备上测量延迟，该设备有一个Qualcomm Snapdragon 855 CPU，输入形状为1024x512。由于Cityscapes数据集中的类的数量很少，我们没有使用有效插值模块（VIM）。所有的结果都是使用单线程评估的，我们使用单尺度推理报告了平均交集大于联盟（mIoU）。</p>
</blockquote>
<p>综上所述，我们的贡献如下：</p>
</li>
<li>
<p>我们引入了StrideFormer，这是一个带有MobileNetV3块的四级骨架，可以有效地提取不同感受野的特征，同时最大限度地减少参数开销。我们还对最后两个阶段的输出应用了strided SEA attention[13, 27]，以改善计算限制下的全局特征表示。</p>
</li>
<li>
<p>我们提出了聚合注意模块（AAM），它通过增强语义特征的集合投票来融合骨干的特征，并以最大感受野的语义特征进一步增强融合后的特征。</p>
</li>
<li>
<p>为了减少最终插值和ArgMax操作造成的巨大延迟，我们设计了有效插值模块（VIM），在推理时间内只对最终预测中存在的类进行上采样。用VIM取代最后的插值和ArgMax操作，大大降低了模型的延迟。</p>
</li>
<li>
<p>我们将上述模块结合起来，创建了一个名为PPMobileSeg的SOTA移动细分模型系列。我们广泛的实验表明，PPMobileSeg在ADE20K和Cityscapes数据集上实现了延迟、模型大小和准确性之间的出色平衡。</p>
</li>
</ul>
<h2 id="2-related-work"><a class="markdownIt-Anchor" href="#2-related-work"></a> 2. Related Work</h2>
<p>在速度和模型大小的限制下，移动语义分割是旨在以高效的网络设计适应语义分割网络的任务。</p>
<h3 id="21-semantic-segmentation"><a class="markdownIt-Anchor" href="#21-semantic-segmentation"></a> 2.1. semantic segmentation</h3>
<p>为了实现语义分割的高性能，几个关键要素是必不可少的，包括捕捉上下文的大感受野[3, 36]，准确分割的大分辨率特征[30, 32]，融合细节和语义特征的精确预测[1, 22]，以及改进特征表示的注意力机制[14, 26, 31]。最先进的模型往往结合了这些元素中的几个甚至全部，以实现卓越的性能。语义分割任务的主要要求是，网络必须能够捕捉到场景的整体视图，同时保留图像的细节和语义。因此，设计能够高效和有效地整合这些元素的网络架构是至关重要的。</p>
<h3 id="22-efficient-network-designs"><a class="markdownIt-Anchor" href="#22-efficient-network-designs"></a> 2.2. Efficient Network Designs</h3>
<p>在深度学习领域，有两种类型的高效网络架构。第一类侧重于在推理过程中不引入不必要的延迟的情况下向网络添加新元素。有代表性的是结构重参数化[8,9]，它在推理时用单个分支来接近多分支cnn块。第二类是以降低模型性能为代价来降低网络的规模。属于这一类的设计包括分组卷积[11]、通道洗牌[34]和高效注意力机制[23, 28, 29]。</p>
<h3 id="23-mobile-semantic-segmentation"><a class="markdownIt-Anchor" href="#23-mobile-semantic-segmentation"></a> 2.3. Mobile semantic segmentation</h3>
<p>由于语义分割的计算复杂度大，在移动设备上进行分割的研究有限，只有少数作品关注这一领域[12, 27, 33]。其中，TopFormer用一个自我关注块增强了标记金字塔，并使用他们提出的注入模块将其与本地特征融合。此外，SeaFormer通过高效的SEA注意力模块提高了模型性能。两者都明显优于MobileSeg和LRASPP，它们目前代表了移动语义分割的最先进水平。</p>
<h2 id="3-architecture"><a class="markdownIt-Anchor" href="#3-architecture"></a> 3. Architecture</h2>
<p><img src="/img/loading.gif" data-original="images/image-20230509151602110.png" alt="image-20230509151602110" /></p>
<blockquote>
<p>图2.PP-MobileSeg网络的结构。AAM的结构在图的右上方。普通插值模块和VIM的区别显示在图的右下方。通过只选择存在于最终预测中的类别，VIM通过对少数通道进行上采样，大大降低了延迟。</p>
</blockquote>
<p>本节介绍了在速度和尺寸限制下设计的移动分割网络的综合探索，旨在实现更好的分割精度。通过研究，我们确定了三个关键模块，它们可以带来更快的推理速度或更小的模型尺寸，并略微提高性能。PP-MobileSeg的完整架构如图2所示，它包括四个主要部分： StrideFormer, Aggregate Attention Module (AAM), segmentation head, and Valid Interpolate Module (VIM)。StrideFormer接收输入图像并生成一个特征金字塔，在最后两个阶段应用分层注意力以纳入全局语义。AAM负责融合局部和语义特征，然后通过分割头来产生分割掩码。最后，上采样模块VIM被用来进一步增强分割掩码，通过只对最终预测中存在的类别所对应的少数通道进行上采样来减少延迟。以下各节对这些模块进行了详细描述。</p>
<h3 id="31-strideformer"><a class="markdownIt-Anchor" href="#31-strideformer"></a> 3.1. StrideFormer</h3>
<p>在StrideFormer模块中，我们利用MobileNetV3[12]块的堆栈来提取不同感受野的特征。关于这个架构的变体的更多详细信息可以在3.4小节中找到。给定一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo>∈</mo><msup><mi>R</mi><mrow><mn>3</mn><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><annotation encoding="application/x-tex">I∈R^{3×H×W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span></span></span></span></span></span></span></span>的图像，其中3、H、W代表图像的通道、高度和宽度。StrideFormer产生的特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mo>×</mo><mn>8</mn></mrow></msub><mo separator="true">,</mo><msub><mi>F</mi><mrow><mo>×</mo><mn>16</mn></mrow></msub><mo separator="true">,</mo><msub><mi>F</mi><mrow><mo>×</mo><mn>32</mn></mrow></msub></mrow><annotation encoding="application/x-tex">F_{×8}, F_{×16}, F_{×32}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mtight">8</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mtight">1</span><span class="mord mtight">6</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mtight">3</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>，代表与输入图像的分辨率相比被降频8次、16次和32次的特征。一个关键的设计选择是骨干网中的阶段数量，其中每个阶段是一个mobilenetv3块的堆栈，产生其中一个特征集，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mo>×</mo><mi>d</mi><mi>o</mi><mi>w</mi><mi>n</mi><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mtext>−</mtext><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">F_{×downsample−rate}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">e</span><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>.。受efficientFormer[13]的启发，我们发现四级模型的参数开销最小，同时与五级模型相比，仍然保持优异的性能，如表3所示。 因此，我们用四阶段模式设计了StrideFormer。用四阶段骨干生成的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mo>×</mo><mn>8</mn></mrow></msub><mo separator="true">,</mo><msub><mi>F</mi><mrow><mo>×</mo><mn>16</mn></mrow></msub><mo separator="true">,</mo><msub><mi>F</mi><mrow><mo>×</mo><mn>32</mn></mrow></msub></mrow><annotation encoding="application/x-tex">F_{×8}, F_{×16}, F_{×32}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mtight">8</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mtight">1</span><span class="mord mtight">6</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mtight">3</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>，我们按照[seaformer]在最后两个阶段的特征上添加M/N SEA注意力块。由于大分辨率输入的自我注意模块的时间复杂性，我们在SEA注意模块之前加入跨度卷积，之后对特征进行上采样。通过这种方式，当我们用全局信息赋予特征时，我们将计算复杂度降低到原始实现的1/4。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230509153436227.png" alt="image-20230509153436227" /></p>
<blockquote>
<p>表3.PPMobileSeg-Base的三个拟议模块在ADE20K数据集上的消融研究。</p>
</blockquote>
<h3 id="32-aggregated-attention-module"><a class="markdownIt-Anchor" href="#32-aggregated-attention-module"></a> 3.2. Aggregated Attention Module</h3>
<p>利用骨干网生成的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mo>×</mo><mn>8</mn></mrow></msub><mo separator="true">,</mo><msub><mi>F</mi><mrow><mo>×</mo><mn>16</mn></mrow></msub><mo separator="true">,</mo><msub><mi>F</mi><mrow><mo>×</mo><mn>32</mn></mrow></msub></mrow><annotation encoding="application/x-tex">F_{×8}, F_{×16}, F_{×32}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mtight">8</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mtight">1</span><span class="mord mtight">6</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mtight">3</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>，我们设计了一个聚合注意模块（AAM）来融合特征。AAM的结构在图2的右上方。在生成的特征中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mo>×</mo><mn>16</mn></mrow></msub><mo separator="true">,</mo><msub><mi>F</mi><mrow><mo>×</mo><mn>32</mn></mrow></msub></mrow><annotation encoding="application/x-tex">F_{×16}, F_{×32}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mtight">1</span><span class="mord mtight">6</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mtight">3</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>具有较大的感受野，包含丰富的语义信息。因此，我们将它们作为信息过滤器，通过集合投票找出细节特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mo>×</mo><mn>8</mn></mrow></msub></mrow><annotation encoding="application/x-tex">F_{×8}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">×</span><span class="mord mtight">8</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>中的重要信息。在过滤过程中，F×16和F×32被上采样到与F×8相同的分辨率。并对它们进行sigmoid运算，以获得权重系数。之后，F×16和F×32被相乘，相乘的结果被用来过滤F×8。我们可以将上述程序表述为公式1 此外，我们观察到，具有丰富语义的特征是对之前过滤的细节特征的补充，对提高模型性能至关重要。因此，应该最大程度地保留它。因此，我们在过滤后的细节特征中加入了F×32，即最大感受野的特征，并在全局视图中进行了增强。</p>
<p>在特征融合后，融合后的特征同时捕捉到了丰富的空间和语义信息，这是分割性能的基础。在此基础上，我们按照TopFormer[33]添加了一个简单的分割头。分割头由一个1×1层组成，它有助于沿通道维度交换信息。然后，应用滤波层和卷积层来产生下采样的分割图。</p>
<h3 id="33-valid-interpolate-module"><a class="markdownIt-Anchor" href="#33-valid-interpolate-module"></a> 3.3. Valid Interpolate Module</h3>
<p>在延迟限制下，我们做了一个延迟分析，发现最后的插值和ArgMax操作占了整个延迟的50%以上。因此，我们设计了Valid Interpolate Module(VIM)来替代插值和ArgMax操作，大大降低了模型的延迟。SeaFormer-Base和PP-MobileSeg-Base的延时情况如图3所示。加入VIM后的详细统计数据见表。3.</p>
<p><img src="/img/loading.gif" data-original="images/image-20230509154012445.png" alt="image-20230509154012445" /></p>
<blockquote>
<p>图3.SeaFormer和PPMobileSeg之间的延迟情况比较。</p>
</blockquote>
<p>VIM是基于这样的观察：在一个训练有素的模型的预测中出现的类的数量往往比数据集中的总体类的数量少得多，特别是对于有大量类的数据集。这是有大量类的数据集的一个常见情况。因此，在插值和ArgMax过程中，没有必要考虑所有的类。VIM的结构在图2的右下方。如该结构所示，VIM包括三个主要步骤。首先，ArgMax和Unique操作被应用于下采样的分割图，以找出必要的通道。然后，索引选择操作只选择那些有效的通道，并将插值应用于瘦身后的特征。最后，被选中的通道被上采样到原始分辨率，产生最终的分割图。用VIM代替插值和ArgMax操作，我们以更少的延迟成本检索出最终的分割图。</p>
<p>VIM的使用大大减少了插值和ArgMax操作中涉及的通道，导致模型延迟的大幅下降。然而，VIM只适用于当类的数量大到足以在模型中出现通道冗余的时候。因此，设置了30个类的阈值，当类的数量低于这个阈值时，VIM就不会生效了。</p>
<h3 id="34-architecture-variants"><a class="markdownIt-Anchor" href="#34-architecture-variants"></a> 3.4. Architecture Variants</h3>
<p>我们提供了PP-MobileSeg的两个变体，以使我们的模型适应不同的复杂性要求，即PPMobileSeg-Base和PP-MobileSeg-Tiny。这两个变体在512x512形状的输入下的尺寸和延迟显示在表1中。1. 基础模型和微小模型有相同数量的MobileNetV3层，而基础模型比微小模型更宽。而且基础模型产生的特征有更多的通道，以丰富特征表示。此外，在注意力区块中也有一些差异。PP-MobileSeg-Base模型在SEA注意模块中有8个头，M/N=3/3注意块。PP-MobileSeg-Tiny模型在SEA注意模块中有4个头，块数为M/N=2/2。后两个阶段的特征通道，PP-MobileSeg-Base为128，192，PP-MobileSeg-Tiny为64，128。特征融合模块的设置对基础模型和微小模型是相同的，AAM的嵌入通道dim被设置为256。关于网络结构的更多细节，请参考源代码。</p>
<h2 id="4-experiments"><a class="markdownIt-Anchor" href="#4-experiments"></a> 4. Experiments</h2>
<p>在这一节中，我们首先介绍了用于模型训练和评估的数据集，并提供了关于我们训练和推理实现的细节。其次，我们将所提出的方法与以前在这个任务上最先进的方法在准确性、推理速度和模型大小方面进行比较。最后，我们进行了一项消融研究，以证明我们提出的方法的有效性。</p>
<h3 id="41-experiments-setup"><a class="markdownIt-Anchor" href="#41-experiments-setup"></a> 4.1. Experiments Setup</h3>
<h4 id="411-datasets"><a class="markdownIt-Anchor" href="#411-datasets"></a> 4.1.1 Datasets</h4>
<p>我们在ADE20K[36]和cityscapes[6]数据集上进行了实验，并使用mIoU作为评价指标。ADE20K是一个解析数据集，总共包含25K张图片和150个细粒度的语义概念。所有图像被分成20K/2K/3K，用于训练、验证和测试。Cityscapes是一个用于语义分割的大规模数据集。它由5000张精细注释的图像组成，2975张用于训练，500张用于验证，1525张用于测试开发。图片的分辨率为2048×1024，这对用于移动设备的模型构成了巨大的挑战。</p>
<h4 id="412-implementation-details"><a class="markdownIt-Anchor" href="#412-implementation-details"></a> 4.1.2 Implementation Details</h4>
<p>我们的实现是建立在PaddleSeg[15]和PaddlePaddle[19]之上的。</p>
<p>训练设置 我们的主干是在ImageNet1K[7]上预训练的，以检索关于图像的共同知识。我们将批次大小设置为32，并对模型进行80K次迭代训练。在训练过程中，我们以4:1的损失率交叉熵损失和Lovasz损失[2]来优化模型。我们使用指数移动平均法来平均不同训练迭代的模型参数，移动平均系数为0.999[25]。学习率为0.006，使用ADAMW[17]优化器，权重衰减设置为0.01。学习率计划被设置为热身计划和因子为1.0的聚能计划的组合。学习率从1e-6上升到1500 iters，然后线性下降。对于ADE20K，我们遵循TopFormer和SeaFormer[27, 33]的数据增强策略，包括[0.5, 2.0]的随机比例范围、图像裁剪到给定尺寸、随机水平翻转和随机变形。对于Cityscapes，除了我们将图像裁剪为1024x512而不是ADE20K数据集中的512x512，以及随机比例范围为[0.125, 1.5]外，数据增强是一样的。我们的模型是用两个Tesla V100 GPU训练的。我们报告了验证集上的单一比例结果，以便与其他方法进行比较。</p>
<p>推理设置 在推理过程中，我们将ADE20K数据集的输入形状设置为512×512，城市景观设置为512×1024。为了测试模型延迟，全精度的PPMobileSeg模型被导出到静态模型中，然后在Qualcomm 855上用PaddleLite单线程测量延迟。在推理过程中，我们使用VIM来代替插值和ArgMax操作。值得注意的是，图像预处理，包括调整大小和归一化，是在推理过程之前完成的，所以推理时间只包括模型推理时间。特别是，VIM的延迟与图像中预测的类的数量相关联。因此，我们使用ADE20K验证集的图像，它具有平均的类别数量，来评估延迟以进行合理的比较。</p>
<h3 id="42-comparison-with-state-of-the-arts"><a class="markdownIt-Anchor" href="#42-comparison-with-state-of-the-arts"></a> 4.2. Comparison with State-of-the-arts</h3>
<p>ADE20K结果表。1列出了PP-MobileSeg与以前的移动语义分割模型的比较，包括轻量级视觉transformer和高效的CNN，并报告了参数、延迟和mIoU方面的结果。如结果所示，PP-MobileSeg不仅在延迟上，而且在模型大小上都优于这些SOTA模型，同时在准确性上也保持了竞争优势。与MobileSeg和LRASPP相比，它们都使用MobileNetV3作为骨干，PPMobileSeg-Tiny在mIoU上比它们高3.0以上，同时比MobileSeg小49.47%，快45%。而PP-MobileSeg-Tiny比LRASPP小55%，快70.5%。与基于SOTA视觉transformer的模型TopFormer和SeaFormer相比，PP-MobileSeg使用基于卷积的全局自我注意作为其语义提取器，以较低的延迟和较小的模型尺寸实现了较高的分割精度。PP-MobileSeg-Base与同类产品相比，尺寸基本相同或小34.9%，速度快42.9%至44.7%，同时在准确率上保持竞争优势，在mIoU上高出1.37%至3.77%。这些结果证明了PP-MobileSeg在改善特征表示方面的有效性。</p>
<p>城市景观结果 从表2中可以看出，PP-MobileSeg-T是一个很好的例子。2中可以看出，PP-MobileSeg-Tiny在准确性、延迟和参数等各个方面都比SeaFormersmall取得了更好的性能。此外，PP-MobileSeg-Base在具有可比性的延迟和较小的模型尺寸的情况下，取得了明显更好的准确性。这些结果表明，即使在高分辨率输入的情况下，PP-MobileSeg也能保持其在准确性、模型大小和速度方面的出色平衡。</p>
<h3 id="43-ablation-study"><a class="markdownIt-Anchor" href="#43-ablation-study"></a> 4.3. Ablation Study</h3>
<p>我们进行了一项消融研究，以讨论我们提出的模块的影响，并对这些模块进行剖析和分析。在表3中，我们通过将三个建议的模块逐一加入到基线中，显示了它们的有效性。</p>
<p>VIM：正如我们之前提到的，VIM作为插值和ArgMax操作的替代品，以加快推理速度。从剖面图的比较中可以看出（图3），应用VIM后，Segmentation的整体延迟从76.32%大大降低到48.71%。而表3的实验结果显示，加入VIM后，模型延迟下降了49.5%。这些实验证明，VIM在具有大量类的数据集上的加速能力是非常出色的。</p>
<p>StrideFormer： 在StrideFormer中使用四级网络，使参数开销明显减少了32.19%。实验结果还显示准确率提高了0.78%，我们将此归功于增强的骨干网。</p>
<p>AAM：AAM提高了0.59%的准确性，同时略微增加了延迟和模型大小。为了深入了解AAM的设计，我们将融合模块分成两个分支：如表4所示的集合投票和最终语义。报告的结果显示了这两个分支的重要性，尤其是最终语义的重要性。没有它，准确率会下降0.45%。</p>
<h2 id="5-conclusion"><a class="markdownIt-Anchor" href="#5-conclusion"></a> 5. Conclusion</h2>
<p>在本文中，我们研究了混合视觉骨干网的设计方案，并解决了移动语义分割网络的延迟瓶颈问题。经过充分的探索，我们确定了有利于移动的设计选择，并提出了一个新的移动语义分割网络系列，称为PP-MobileSeg，该系列结合了transformer块和CNN。通过精心设计的主干模块、融合模块和插值模块，PP-MobileSeg在基于ARM的设备上实现了模型大小、速度和准确性之间的SOTA平衡。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/bisenetv1/" title="BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation</div></div><div class="info-2"><div class="info-item-1"> BiSeNet BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation  Abstract. 语义分割需要丰富的空间信息和相当大的感受野。然而，现代方法通常会牺牲空间分辨率来实现实时推理速度，这导致了性能不佳。在本文中，我们用一个新颖的双边分割网络（BiSeNet）来解决这个难题。我们首先设计了一个小步幅的空间路径，以保留空间信息并产生高分辨率的特征。同时，采用快速下采样策略的Context Path，以获得足够的感受野。在这两条路径的基础上，我们引入了一个新的特征融合模块来有效地结合特征。在Cityscapes、CamVid和COCO-Stuff数据集上，提议的架构在速度和分割性能之间取得了适当的平衡。具体来说，对于2048×1024的输入，我们在Cityscapes测试数据集上实现了68.4%的平均IOU，在一块NVIDIA Titan XP卡上的速度为105FPS，这比现有的具有可比性的方法要快得多。  1...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/mVIT1/" title="MobileViT"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">MobileViT</div></div><div class="info-2"><div class="info-item-1"> MobileViT模型简介 blog.csdn.net 论文名称：MobileViT: Light-Weight, General-Purpose, and Mobile-Friendly Vision Transformer- 论文下载地址：https://arxiv.org/abs/2110.02178- 官方源码（Pytorch实现）：https://github.com/apple/ml-cvnets- 自己从ml-cvnets仓库中剥离的代码：https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_classification/MobileViT   文章目录 [TOC]   0 前言 自从2020年ViT(Vision...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">CA</div></div><div class="info-2"><div class="info-item-1"> Coordinate Attention for Efficient Mobile Network Design  Abstract 最近关于移动网络设计的研究已经证明了通道注意（如SE注意力机制）对于提高模型性能的显著效果，但他们通常忽略了位置信息，而位置信息对于生成空间选择性注意图是很重要的。在本文中，我们提出了一种新的移动网络注意机制，将位置信息嵌入到通道注意中，我们称之为...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pp-mobileseg-explore-the-fast-and-accurate-semantic-segmentation-model-on-mobile-devices"><span class="toc-text"> PP-MobileSeg: Explore the Fast and Accurate Semantic Segmentation Model on Mobile Devices</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-abstract"><span class="toc-text"> 1. Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-introduction"><span class="toc-text"> 2. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-related-work"><span class="toc-text"> 2. Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-semantic-segmentation"><span class="toc-text"> 2.1. semantic segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-efficient-network-designs"><span class="toc-text"> 2.2. Efficient Network Designs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#23-mobile-semantic-segmentation"><span class="toc-text"> 2.3. Mobile semantic segmentation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-architecture"><span class="toc-text"> 3. Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-strideformer"><span class="toc-text"> 3.1. StrideFormer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-aggregated-attention-module"><span class="toc-text"> 3.2. Aggregated Attention Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-valid-interpolate-module"><span class="toc-text"> 3.3. Valid Interpolate Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#34-architecture-variants"><span class="toc-text"> 3.4. Architecture Variants</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-experiments"><span class="toc-text"> 4. Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-experiments-setup"><span class="toc-text"> 4.1. Experiments Setup</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#411-datasets"><span class="toc-text"> 4.1.1 Datasets</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#412-implementation-details"><span class="toc-text"> 4.1.2 Implementation Details</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-comparison-with-state-of-the-arts"><span class="toc-text"> 4.2. Comparison with State-of-the-arts</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-ablation-study"><span class="toc-text"> 4.3. Ablation Study</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-conclusion"><span class="toc-text"> 5. Conclusion</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA">CA</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>