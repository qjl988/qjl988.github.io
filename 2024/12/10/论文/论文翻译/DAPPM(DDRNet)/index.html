<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="用于实时和准确道路场景语义分割的深度双分辨率网络 Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes  Abstract 语义分割是自动驾驶汽车理解周围场景的关键技术。当代模型的迷人性能通常是以繁重的计算和漫长的推理时间为代价的，这对于自动驾驶来说是无法忍受的。">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/DAPPM(DDRNet)/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="用于实时和准确道路场景语义分割的深度双分辨率网络 Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes  Abstract 语义分割是自动驾驶汽车理解周围场景的关键技术。当代模型的迷人性能通常是以繁重的计算和漫长的推理时间为代价的，这对于自动驾驶来说是无法忍受的。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:04:02.011Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/DAPPM(DDRNet)/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:04:02.011Z" title="更新于 2024-12-11 01:04:02">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">5.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="用于实时和准确道路场景语义分割的深度双分辨率网络"><a class="markdownIt-Anchor" href="#用于实时和准确道路场景语义分割的深度双分辨率网络"></a> 用于实时和准确道路场景语义分割的深度双分辨率网络</h1>
<p><a href="zotero://open-pdf/library/items/QYYI4WG2">Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes</a></p>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>语义分割是自动驾驶汽车理解周围场景的关键技术。当代模型的迷人性能通常是以繁重的计算和漫长的推理时间为代价的，这对于自动驾驶来说是无法忍受的。利用轻量级架构（编码器-解码器或双通道）或在低分辨率图像上进行推理，最近的方法实现了非常快速的场景解析，甚至可以在单个1080Ti GPU上以超过100 FPS的速度运行。然而，这些实时方法与基于扩张骨架的模型在性能上仍有很大差距。为了解决这个问题，我们提出了一系列专为实时语义分割设计的高效骨干网络。所提出的深度双分辨率网络（DDRNets）由两个深度分支组成，在这两个分支之间执行多个双边融合。此外，我们还设计了一种名为深度聚合金字塔池化模块（DAPPM）的新型上下文信息提取器，以扩大有效感受野，并基于低分辨率特征图进行多尺度上下文融合。在Cityscapes和CamVid数据集上，我们的方法在准确性和速度之间实现了一种新的先进权衡。特别是，在单个2080Ti GPU上，DDRNet-23-slim在Cityscapes测试集上以102 FPS获得77.4%的mIoU，在CamVid测试集上以230 FPS获得74.7%的mIoU。在广泛使用测试增强的情况下，我们的方法优于大多数最先进的模型，而且所需的计算量更少。代码和训练模型可在线获取。</p>
<h2 id="i-introduction"><a class="markdownIt-Anchor" href="#i-introduction"></a> I. INTRODUCTION</h2>
<p>语义分割是一项基本任务，在这项任务中，输入图像的每个像素都应被赋予相应的标签[1]-[3]。它在许多实际应用中发挥着重要作用，例如医学图像分割、自动驾驶汽车和机器人导航[4], [5]。随着深度学习技术的兴起，卷积神经网络被应用于图像分割，并大大优于传统的基于手工特征的方法。自全卷积网络（FCN）[6]被提出用于处理语义分割问题以来，一系列新型网络被提出。DeepLab[7]取消了ResNet中的部分下采样以保持高分辨率，并利用大扩张卷积[8]来扩大感受野。从那时起，基于扩张卷积和上下文提取模块的骨干网络成为标准布局，广泛应用于多种方法中，包括DeepLabV2[9]、DeepLabV3[10]、PSPNet[11]和DenseASPP[12]。</p>
<p>由于语义分割是一种密集预测任务，神经网络需要输出大感受野的高分辨率特征图才能产生令人满意的结果，这在计算上是非常昂贵的。这个问题对于自动驾驶的场景解析尤为关键，因为自动驾驶需要在非常大的图像上执行，以覆盖宽广的视野。因此，上述方法在推理阶段非常耗时，无法直接部署在实际的自动驾驶车辆上。由于需要利用多尺度测试来提高准确性，它们甚至无法在一秒钟内处理一幅图像。</p>
<p>随着移动设备部署需求的不断增长，实时分割算法[13]-[17]越来越受到关注。DFANet[18]采用深度多尺度特征聚合和轻量级深度可分离卷积，在100 FPS下实现了71.3%的测试mIoU。与编码器-解码器模式不同，作者在[19]中提出了一种由空间路径和上下文路径组成的新型双边网络。具体而言，空间路径利用三个相对较宽的3×3卷积层来捕捉空间细节，而上下文路径是一个紧凑的预训练骨干层，用于提取上下文信息。这种双边方法包括[20]，其推理速度高于当时的编码器-解码器结构。</p>
<p>最近，提出了一些具有竞争力的实时方法，旨在对道路场景进行语义分割。这些方法可分为两类。一种是利用GPU高效骨干网，特别是ResNet-18[21]-[23]。其中，BiSeNetV2[24]在实时性能方面达到了一个新的峰值，在Cityscapes上以156 FPS实现了72.6%的测试mIoU。然而，除了使用额外训练数据的[23]之外，这些最新研究并未显示出获得更高质量结果的潜力。其中一些作品由于刻意设计的架构和调整的超参数而缺乏可扩展性。此外，鉴于更强大的骨干网的繁荣，ResNet-18的优势不大。</p>
<p>在本文中，我们提出了具有深度高分辨率表示的双分辨率网络，用于高分辨率图像的实时语义分割，尤其是道路驾驶图像。我们的DDR网络从一个主干开始，然后分成两个不同分辨率的并行深度分支。一个深度分支生成相对高分辨率的特征图，另一个深度分支通过多次降采样操作提取丰富的语义信息。两个分支之间通过多个双边连接实现高效的信息融合。此外，我们还提出了一个名为DAPPM的新模块，该模块输入低分辨率特征图，提取多尺度上下文信息，并以级联方式进行融合。在对语义分割数据集进行训练之前，双分辨率网络按照通用范式在ImageNet上进行训练。</p>
<p>根据在三个流行基准（即Cityscapes、CamVid和COCOStuff）上的大量实验结果，DDR网络在分割精度和推理速度之间达到了极佳的平衡。与其他实时算法相比，我们的方法在Cityscapes和CamVid上都达到了最新的准确度，而且不需要注意机制和额外的功能。通过标准测试增强，DDRNet可与最先进的模型相媲美，而且所需的计算资源更少。我们还报告了统计相关性能，并进行了消融实验，以分析架构改进和标准训练技巧的效果。</p>
<p>主要贡献总结如下</p>
<ul>
<li>为实时语义分割提出了一系列具有深度双分辨率分支和多双边融合的新型双边网络，作为高效的骨干网络。</li>
<li>设计了一个新模块，通过将特征聚合与金字塔池相结合来获取丰富的上下文信息。当在低分辨率特征图上执行时，推理时间几乎不会增加。</li>
<li>使用2080Ti，我们的方法在准确性和速度之间实现了新的先进权衡，在Cityscapes测试集上以102 FPS的速度实现了77.4%的mIoU，在CamVid测试集上以230 FPS的速度实现了74.7%的mIoU。据我们所知，我们是首家仅在Cityscapes测试集上使用精细注释以几乎实时的速度（22 FPS）达到80.4% mIoU的公司。</li>
</ul>
<h2 id="ii-related-work"><a class="markdownIt-Anchor" href="#ii-related-work"></a> II. RELATED WORK</h2>
<p>近年来，基于扩张卷积的方法提升了许多挑战性场景下的语义分割的性能。而且，开创性的工作为轻量级架构探索了更多的可能性，如编码器-解码器和双路径。此外，上下文信息被证明对场景解析任务非常关键。在本节中，我们将相关工作分为三类，即高性能语义分割、实时语义分割和上下文提取模块。</p>
<h3 id="a-high-performance-semantic-segmentation"><a class="markdownIt-Anchor" href="#a-high-performance-semantic-segmentation"></a> A. High-performance Semantic Segmentation</h3>
<p>由于缺乏空间细节，普通编码器的最后一层的输出不能直接用于预测分割掩码。而且，如果只是摆脱分类骨干的下采样，有效的感受野会太小，无法学习高级语义信息。一个可以接受的策略是利用扩张卷积来建立像素间的长距离连接，同时去除最后两个下采样层[10]，[11]，如图2（a）所示。然而，由于高分辨率特征图维度的指数级增长和稀释卷积实现的不充分优化，它也给实时推理带来了新的挑战。有一个事实是，大多数先进的模型都是建立在扩张骨架上的，因此基本上没有资格用于自驾车的场景解析。</p>
<p>一些作品试图探索标准扩张骨架的替代物。DeepLabv3plus[25]的作者提出了一个简单的解码器，将上采样的特征图与低层次的特征图融合起来。它减轻了对直接由扩张卷积产生的高分辨率特征图的要求。DeepLabv3plus可以取得有竞争力的结果，尽管编码器的输出跨度被设置为16。HRNet[26]突出了深层的高分辨率表征，体现了比扩张骨干更高的效率。我们发现，HRNet更高的计算效率和推理速度归功于其更薄的高分辨率信息流。以HRNetV2-W48为例，1/4分辨率和1/8分辨率特征的尺寸分别为48和96，这比预训练的ResNets[27]的扩张卷积小得多。尽管HRNet的高分辨率分支要薄得多，但通过平行的低分辨率分支和重复的多尺度融合，可以大大增强它们。</p>
<p>我们的工作从深、薄、高分辨率的表征开始，提出了更紧凑的架构。他们通过两个简明的主干保持高分辨率的表征并同时提取高层次的上下文信息。</p>
<h3 id="b-real-time-semantic-segmentation"><a class="markdownIt-Anchor" href="#b-real-time-semantic-segmentation"></a> B. Real-time Semantic Segmentation</h3>
<p>几乎所有的实时语义分割模型都采用两种基本方法：编码器-解码器方法和双路径方法。本文还讨论了在这两种方法中发挥重要作用的轻量级编码器。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230709204751733.png" alt="image-20230709204751733" /></p>
<h4 id="encoder-decoder-architecture"><a class="markdownIt-Anchor" href="#encoder-decoder-architecture"></a> Encoder-decoder Architecture</h4>
<p>与基于扩张卷积的模型相比，编码器-解码器架构直观地花费了较少的计算和推理时间。编码器通常是一个深度网络，通过反复的空间还原来提取上下文信息，解码器通过插值或转置卷积[28]来恢复分辨率，完成密集的预测，如图2（b）所示。特别是，编码器可以是在ImageNet上预先训练的轻量级骨干，也可以是像ERFNet[5]和ESPNet[16]那样从头训练的高效变体。SwiftNet[21]捍卫了在ImageNet上预训练编码器的优势，并利用轻量级的横向连接来协助进行上采样。[29]中的作者提出了一个多重空间融合和类边界监督的策略。FANNet[22]通过快速注意力模块和整个网络的额外下采样，在速度和准确性之间实现了良好的权衡。SFNet[23]提供了一个Flow Alignment Module (FAM)，用于对齐相邻层次的特征图以实现更好的融合。</p>
<h4 id="two-pathway-architecture"><a class="markdownIt-Anchor" href="#two-pathway-architecture"></a> Two-pathway Architecture</h4>
<p>编码器-解码器架构减少了计算量，但由于在反复下采样过程中损失了一些信息，不能通过解采样完全恢复，这就损害了语义分割的准确性。为了缓解这个问题，提出了双路径结构[19]，如图2（c）所示。除了一条提取语义信息的途径外，另一条高分辨率的浅层途径提供丰富的空间细节作为补充。为了进一步提高准确性，BiSeNetV2[24]使用全局平均池进行语境嵌入，并提出基于注意力的特征融合。BiSeNetV1&amp;V2中的两个路径最初是独立的，而Fast-SCNN[20]中的两个分支共享学习降样模块。CABiNet[30]采用了Fast-SCNN的整体架构，但使用MobileNetV3[31]作为上下文分支。</p>
<p>与现有的双路径方法相比，DDRNets的深而薄的高分辨率分支能够实现多种特征融合和充分的ImageNet预训练，同时保证了推理效率。我们的方法可以很容易地进行扩展，以达到更高的精度（在Cityscapes上超过80%的mIoU）。</p>
<h4 id="lightweight-encoders"><a class="markdownIt-Anchor" href="#lightweight-encoders"></a> Lightweight Encoders</h4>
<p>有许多计算效率高的骨干网可以用作编码器，如MobileNet[32]、ShuffleNet[33]和小版本的Xception[34]。MobileNet用深度可分离卷积代替了标准卷积，以减少参数和计算量。在MobileNetV2[35]中，深度可分离卷积的强正则化效应被倒置的剩余块所缓解。ShuffleNet利用了分组卷积的紧凑性，并提出了一个通道洗牌操作来促进不同组之间的信息融合。然而，这些网络包含了许多深度可分离的卷积，无法用现有的GPU架构有效实现。因此，尽管ResNet-18[27]的FLOPs约为MobileNetV2 1.0×的6倍，但在单台1080Ti GPU上，前者的推理速度要高于后者[21]。然而，现有的轻量级骨干网对于语义分割来说可能是次优的，因为它们通常为图像分类进行了过度的调整。</p>
<h3 id="c-context-extraction-modules"><a class="markdownIt-Anchor" href="#c-context-extraction-modules"></a> C. Context Extraction Modules</h3>
<p>语义分割的另一个关键是如何捕捉更丰富的上下文信息。Atrous Spatial Pyramid Pooling（ASPP）[9]由具有不同速率的平行Atrous卷积层组成，可以关注多尺度的上下文信息。PSPNet中的金字塔池化模块（PPM）[11]通过在卷积层之前实现金字塔池化，在计算上比ASPP更有效率。与卷积核的局部性不同，自我注意机制善于捕捉全局的依赖性。这样一来，双注意网络（DANet）[36]就利用了位置注意和通道注意的优势来进一步改善特征表示。物体背景网络（OCNet）[37]利用自我注意机制来探索物体背景，它被定义为属于同一物体类别的像素集合。CCNet[38]的作者提出了纵横交错的注意力来提高内存使用和计算的效率。然而，这些上下文提取模块是针对高分辨率的特征图设计和执行的，对于轻量级的模型来说太耗时了。以低分辨率的特征图为输入，我们用更多的尺度和深度特征聚合来加强PPM模块。当附加到低分辨率分支的末端时，所提出的模块在OCNet中的表现优于PPM和Base-OC模块。</p>
<h2 id="iii-method"><a class="markdownIt-Anchor" href="#iii-method"></a> III. METHOD</h2>
<p>在本节中，将介绍整个管道，它由两个主要部分组成：深度双分辨率网络和深度聚合金字塔池模块。</p>
<h3 id="a-deep-dual-resolution-network"><a class="markdownIt-Anchor" href="#a-deep-dual-resolution-network"></a> A. Deep Dual-resolution Network</h3>
<p>为了方便起见，我们可以在广泛使用的分类骨干网（如ResNets）上增加一个高分辨率分支。为了在分辨率和推理速度之间取得平衡，我们让高分辨率分支创建分辨率为输入图像分辨率1/8的特征图。因此，高分辨率分支被附加到conv3阶段的末尾。注意，高分辨率分支不包含任何降采样操作，与低分辨率分支一一对应，形成深度高分辨率表示。然后可以在不同阶段进行多个双边特征融合，以充分融合空间信息和语义信息。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230710202209286.png" alt="image-20230710202209286" /></p>
<p>DDRNet-23-slim和DDRNet-39的具体架构如表I所示。我们修改了原始ResNet的输入干，用两个连续的3×3卷积层替换了一个7×7卷积层。剩余的基本块被用来构建主干和随后的两个分支。为了扩展输出维度，在每个分支的末端添加了一个瓶颈块。双边融合包括将高分辨率分支融合到低分辨率分支（高到低融合）和将低分辨率融合到高分辨率分支（低到高融合）。在高分辨率到低分辨率的融合中，高分辨率特征图在进行点式求和之前通过3×3卷积序列进行降采样，步长为2。对于低分辨率到高分辨率的融合，低分辨率特征图首先通过1×1卷积进行压缩，然后通过双线性插值进行上采样。图3显示了双边融合是如何实现的。第i个高分辨率特征图XHi和低分辨率特征图<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>L</mi><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">X_{Li}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>可以写为</p>
<p><img src="/img/loading.gif" data-original="images/image-20230710201652803.png" alt="image-20230710201652803" /></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>H</mi></msub></mrow><annotation encoding="application/x-tex">F_H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>L</mi></msub></mrow><annotation encoding="application/x-tex">F_L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>分别代表高分辨率和低分辨率的残差基本块序列，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mrow><mi>L</mi><mo>−</mo><mi>H</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{L-H}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mrow><mi>H</mi><mo>−</mo><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{H-L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>分别代表低分辨率到高分辨率和高分辨率到低分辨率的变换器，R代表ReLU函数。</p>
<p>我们构建了四个不同深度和宽度的双分辨率网络。DDRNet-23的宽度是DDRNet-23-slim的两倍，DDRNet-39 1.5×也是DDRNet-39的加宽版本。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230710201717108.png" alt="image-20230710201717108" /></p>
<h3 id="b-deep-aggregation-pyramid-pooling-module"><a class="markdownIt-Anchor" href="#b-deep-aggregation-pyramid-pooling-module"></a> B. Deep Aggregation Pyramid Pooling Module</h3>
<p><img src="/img/loading.gif" data-original="images/image-20230710202635003.png" alt="image-20230710202635003" /></p>
<blockquote>
<p>图 5. 深度聚合金字塔池化模块的具体结构。多尺度分支的数量可根据输入分辨率进行调整。</p>
</blockquote>
<p>在此，我们提出一个新的模块，进一步从低分辨率特征图中提取上下文信息。图5显示了DAPPM的内部结构。以图像分辨率为1/64的特征图为输入，执行指数级跨度的大池核，生成图像分辨率为1/128、1/256、1/512的特征图。输入特征图和全局平均池生成的图像级信息也被利用。我们认为，通过单一的3×3或1×1卷积来混合所有多尺度上下文信息是不够的。受Res2Net[39]的启发，我们首先对特征图进行上采样，然后使用更多的3×3卷积以分层-残差的方式融合不同尺度的上下文信息。考虑到输入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>，每个尺度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>可以写为</p>
<p><img src="/img/loading.gif" data-original="images/image-20230710201754583.png" alt="image-20230710201754583" /></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">C_{1×1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1×1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>卷积，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msub></mrow><annotation encoding="application/x-tex">C_{3×3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mbin mtight">×</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3×3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>卷积，U表示上采样操作，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>j</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{j,k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>表示核大小为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>、步长为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>的池层，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>g</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{global}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>表示全局平均池化。最后，所有特征图通过1×1卷积进行合并和压缩。此外，为了便于优化，还添加了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1×1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>投影快捷方式。与SwiftNet[21]中的SPP类似，DAPPM是通过BN-ReLU-Conv序列实现的。</p>
<p>在DAPPM内部，通过较大的池核提取的上下文与较深的信息流进行整合，通过不同大小的池核整合不同深度的上下文，形成多尺度性。表2显示，DAPPM能够提供比PPM更丰富的上下文。虽然DAPPM包含更多的卷积层和更复杂的融合策略，但由于输入分辨率仅为图像分辨率的1/64，因此几乎不会影响推理速度。例如，对于1024×1024的图像，特征图的最大分辨率为16×16。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230710203053915.png" alt="image-20230710203053915" /></p>
<h3 id="c-overall-architecture-for-semantic-segmentation"><a class="markdownIt-Anchor" href="#c-overall-architecture-for-semantic-segmentation"></a> C. Overall Architecture for Semantic Segmentation</h3>
<p><img src="/img/loading.gif" data-original="images/image-20230710202533252.png" alt="image-20230710202533252" /></p>
<blockquote>
<p>图 4. 语义分割DDRNets概览。&quot;RB &quot;表示连续的残余基本区块。&quot;RBB &quot;表示单个残余瓶颈块。&quot;DAPPM &quot;表示深度聚合金字塔池化模块。&quot;Seg. 头 &quot;表示分割头。黑色实线表示带数据处理（包括上采样和下采样）的信息路径，黑色虚线表示不带数据处理的信息路径。&quot;sum &quot;表示点求和。虚线框表示在推理阶段丢弃的成分。</p>
</blockquote>
<p>我们的方法概览见图4。为了适应语义分割任务，我们对双分辨率网络做了一些改动。首先，将低分辨率分支RBB中3×3卷积的步长设为2，以进一步降低采样率。然后，在低分辨率分支的输出中加入DAPPM，从1/64图像分辨率的高层特征图中提取丰富的上下文信息。此外，通过双线性插值和求和融合实现的低到高融合取代了最后的高到低融合。最后，我们设计了一个由3×3卷积层和1×1卷积层组成的简单分割头。分割头的计算负荷可以通过改变3×3卷积层的输出维度来调整。我们为DDRNet-23-slim设置了64维，为DDRNet-23设置了128维，为DDRNet39设置了256维。请注意，除了分割头和DAPPM模块外，所有模块都在ImageNet上进行了预训练。</p>
<h3 id="d-deep-supervision"><a class="markdownIt-Anchor" href="#d-deep-supervision"></a> D. Deep Supervision</h3>
<p>训练阶段的额外监督可以简化深度卷积神经网络（DCNN）的优化。在PSPNet中，根据实验结果[11]，增加了一个辅助损失来监督ResNet-101的res4 22块的输出，并将相应的权重设置为0.4。BiSeNetV2[24]提出了一种助推训练策略，即在语义分支的每个阶段末尾添加额外的分割头。然而，这种方法需要进行大量实验才能找到平衡每种损失的最佳权重，并导致训练内存不可忽略地增加。为了获得更好的结果，SFNet[23]采用了一种类似的策略，即级联深度监督学习（Cascaded Deeply Supervised Learning）。在本文中，为了与大多数方法进行公平比较，我们只采用了简单的额外监督。如图4所示，我们添加了辅助损失，并按照PSPNet将权重设置为0.4。辅助分割头在测试阶段被丢弃。最终损失是交叉熵损失的加权和，可以表示为</p>
<p><img src="/img/loading.gif" data-original="images/image-20230710201851340.png" alt="image-20230710201851340" /></p>
<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>f</mi></msub><mo separator="true">,</mo><msub><mi>L</mi><mi>n</mi></msub><mo separator="true">,</mo><msub><mi>L</mi><mi>α</mi></msub></mrow><annotation encoding="application/x-tex">L_f,L_n,L_{\alpha}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>分别表示最终损失、正常损失和辅助损失，α表示辅助损失的权重，本文中为0.4。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CBL%E6%9D%83%E9%87%8D%E8%AE%BE%E7%BD%AE/" title="CBL权重设置"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">CBL权重设置</div></div><div class="info-2"><div class="info-item-1"> 概念 Class Balanced Loss（CB Loss）是一种用于处理类别不平衡问题的损失函数，可以在训练深度学习模型时有效地解决由于训练数据中类别分布不均衡所导致的问题。其基本思想是为数据集中出现频率较低的类别分配更大的权重，以使得训练过程中每个类别的贡献相对均衡。 CB Loss的权重设置方式可以通过以下步骤实现：  统计每个类别在训练数据中的样本数量。 计算每个类别的权重，常用的权重计算方式包括：  Inverse frequency weighting（IF）：将每个类别的权重设置为类别出现次数的倒数。 Effective number of samples（ENS）：将每个类别的权重设置为每个类别的有效样本数，计算方式为： wc=1−β1−βncw_c = \frac{1-\beta}{1-\beta^{n_c}}wc​=1−βnc​1−β​ 其中，wcw_cwc​是第ccc个类别的权重，β\betaβ是一个可调参数，ncn_cnc​是第ccc个类别的样本数量。 Beta...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CariesNet/" title="CariesNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">CariesNet</div></div><div class="info-2"><div class="info-item-1"> CariesNet: a deep learning approach for segmentation of multi-stage caries lesion from oral panoramic X-ray image  Abstract 龋齿一直是世界各地常见的健康问题，它甚至可以最终导致牙髓和根尖炎症。及时有效地治疗龋齿对患者减少痛苦至关重要。传统的龋齿疾病诊断方法，如肉眼检测和全景X光片检查，依赖于有经验的医生，这可能会导致误诊和高耗时。为此，我们提出了一种新型的深度学习架构，即CariesNet，以从全景射线照片中划分出不同的龋齿程度。我们首先收集了一个高质量的全景射线照片数据集，其中有3127个划线清晰的龋齿病变，包括浅度龋齿、中度龋齿和深度龋齿。然后，我们将CariesNet构建为一个U型网络，并增加了全尺寸轴向关注模块，以从口腔全景图像中分割这三种龋齿类型。此外，我们测试了CariesNet和其他基线方法的分割性能。实验表明，我们的方法在对三种不同程度的龋齿进行分割时，可以达到平均93.64%的Dice系数和93.61%的准确率。  1...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BAM</div></div><div class="info-2"><div class="info-item-1"> BAM  Abstract 近来，小样本分割分割技术（FSS）得到了广泛的发展。然而，训练出来的模型偏向于所看到的类别，而不是理想的类别无关性，从而阻碍了对新内容的识别。本文提出了一种新颖而直接的见解来缓解这一问题。具体来说，我们在传统的 FSS 模型（元学习器）上增加了一个分支（基础学习器），以明确识别基础类别的目标，即不需要分割的区域。然后，对这两个学习器并行输出的粗略结果进行自适应整合，从而得出精确的分割预测结果。考虑到元学习器的敏感性，我们进一步引入了一个调整因子来估计输入图像对之间的场景差异，以促进模型的集合预测。在 PASCAL-5i 和 COCO-20i 上取得的显著性能提升验证了这一方法的有效性，而且令人惊讶的是，即使使用两个普通学习器，我们的多功能方案也创造了新的一流水平。此外，鉴于所提方法的独特性，我们还将其扩展到了更现实但更具挑战性的环境中，即广义 FSS，在这种环境中，基础类和新类别的像素都需要确定。  1. Introduction   得益于成熟的大规模数据集 [8, 9,...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E4%BA%8E%E5%AE%9E%E6%97%B6%E5%92%8C%E5%87%86%E7%A1%AE%E9%81%93%E8%B7%AF%E5%9C%BA%E6%99%AF%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%8F%8C%E5%88%86%E8%BE%A8%E7%8E%87%E7%BD%91%E7%BB%9C"><span class="toc-text"> 用于实时和准确道路场景语义分割的深度双分辨率网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#i-introduction"><span class="toc-text"> I. INTRODUCTION</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ii-related-work"><span class="toc-text"> II. RELATED WORK</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#a-high-performance-semantic-segmentation"><span class="toc-text"> A. High-performance Semantic Segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#b-real-time-semantic-segmentation"><span class="toc-text"> B. Real-time Semantic Segmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#encoder-decoder-architecture"><span class="toc-text"> Encoder-decoder Architecture</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#two-pathway-architecture"><span class="toc-text"> Two-pathway Architecture</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lightweight-encoders"><span class="toc-text"> Lightweight Encoders</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#c-context-extraction-modules"><span class="toc-text"> C. Context Extraction Modules</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#iii-method"><span class="toc-text"> III. METHOD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#a-deep-dual-resolution-network"><span class="toc-text"> A. Deep Dual-resolution Network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#b-deep-aggregation-pyramid-pooling-module"><span class="toc-text"> B. Deep Aggregation Pyramid Pooling Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#c-overall-architecture-for-semantic-segmentation"><span class="toc-text"> C. Overall Architecture for Semantic Segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#d-deep-supervision"><span class="toc-text"> D. Deep Supervision</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/11/%E5%B0%8F%E7%B1%B3/syzkaller/syzkaller01-%E9%85%8D%E7%BD%AE/" title="syzkaller01-配置">syzkaller01-配置</a><time datetime="2024-12-10T17:10:45.000Z" title="发表于 2024-12-11 01:10:45">2024-12-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM">BAM</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>