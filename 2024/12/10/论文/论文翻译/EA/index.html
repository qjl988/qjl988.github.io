<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>EANet | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="EA 文章链接  Abstract 注意机制，特别是自我注意，在视觉任务的深度特征表示中发挥了越来越重要的作用。自我注意通过计算特征的加权和来更新每个位置的特征，使用所有位置的成对亲和力来捕捉单个样本内的长程依赖性。然而，自我注意具有二次复杂性，并且忽略了不同样本之间的潜在相关性。本文提出了一种新的注意机制，我们称之为外部注意，它基于两个外部的、小的、可学习的、共享的记忆，可以通过简单地使用两个">
<meta property="og:type" content="article">
<meta property="og:title" content="EANet">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/EA/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="EA 文章链接  Abstract 注意机制，特别是自我注意，在视觉任务的深度特征表示中发挥了越来越重要的作用。自我注意通过计算特征的加权和来更新每个位置的特征，使用所有位置的成对亲和力来捕捉单个样本内的长程依赖性。然而，自我注意具有二次复杂性，并且忽略了不同样本之间的潜在相关性。本文提出了一种新的注意机制，我们称之为外部注意，它基于两个外部的、小的、可学习的、共享的记忆，可以通过简单地使用两个">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:04:01.441Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/EA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'EANet',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">EANet</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">EANet</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:04:01.441Z" title="更新于 2024-12-11 01:04:01">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">5.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="ea"><a class="markdownIt-Anchor" href="#ea"></a> EA</h1>
<p><a href="zotero://open-pdf/library/items/FVICTQUK">文章链接</a></p>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>注意机制，特别是自我注意，在视觉任务的深度特征表示中发挥了越来越重要的作用。自我注意通过计算特征的加权和来更新每个位置的特征，使用所有位置的成对亲和力来捕捉单个样本内的长程依赖性。然而，自我注意具有二次复杂性，并且忽略了不同样本之间的潜在相关性。本文提出了一种新的注意机制，我们称之为外部注意，它基于两个外部的、小的、可学习的、共享的记忆，可以通过简单地使用两个级联的线性层和两个规范化层来实现；它方便地取代了现有流行架构中的自我注意。外部注意力具有线性复杂性，并隐含地考虑了所有数据样本之间的相关性。我们进一步将多头机制纳入外部注意力，以提供一个全MLP架构，即外部注意力MLP（EAMLP），用于图像分类。在图像分类、物体检测、语义分割、实例分割、图像生成和点云分析方面的大量实验表明，我们的方法提供了与自我注意机制及其某些变体相当或更高的结果，而且计算和内存成本低得多。</p>
<h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1. INTRODUCTION</h2>
<p>由于其捕捉长距离依赖关系的能力，自我注意机制有助于提高各种自然语言处理[1], [2]和计算机视觉[3], [4]任务的性能。自我关注的工作方式是通过聚合单个样本中所有其他位置的特征来完善每个位置的表示，这导致了样本中位置数量的二次计算复杂性。因此，一些变种试图以较低的计算成本接近自我注意[5]，[6]，[7]，[8]。</p>
<p>此外，自我注意集中在单个样本内不同位置之间的自我亲和力，而忽略了与其他样本的潜在关联性。不难看出，纳入不同样本之间的关联性有助于促进更好的特征表示。例如，在语义分割任务中，属于同一类别但分布在不同样本中的特征应该被一致对待，类似的观察也适用于图像分类和其他各种视觉任务。</p>
<p>本文提出了一种新颖的轻量级注意力机制，我们称之为外部注意力（见图1c））。如图1a）所示，计算自我注意力需要首先通过计算自我查询向量和自我关键向量之间的亲和力来计算一个注意力图，然后通过用这个注意力图对自我价值向量加权来生成一个新的特征图。外部注意的工作方式不同。我们首先通过计算自我查询向量和外部可学习的关键记忆之间的亲和力来计算注意图，然后通过将这个注意图乘以另一个外部可学习的价值记忆来产生一个精炼的特征图。</p>
<p>在实践中，这两个存储器是用线性层实现的，因此可以通过反向传播以端到端的方式进行优化。它们独立于单个样本，并在整个数据集中共享，这起到了强大的正则化作用，提高了注意力机制的泛化能力。外部注意的轻量级性质的关键是，记忆中的元素数量远远小于输入特征中的数量，产生的计算复杂性与输入中的元素数量成线性关系。外部记忆的设计是为了学习整个数据集中最具辨别力的特征，捕捉信息量最大的部分，同时排除其他样本的干扰信息。类似的想法可以在稀疏编码[9]或字典学习[10]中找到。然而，与这些方法不同的是，我们既不试图重构输入特征，也不对注意力图谱应用任何明确的稀疏正则化。</p>
<p>尽管提出的外部注意方法很简单，但它对各种视觉任务都很有效。由于它的简单性，它可以很容易地被纳入现有的基于自我注意的流行架构，如DANet[4]、SAGAN[11]和T2T-Transformer[12]。图3展示了一个典型的架构，在图像语义分割任务中用我们的外部注意力取代了自我注意力。我们对诸如分类、物体检测、语义分割、实例分割和生成等基本视觉任务进行了广泛的实验，并采用了不同的输入模式（图像和点云）。结果显示，我们的方法以更低的计算成本取得了与原始自我注意机制及其一些变体相当或更好的结果。</p>
<p>为了学习同一输入的不同方面的注意力，我们将多头机制纳入外部注意力，提高其能力。受益于所提出的多头外部注意力，我们设计了一个新的全MLP架构，名为EAMLP，它在图像分类任务中可以与CNN和原始变形器相媲美。</p>
<p>本文的主要贡献总结如下：</p>
<ul>
<li>一种新的注意机制，即外部注意，具有O(n)的复杂性；它可以取代现有架构中的自我注意。它可以在整个数据集上挖掘潜在的关系，提供一个强大的规范化作用，并提高注意力机制的泛化能力。</li>
<li>多头外部关注，这有利于我们建立一个全MLP架构；它在ImageNet-1K数据集上达到了79.4%的top1准确率。</li>
<li>利用外部注意力进行图像分类、物体检测、语义分割、实例分割、图像生成、点云分类和点云分割的大量实验。在必须保持低计算量的情况下，它比原始的自我注意机制和它的一些变种取得了更好的结果。</li>
</ul>
<h2 id="2-related-work"><a class="markdownIt-Anchor" href="#2-related-work"></a> 2. RELATED WORK</h2>
<p>由于对注意力机制的全面回顾超出了本文的范围，我们只讨论视觉领域中最密切相关的文献。</p>
<h3 id="21-the-attention-mechanism-in-visual-tasks"><a class="markdownIt-Anchor" href="#21-the-attention-mechanism-in-visual-tasks"></a> 2.1 The attention mechanism in visual tasks</h3>
<p>注意机制可以被看作是一种根据激活的重要性重新分配资源的机制。它在人类视觉系统中起着重要的作用。在过去的十年中，这一领域有了蓬勃的发展[3]，[13]，[14]，[15]，[16]，[17]，[18]。Hu等人提出了SENet[15]，表明注意力机制可以减少噪声并提高分类性能。随后，许多其他论文将其应用于视觉任务。Wang等人提出了用于视频理解的非局部网络[3]，Hu等人[19]将注意力用于物体检测，Fu等人提出了用于语义分割的DANet[4]，Zhang等人[11]证明了注意力机制在图像生成中的有效性，Xie等人提出了用于点云处理的A-SCN[20]。</p>
<h3 id="22-self-attention-in-visual-tasks"><a class="markdownIt-Anchor" href="#22-self-attention-in-visual-tasks"></a> 2.2 Self-attention in visual tasks</h3>
<p>自我注意是注意力的一个特例，许多论文[3]、[4]、[11]、[17]、[21]，都考虑了视觉的自我注意机制。自我注意的核心思想是计算特征之间的亲和力以捕捉长距离的依赖关系。然而，随着特征图大小的增加，计算和内存的开销也呈四级增长。为了降低计算和内存成本，Huang等人[5]提出了纵横交错的注意力，即依次考虑行注意力和列注意力来捕捉全局环境。Li等人[6]采用期望最大化（EM）聚类法来优化自我注意。Yuan等人[7]提出使用对象-上下文向量来处理注意力；然而，它依赖于语义标签。Geng等人[8]表明矩阵分解是在语义分割和图像生成中对全局环境进行建模的更好方法。其他作品[22]，[23]也探索了通过使用自我注意机制来提取局部信息。</p>
<p>与自我注意不同的是，自我注意是通过计算自我查询和自我键之间的亲和力来获得注意图，而我们的外部注意则是计算自我查询和一个小得多的可学习的键存储器之间的关系，它捕捉了数据集的全局背景。外部注意力不依赖于语义信息，可以通过反向传播算法以端到端的方式进行优化，而不需要迭代算法。</p>
<h3 id="23-transformer-in-visual-tasks"><a class="markdownIt-Anchor" href="#23-transformer-in-visual-tasks"></a> 2.3 Transformer in visual tasks</h3>
<p>基于变换器的模型在自然语言处理方面取得了巨大的成功[1]、[2]、[16]、[24]、[25]、[26]、[27]。最近，它们在视觉任务方面也表现出巨大的潜力。Carion等人[28]提出了一个端到端的检测转化器，它将CNN的特征作为输入，并用转化器生成边界框。Dosovitskiy[18]提出了基于补丁编码和变换器的ViT，表明在有足够训练数据的情况下，变换器能提供比传统CNN更好的性能。Chen等人[29]提出了基于使用变换器的图像生成的iGPT。</p>
<p>随后，变换器方法被成功地应用于许多视觉任务，包括图像分类[12]、[30]、[31]、[32]、物体检测[33]、低层次视觉[34]、语义分割[35]、跟踪[36]、视频实例分割[37]、图像生成[38]、多模态学习[39]、物体重新识别[40]、图像标题[41]、点云学习[42]和自监督学习[43]。读者可以参考最近的调查报告[44], [45]，以更全面地回顾在视觉任务中使用变换器方法的情况。</p>
<h2 id="3-methodology"><a class="markdownIt-Anchor" href="#3-methodology"></a> 3. METHODOLOGY</h2>
<p>在本节中，我们首先分析了原始的自我注意机制。然后我们详细介绍了我们定义注意力的新方法：外部注意力。它只需使用两个线性层和两个归一化层就可以轻松实现，如后面算法1所示。</p>
<h3 id="31-self-attention-and-external-attention"><a class="markdownIt-Anchor" href="#31-self-attention-and-external-attention"></a> 3.1 Self-Attention and External Attention</h3>
<p>我们首先重温一下自我注意的机制（见图1a））。给定一个输入特征图F∈RN×d，其中N是元素数（或图像中的像素），d是特征维数，自我关注将输入线性地投射到一个查询矩阵Q∈RN×d′，一个关键矩阵K∈RN×d′，和一个值矩阵V∈RN×d[16]。那么，自我关注可以被表述为：</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202301191250297.png" alt="" /></p>
<p>其中，A∈RN×N是注意力矩阵，αi,j是第i个和第j个元素之间（相似性）的成对亲和力。</p>
<p>一个常见的简化变体（图1b））的自我注意直接从输入特征F中计算出一个注意图，使用：</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202301191252238.png" alt="" /></p>
<p>在这里，注意力图是通过计算特征空间中的像素级相似度得到的，而输出是输入的精炼特征表示。</p>
<p>然而，即使在简化的情况下，O(dN 2)的高计算复杂性也给自我关注的使用带来了很大的缺陷。输入像素数的二次复杂度使自我关注直接应用于图像是不可行的。因此，以前的工作[18]利用对斑块而不是像素的自我注意来减少计算工作量。</p>
<p>自我注意可以被看作是使用自我值的线性组合来完善输入特征。然而，远的不说，在这个线性组合中，我们确实需要N×N的自我注意矩阵和N元素的自我值矩阵。此外，自我注意只考虑了一个数据样本内元素之间的关系，而忽略了不同样本内元素之间的潜在关系，这可能限制了自我注意的能力和灵活性。</p>
<p>因此，我们提出了一个新的注意力模块，名为外部注意力，它通过以下方式计算输入像素和外部记忆单元M∈RS×d之间的注意力：</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230119125249365.png" alt="image-20230119125249365" /></p>
<p>与自我注意力不同，方程（5）中的αi,j是第i个像素与M的第j行之间的相似度，其中M是一个独立于输入的可学习参数，它作为整个训练数据集的记忆。A是由这个学习的数据集级的先验知识推断出的注意力图；它以类似于自我注意力的方式被归一化（见3.2节）。最后，我们通过A的相似性更新M的输入特征。</p>
<p>在实践中，我们使用两个不同的存储单元Mk和Mv作为键和值，以增加网络的能力。这稍微改变了外部注意力的计算</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230119125315745.png" alt="image-20230119125315745" /></p>
<p>外部注意力的计算复杂度是O(dsn )；由于d和S是超参数，所提出的算法在像素数上是线性的。事实上，我们发现一个小的S，例如64，在实验中效果很好。因此，外部注意比自我注意有效得多，允许其直接应用于大规模输入。我们还注意到，外部注意的计算负荷大致相当于1×1的卷积。</p>
<h3 id="32-normalization"><a class="markdownIt-Anchor" href="#32-normalization"></a> 3.2 Normalization</h3>
<p>在自我注意中采用了Softmax来归一化注意图，使∑ j αi,j = 1。然而，注意图是通过矩阵乘法计算的。与余弦相似性不同，注意力图对输入特征的尺度很敏感。为了避免这个问题，我们选择了[42]中提出的双重规范化，它分别对列和行进行规范化。这种双重规范化的表述是：</p>
<p>算法1中列出了外部注意力的python风格的伪代码。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230119131721206.png" alt="image-20230119131721206" /></p>
<h3 id="33-multi-head-external-attention"><a class="markdownIt-Anchor" href="#33-multi-head-external-attention"></a> 3.3 Multi-head external attention</h3>
<p>在Transformer[16]中，自我注意在不同的输入通道上被多次计算，这被称为多头注意。多头注意可以捕捉到标记之间的不同关系，提高了单头注意的能力。我们对多头外部注意采用了类似的方法，如算法2和图2所示。</p>
<p>多头外部注意可以写成。</p>
<p>其中hi是第i个头，H是头的数量，Wo是一个线性变换矩阵，使输入和输出的尺寸一致。Mk∈RS×d和Mv∈RS×d是不同头的共享内存单元。</p>
<p>这种架构的灵活性使我们能够在共享内存单元中的头数H和元素数S之间取得平衡。例如，我们可以将H乘以k，而将S除以k。</p>
<h2 id="4-experiments"><a class="markdownIt-Anchor" href="#4-experiments"></a> 4 EXPERIMENTS</h2>
<p>我们对图像分类、物体检测、语义分割、实例分割、图像生成、点云分类和点云分割任务进行了实验，以评估我们提出的外部注意力方法的有效性。所有的实验都是用Jittor[86]和/或Pytorch[87]深度学习框架实现的。</p>
<h3 id="41-ablation-study"><a class="markdownIt-Anchor" href="#41-ablation-study"></a> 4.1 Ablation study</h3>
<p>为了验证我们完整模型中的建议模块，我们在PASCAL VOC分割数据集[88]上进行了实验。图3描述了用于消融研究的架构，它以FCN[46]为特征骨架。批量大小和迭代总数分别被设定为12和30,000。我们重点关注记忆单元的数量、自我注意与外部注意、骨干、归一化方法和骨干的输出步长。如表1所示，我们可以观察到在Pascal VOC数据集上，外部注意力提供了比自我注意力更好的准确性。选择一个合适的存储单元数量对结果的质量很重要。归一化方法可以对外部注意力产生巨大的积极影响，并对自我注意力有所改善。</p>
<h3 id="42-visual-analysis"><a class="markdownIt-Anchor" href="#42-visual-analysis"></a> 4.2 Visual analysis</h3>
<p>使用外部注意力进行分割（见图3）和使用多头外部注意力进行分类（见第4.3节）的注意力图分别显示在图4和图5。我们从一个层的记忆单元Mk中随机选择一行M i k。然后通过计算M i k对输入特征的注意力来描绘注意力图。我们观察到，如图4所示，学习到的注意力图集中在有意义的物体或背景上进行分割任务。图5中的最后两行表明，不同行的Mk对不同的区域给予了关注。如图5所示，多头外部注意的每个头可以在不同程度上激活感兴趣的区域，提高了外部注意的表现能力。</p>
<h3 id="43-image-classification"><a class="markdownIt-Anchor" href="#43-image-classification"></a> 4.3 Image classification</h3>
<p>ImageNet-1K[89]是一个广泛使用的图像分类数据集。我们替换了Performer[90]，多头机制对自我注意和外部注意都是必要的。我们还尝试了MoCo V3[43]提出的策略，在T2T-ViT骨干的MLP块（不是外部注意力块）中用BatchNorm(BN)[92]取代LayerNorm(LN) [91]。我们观察到在我们的EAMLP-7上有1%的改进。然而，它在我们的大模型EAMLP-14和EAMLP-19中产生了失败的案例。</p>
<h3 id="44-object-detection-and-instance-segmentation"><a class="markdownIt-Anchor" href="#44-object-detection-and-instance-segmentation"></a> 4.4 Object detection and instance segmentation</h3>
<p>MS COCO数据集[93]是一个流行的物体检测和实例分割的基准。它包含了超过200,000张图片和来自80个类别的超过500,000个注释的物体实例。</p>
<p>MMDetection[47]是一个广泛使用的对象检测和实例分割的工具包。我们使用带有RestNet-50主干的MMDetection进行物体检测和实例分割实验，并应用于COCO数据集。我们只在Resnet第四阶段结束时加入了我们的外部关注。表3和表4中的结果显示，外部注意力给我们带来了很大的帮助。3和4显示，外部注意力给物体检测和实例分割任务的准确性带来了1%的提高。</p>
<h3 id="45-semantic-segmentation"><a class="markdownIt-Anchor" href="#45-semantic-segmentation"></a> 4.5 Semantic segmentation</h3>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230119131851153.png" alt="image-20230119131851153" /></p>
<blockquote>
<p>图3 使用我们提出的外部注意力进行语义分割的EANet。</p>
</blockquote>
<p>在本实验中，我们采用了图3中的语义分割架构，将其称为EANet，并将其应用于Pascal VOC[88]、ADE20K[94]和cityscapes[95]数据集。</p>
<p>Pascal VOC包含10,582张图像用于训练，1,449张图像用于验证，1,456张图像用于测试。它有20个前景物体类别和一个用于分割的背景类别。我们使用输出步长为8的扩张ResNet-101作为骨干，与所有比较的方法一样；它在ImageNet-1K上进行了预训练。在训练过程中，我们采用了多学习率政策。初始学习率、批次大小和输入大小被设定为0.009、16和513×513。我们首先在训练集上训练了45000次迭代，然后在训练集上微调了15000次迭代。最后我们在测试集上使用了多尺度和翻转测试。图4显示了视觉结果，表5给出了定量结果：我们的方法可以达到与最先进方法相当的性能。</p>
<p>ADE20K是一个更具挑战性的数据集，有150个类，分别有20K、2K和3K的图像用于训练、验证和测试。我们采用输出步长为8的扩张ResNet-101作为骨干。实验配置与mmsegmentation[60]相同，训练ADE20K进行16万次迭代。表6中的结果显示，我们的方法在ADE20K值集上的表现优于其他方法。</p>
<p>Cityscapes包含5,000个高质量的像素级精细注释的标签，用于城市场景理解的19个语义类别。每张图片是1024×2048像素。它分为2975、500和1525幅图像，用于训练、验证和测试。(它还包含20000张粗略标注的图像，我们在实验中没有使用这些图像）。我们采用输出步长为8的扩张ResNet-101作为所有方法的骨干。实验配置也与mmsegmentation相同，用80k次迭代来训练城市景观。表7中的结果显示，我们的方法在城市景观估值集上取得了与最先进的方法，即DANet[4]相当的结果。</p>
<h3 id="46-image-generation"><a class="markdownIt-Anchor" href="#46-image-generation"></a> 4.6 Image generation</h3>
<p>自我注意通常被用于图像生成，一个代表性的方法是SAGAN[11]。我们在生成器和判别器中用我们的外部注意方法取代了SAGAN中的自我注意机制，以获得我们的EAGAN模型。所有的实验都是基于流行的PyTorch-StudioGAN repo[96]。超参数使用SAGAN的默认配置。我们使用Frechet Inception Distance（FID）[97]和Inception Score（IS）[98]作为我们的评价指标。一些生成的图像显示在图6中，定量结果在表8和表9中给出。8和9：外部关注提供了比SAGAN和其他一些GANs更好的结果。</p>
<h3 id="47-point-cloud-classification"><a class="markdownIt-Anchor" href="#47-point-cloud-classification"></a> 4.7 Point cloud classification</h3>
<p>ModelNet40[99]是一个流行的三维形状分类的基准，包含40个类别的12,311个CAD模型。它有9,843个训练样本和2,468个测试样本。我们的EAT模型取代了PCT[42]中所有的自我注意模块。我们在每个形状上取样1024个点，并按照PCT[42]的方法，用随机平移、各向异性缩放和放弃来增强输入。表11表明，我们的方法优于其他所有的方法，包括其他基于注意力的方法，如PCT。我们提出的方法为二维和三维视觉提供了一个杰出的骨干。</p>
<h3 id="48-point-cloud-segmentation"><a class="markdownIt-Anchor" href="#48-point-cloud-segmentation"></a> 4.8 Point cloud segmentation</h3>
<p>我们在ShapeNet零件数据集[100]上进行了点云分割的实验。它在训练集中有14,006个三维模型，在评估集中有2,874个。每个形状都被分割成部分，共有16个对象类别和50个部分标签。我们沿用了PCT[42]中的实验设置。如表10所示，EAT在这个数据集上取得了最好的结果。</p>
<h3 id="49-computational-requirements"><a class="markdownIt-Anchor" href="#49-computational-requirements"></a> 4.9 Computational requirements</h3>
<p>相对于输入的大小，线性复杂度带来了效率上的显著优势。我们将外部注意（EA）模块与标准的自我注意（SA）[16]及其几个变体在参数数量和推理操作方面进行了比较，输入大小为1×512×128×128，给出的结果见表12。外部注意只需要自我注意所需参数的一半，速度快32倍。与最佳变体相比，外部注意的速度仍然是其两倍左右。</p>
<h2 id="5-conclusions"><a class="markdownIt-Anchor" href="#5-conclusions"></a> 5 CONCLUSIONS</h2>
<p>本文提出了外部注意力，这是一种新颖的轻量级但有效的注意力机制，对各种视觉任务都有用。外部注意中采用的两个外部记忆单元可以被看作是整个数据集的字典，并且能够为输入学习更多有代表性的特征，同时降低计算成本。我们希望外部注意力能够激发其在其他领域（如NLP）的实际应用和研究。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/DAPPM(DDRNet)/" title="Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes</div></div><div class="info-2"><div class="info-item-1"> 用于实时和准确道路场景语义分割的深度双分辨率网络 Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes  Abstract 语义分割是自动驾驶汽车理解周围场景的关键技术。当代模型的迷人性能通常是以繁重的计算和漫长的推理时间为代价的，这对于自动驾驶来说是无法忍受的。利用轻量级架构（编码器-解码器或双通道）或在低分辨率图像上进行推理，最近的方法实现了非常快速的场景解析，甚至可以在单个1080Ti GPU上以超过100...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Data-augmentation-using-learned-transformations-for-one-shot-medical-image-segmentation/" title="Data augmentation using learned transformations for one-shot medical image segmentation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Data augmentation using learned transformations for one-shot medical image segmentation</div></div><div class="info-2"><div class="info-item-1"> Data augmentation using learned transformations for one-shot medical image segmentation  文章链接   Abstract 图像分割是许多医疗应用中的一项重要任务。基于卷积神经网络的方法达到了最先进的精确度；然而，它们通常依赖于有监督的训练，并有大量的标记数据集。给医学图像贴标签需要大量的专业知识和时间，典型的手工调整的数据增强方法不能捕捉到此类图像的复杂变化。 我们提出了一种用于合成有标签的医学图像的自动数据增强方法。我们在磁共振成像（MRI）脑部扫描的分割任务上展示了我们的方法。我们的方法只需要一个单一的分割扫描，并以半监督的方式利用其他未标记的扫描。我们从图像中学习一个转换模型，并将该模型与标记的例子一起用来合成额外的标记例子。每个变换都是由空间变形场和强度变化组成的，能够合成复杂的效果，如解剖学和图像采集程序的变化。我们表明，用这些新的例子训练一个有监督的分割器，比最先进的单次生物医学图像分割方法有显著的改进。  1....</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">CA</div></div><div class="info-2"><div class="info-item-1"> Coordinate Attention for Efficient Mobile Network Design  Abstract 最近关于移动网络设计的研究已经证明了通道注意（如SE注意力机制）对于提高模型性能的显著效果，但他们通常忽略了位置信息，而位置信息对于生成空间选择性注意图是很重要的。在本文中，我们提出了一种新的移动网络注意机制，将位置信息嵌入到通道注意中，我们称之为...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#ea"><span class="toc-text"> EA</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-introduction"><span class="toc-text"> 1. INTRODUCTION</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-related-work"><span class="toc-text"> 2. RELATED WORK</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-the-attention-mechanism-in-visual-tasks"><span class="toc-text"> 2.1 The attention mechanism in visual tasks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-self-attention-in-visual-tasks"><span class="toc-text"> 2.2 Self-attention in visual tasks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#23-transformer-in-visual-tasks"><span class="toc-text"> 2.3 Transformer in visual tasks</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-methodology"><span class="toc-text"> 3. METHODOLOGY</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-self-attention-and-external-attention"><span class="toc-text"> 3.1 Self-Attention and External Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-normalization"><span class="toc-text"> 3.2 Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-multi-head-external-attention"><span class="toc-text"> 3.3 Multi-head external attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-experiments"><span class="toc-text"> 4 EXPERIMENTS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-ablation-study"><span class="toc-text"> 4.1 Ablation study</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-visual-analysis"><span class="toc-text"> 4.2 Visual analysis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-image-classification"><span class="toc-text"> 4.3 Image classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#44-object-detection-and-instance-segmentation"><span class="toc-text"> 4.4 Object detection and instance segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#45-semantic-segmentation"><span class="toc-text"> 4.5 Semantic segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#46-image-generation"><span class="toc-text"> 4.6 Image generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#47-point-cloud-classification"><span class="toc-text"> 4.7 Point cloud classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#48-point-cloud-segmentation"><span class="toc-text"> 4.8 Point cloud segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#49-computational-requirements"><span class="toc-text"> 4.9 Computational requirements</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-conclusions"><span class="toc-text"> 5 CONCLUSIONS</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA">CA</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>