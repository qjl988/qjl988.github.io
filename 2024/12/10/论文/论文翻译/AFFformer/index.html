<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Head-Free Lightweight Semantic Segmentation with Linear Transformer | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFF">
<meta property="og:type" content="article">
<meta property="og:title" content="Head-Free Lightweight Semantic Segmentation with Linear Transformer">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFF">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T16:51:00.100Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Head-Free Lightweight Semantic Segmentation with Linear Transformer',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">Head-Free Lightweight Semantic Segmentation with Linear Transformer</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Head-Free Lightweight Semantic Segmentation with Linear Transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T16:51:00.100Z" title="更新于 2024-12-11 00:51:00">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>22分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="head-free-lightweight-semantic-segmentation-with-linear-transformer"><a class="markdownIt-Anchor" href="#head-free-lightweight-semantic-segmentation-with-linear-transformer"></a> Head-Free Lightweight Semantic Segmentation with Linear Transformer</h1>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision Transformer）进行像素嵌入和原型表示，以进一步节省计算成本。此外，从空间域的角度来看，transformer的复杂性很难线性化。由于语义分割对频率信息非常敏感，我们构建了一个轻量级的原型学习块，其复杂度为O(n)的自适应频率滤波器，以O(n2)取代标准的self-attention。在广泛采用的数据集上进行的广泛实验表明，AFFormer在只保留3M参数的情况下实现了卓越的准确性。在ADE20K数据集上，AFFormer实现了41.8 mIoU和4.6 GFLOPs，比Segformer高4.4 mIoU，GFLOPs减少45%。在Cityscapes数据集上，AFFormer实现了78.7 mIoU和34.4 GFLOPs，比Segformer高2.5 mIoU，GFLOPs减少72.5%。</p>
<h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<p>语义分割旨在将图像划分为子区域（像素集合），并被定义为像素级分类任务（Long, Shelhamer, and Darrell 2015; Xie et al. 2021; Zhao et al. 2017; Chen et al. 2018; Strudel et al. 2021; Cheng, Schwing, and Kirillov 2021）全卷积网络（FCN）（Long, Shelhamer, and Darrell 2015）与图像分类相比，它有两个独特的特点：像素级的密集预测和多类表示，通常分别建立在高分辨率的特征上，需要图像语义的全局归纳能力。之前的语义分割方法（Zhao等人，2017；Chen等人，2018；Strudel等人，2021；Xie等人，2021；Cheng, Schwing, and Kirillov，2021；Yuan等人，2021b）主要是以分类网络为骨干，提取多尺度特征，并设计复杂的解码头，建立多尺度特征之间的关系。然而，这些改进是以大的模型尺寸和高的计算成本为代价的。例如，著名的PSPNet（Zhao等人，2017）使用轻量级的MobilenetV2（Sandler等人，2018）作为骨干，在输入尺度为512×512时，包含13.7M的参数和52.2GFLOPs。广泛使用的DeepLabV3+（Chen等人，2018）以相同的骨架需要15.4M参数和25.8GFLOPs。固有的设计方式限制了这一领域的发展，阻碍了许多现实世界的应用。因此，我们提出以下问题：语义分割能否像图像分类一样简单？</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230426151850291.png" alt="image-20230426151850291" /></p>
<blockquote>
<p>图1：左图：不同输入规模下的计算复杂性。与传统方法相比，Segformer（Xie等人，2021）大大降低了计算复杂度，如PSPNet（Zhao等人，2017）和DeepLabV3+（Chen等人，2018），它们以mobilenetV2（Sandler等人，2018）作为骨干。然而，Segformer对于更高的分辨率仍然有巨大的计算负担。对： AFFormer在ADE20K和Cityscapes数据集上取得了更好的准确性，FLOPs明显降低。</p>
</blockquote>
<p>最近，transformer（ViTs）（Liu等人2021；Lee等人2022；Xie等人2021；Strudel等人2021；Cheng, Schwing, and Kirillov 2021；Xu等人2021；Lee等人2022）在语义分割方面显示出巨大的潜力，然而，当它们部署在超低计算能力设备上时，面临着平衡性能和内存使用的挑战。标准变形金刚在空间域的计算复杂度为O(n2)，其中n为输入分辨率。现有的方法通过减少标记的数量（Xie等人2021；Wang等人2021；Liang等人2022；Ren等人2022）或滑动窗口（Liu等人2021；Yuan等人2021a）来缓解这种情况，但它们对计算复杂性的减少有限，甚至会损害分割任务的全局或局部语义。同时，语义分割作为一个基础研究领域，有着广泛的应用场景，需要处理各种分辨率的图像。如图1所示，尽管众所周知的高效Segformer（Xie等人，2021）与PSPNet和DeepLabV3+相比取得了很大的突破，但对于更高的分辨率，它仍然面临巨大的计算负担。在512×512的尺度上，尽管Segformer与PSPNet和DeepLabV3+相比非常轻，但它的成本几乎是我们的两倍（8.4 GFLOPs vs 4.6 GFLOPs）；在2048×2048的尺度上，甚至需要5倍的GFLOPs（384.3 GFLOPs vs 73.2 GFLOPs）。因此，我们提出了另一个问题：我们能否为超低计算场景下的语义分割设计一个高效、轻量的Transformer网络？</p>
<p>上述两个问题的答案是肯定的。为此，我们提出了一个无头轻量级语义分割的具体架构，名为自适应频率变换器（AFFormer）。受ViT保持单一的高分辨率特征图以保持细节（Dosovitskiy等人，2021）和金字塔结构降低分辨率以探索语义并降低计算成本（He等人，2016；Wang等人，2021；Liu等人，2021）的特性启发，AFFormer采用并行结构，利用原型表征作为特定的可学习局部描述，取代解码器并保留高分辨率特征上的丰富图像语义。并行结构通过移除解码器压缩了大部分的计算，但对于超低的计算资源来说，这还是不够的。此外，我们对像素嵌入特征和局部描述特征采用了异质算子，以节省更多的计算成本。一个名为prototype learning (PL)的基于变换器的模块被用来学习原型表征，而一个名为pixel descriptor (PD)的基于卷积的模块将像素嵌入特征和学到的原型表征作为输入，将它们转换回完整的像素嵌入空间以保留高分辨率语义。</p>
<p>然而，从空间域的角度来看，要将transformer的复杂性线性化仍然非常困难。受频率对分类任务影响的启发（Rao等人，2021；Wang等人，2020），我们发现，语义分割对频率信息也非常敏感。因此，**我们构建了一个复杂度为O(n)的轻量级自适应频率滤波器作为原型学习，以取代O(n2)的标准自我注意。**这个模块的核心是由频率相似性核、动态低通和高通滤波器组成，它们分别从强调重要的频率成分和动态过滤频率的角度捕捉有利于语义分割的频率信息。最后，通过共享高、低频提取和增强模块的权重，进一步降低了计算成本。**我们还在前馈网络（FFN）层中嵌入了一个简化的深度卷积层以增强融合效果，**减少了两个矩阵变换的大小。</p>
<p>在并行异构结构和自适应频率滤波器的帮助下，**我们只用一个卷积层作为分类层（CLS）来处理单尺度特征，实现了最佳性能，**使语义分割像图像分类一样简单。我们在三个广泛使用的数据集上证明了所提出的AFFormer的优势： ADE20K、Cityscapes和COCO-stuff。仅用3个参数，AFFormer就大大超过了最先进的轻型方法。在ADE20K上，AFFormer以4.6 GFLOPs实现了41.8 mIoU，比Segformer高出4.4 mIoU，同时GFLOPs减少45%。在Cityscapes上，AFFormer实现了78.7 mIoU和34.4 GFLOPs，比Segformer高2.5 mIoU，GFLOPs减少72.5%。广泛的实验结果表明，在计算受限的情况下应用我们的模型是可能的，而且在不同的数据集上仍然保持高性能和稳健性。</p>
<h2 id="related"><a class="markdownIt-Anchor" href="#related"></a> Related</h2>
<h3 id="work-semantic-segmentation"><a class="markdownIt-Anchor" href="#work-semantic-segmentation"></a> Work Semantic Segmentation</h3>
<p>语义分割被认为是一项像素分类任务（Strudel等人，2021；Xu等人，2017；Xie等人，2021）。在过去的两年里，出现了基于transformer的新范式，这些范式通过查询或动态内核实现掩模分类（Zhang等人2021；Li等人2022；Cheng, Schwing, and Kirillov 2021；Cheng等人2022）。例如，Maskformer（Cheng, Schwing, and Kirillov 2021）学习对象查询并将其转换为面具的嵌入。Mask2former（Cheng等人，2022）用一个强大的多尺度掩模Transformer（Zhu等人，2021）加强了查询学习。K-Net（Zhang等人，2021）采用了动态内核来生成掩码。MaskDINO（Li等人，2022）将物体检测引入语义分割，进一步提高了查询能力。然而，由于学习高效查询和动态内核的计算成本很高，上述方法都不适合低计算能力的场景。我们认为，这些范式的本质是通过用单个表征代替整体来更新像素语义。因此，我们利用像素嵌入作为特定的可学习的局部描述，提取图像和像素语义并允许语义互动。</p>
<h3 id="efficient-vision-transformers"><a class="markdownIt-Anchor" href="#efficient-vision-transformers"></a> Efficient Vision Transformers</h3>
<p>transformer的轻量级解决方案主要集中在对自我注意力的优化上，包括以下方式：减少令牌长度（Wang等人2021；Xie等人2021；Wang等人2022）和使用局部窗口（Liu等人2021；Yuan等人2021a）。PVT（Wang等人，2021）通过空间缩减对键和值进行空间压缩，PVTv2（Wang等人，2022）通过池化操作进一步取代了空间缩减，但这种方式会丢失很多细节。Swin（Liu等人，2021；Yuan等人，2021a）通过将自我注意力限制在局部窗口，大大减少了令牌的长度，而这些违背了Transformer的全局性，限制了全局接受域。同时，许多轻量级设计（Chen et al. 2022; Mehta and Rastegari 2022）在MobileNet中引入Transformer，以获得更多的全局语义，但这些方法仍然受到传统Transformer的方形计算复杂性的影响。Mobile-Former（Chen等人，2022）结合了MobileNet（Sandler等人，2018）和Transformer（Dosovitskiy等人，2021）的并行设计，可以实现本地和全局特征的双向融合性能，远远超过MobileNetV3等轻量级网络。然而，它只使用非常少的标记，这不利于语义分割任务。</p>
<h2 id="method"><a class="markdownIt-Anchor" href="#method"></a> Method</h2>
<p>在本节中，我们介绍了用于语义分割的轻型并行异构网络。首先证明了用并行异构网络取代语义解码器的基本信息。然后，我们介绍了像素描述和语义频率的建模。最后，我们讨论了并行架构的具体细节和计算开销。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>b</mi><mi>o</mi><mi>l</mi><mi>d</mi><mi>s</mi><mi>y</mi><mi>m</mi><mi>b</mi><mi>o</mi><mi>l</mi><mi>G</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></msubsup><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">boldsymbol{G}(s)=sum_{i=0}^nw_ix_i
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">d</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">m</span><span class="mord mathnormal">b</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord"><span class="mord mathnormal">G</span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9613919999999999em;vertical-align:-0.247em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">u</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/image-20240130210316093.png" alt="image-20240130210316093" /></p>
<blockquote>
<p>图2：自适应频率变换器（AFFormer）的概述。我们首先展示平行异构网络的整体结构。具体来说，首先对贴片嵌入后的特征F进行聚类，得到原型特征G，从而构建一个平行网络结构，其中包括两个异质算子。基于变换器的模块作为原型学习，捕捉G中有利的频率成分，得到原型表示G′。最后G′被一个基于CNN的像素描述器恢复，产生F′，用于下一阶段。</p>
</blockquote>
<h3 id="parallel-heterogeneous-architecture"><a class="markdownIt-Anchor" href="#parallel-heterogeneous-architecture"></a> Parallel Heterogeneous Architecture</h3>
<p>语义解码器将编码器获得的图像语义传播到每个像素，并恢复降采样时丢失的细节。一种直接的替代方法是在高分辨率特征中提取图像语义，但这会带来大量的计算，尤其是对于视觉变换器而言。相比之下，我们提出了一种用原型语义描述像素语义信息的新策略。在每个阶段，给定一个特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>i</mi><mi>n</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>b</mi><mi>b</mi><msup><mi>R</mi><mrow><mi>H</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>W</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">Finmathbb{R}^{Htimes Wtimes C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">b</span><span class="mord mathnormal">b</span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span></span></span></span></span></span></span></span></span>后，我们首先初始化一个网格 G∈Rh×w×C 作为图像的原型，其中 G 中的每个点都作为局部聚类中心，初始状态只包含周围区域的信息。在这里，我们使用 1 × C 向量来表示每个点的本地语义信息。对于每个特定像素点，由于周围像素点的语义并不一致，因此每个聚类中心之间存在语义重叠。聚类中心在其对应的区域 α2 中进行加权初始化，每个聚类中心的初始化表示为</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230426142250414.png" alt="image-20230426142144244" /></p>
<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">n=alpha times alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">p</span><span class="mord mathnormal">h</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mord mathnormal">s</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">p</span><span class="mord mathnormal">h</span><span class="mord mathnormal">a</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>表示<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的权重，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">α</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>设为3。我们的目的是更新网格G中的每个簇中心s，而不是直接更新特征F。由于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>×</mo><mi>w</mi><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">h×w  H×W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>，它大大简化了计算。</p>
<p>在这里，我们使用一个基于Transformer的模块作为原型学习来更新每个聚类中心，总共包含L层，更新后的中心表示为G′(s)。对于每个更新的聚类中心，我们通过一个像素描述符来恢复它。让F′i表示恢复的特征，它不仅包含来自F的丰富的像素语义，还包含集群中心G′(s)所收集的原型语义。由于聚类中心聚集了周围像素的语义，导致了局部细节的损失，因此PD首先用像素语义来模拟F中的局部细节。具体来说，F被投射到一个低维空间，在像素之间建立局部关系，使每个局部斑块保持一个独特的边界。然后G′(s)被嵌入到F中，通过双线性插值恢复到原始空间特征F′。最后，它们通过线性投影层被整合。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230426142325152.png" alt="image-20230426153043215" /></p>
<blockquote>
<p>图3：不同频率成分对语义分割的影响。我们使用切边方法Segformer（Xie等人，2021）来评估频率成分对广泛使用的ADE20K数据集（Zhou等人，2017）的语义分割的影响。通过快速傅里叶变换（Heideman, Johnson, and Burrus 1984）将图像转换到频域，并使用半径为的低通算子将高频信息过滤掉。去除不同层次的高频成分后，预测性能会明显下降。</p>
</blockquote>
<h3 id="prototype-learning-by-adaptive-frequency-filter"><a class="markdownIt-Anchor" href="#prototype-learning-by-adaptive-frequency-filter"></a> Prototype Learning by Adaptive Frequency Filter</h3>
<p>**动机:**语义分割是一项极其复杂的像素级分类任务，容易出现类别混淆。频率表示可以作为学习类别间差异的新范式，它可以挖掘出人类视觉所忽略的信息（Zhong等人，2022；Qian等人，2020）。如图3所示，人类对频率信息的去除是稳健的，除非绝大部分的频率成分被过滤掉了。然而，该模型对频率信息的去除极为敏感，即使去除少量的频率信息也会导致性能的显著下降。这表明，对于该模型来说，挖掘更多的频率信息可以增强类别之间的差异，使每个类别之间的边界更加清晰，从而提高语义分割的效果。</p>
<p>由于特征F包含丰富的频率特征，网格G中的每个聚类中心也会收集这些频率信息。在上述分析的激励下，在网格G中提取更多有益的频率有助于区分每个聚类的属性。为了提取不同的频率特征，直接的方法是通过傅里叶变换将空间域特征转化为频谱特征，并在频域中使用一个简单的掩码滤波器来增强或衰减频谱中每个频率分量的强度。然后通过反傅里叶变换将提取的频率特征转换到空间域。然而，傅里叶变换和反变换带来了额外的计算费用，而且许多硬件不支持这种运算。因此，我们从光谱相关的角度出发，在香草transformer的基础上设计了一个自适应频率滤波器块，以直接在空间域捕获重要的高频和低频特征。核心部件如图4所示，公式定义为：：</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230426152029338.png" alt="image-20230426142250414" /></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>D</mi><mi>h</mi><mrow><mi>f</mi><mi>c</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">D_{h}^{fc}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2683239999999998em;vertical-align:-0.3013079999999999em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9670159999999999em;"><span style="top:-2.3986920000000005em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span style="top:-3.1809080000000005em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mord mathnormal mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013079999999999em;"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>D</mi><mi>m</mi><mrow><mi>l</mi><mi>f</mi></mrow></msubsup><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{m}^{lf}(X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0991079999999998em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>D</mi><mi>n</mi><mrow><mi>h</mi><mi>f</mi></mrow></msubsup><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{n}^{hf}(X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0991079999999998em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>分别表示为实现频率成分相关性增强而采用的H组的频率相似性核，M组的动态低通滤波器和N组的动态高通滤波器。||·||表示串联。值得注意的是，这些运算符采用了并行结构，通过共享权重来进一步降低计算成本。</p>
<h3 id="frequency-similarity-kernel-fsk"><a class="markdownIt-Anchor" href="#frequency-similarity-kernel-fsk"></a> Frequency Similarity Kernel (FSK)</h3>
<p>不同的频率成分分布在G中，而我们的目的是选择和加强有助于语义解析的重要成分。为此，我们设计了一个频率相似性内核模块。一般来说，这个模块是由transformer实现的。给定一个特征X∈R(hw)×C，通过卷积层对G进行相对位置编码（Wu等人，2021）。我们首先使用一个固定大小的相似性核A∈RC/H×C/H来表示不同频率成分之间的对应关系，并通过查询相似性核来选择重要的频率成分。我们将其视为一个函数转移，通过线性层计算出频率成分的键值K和值V，并通过Softmax操作将各频率成分的键值归一化。每个成分都集成了一个相似性核Ai,j，其计算方式为：</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230426142410927.png" alt="image-20230426142325152" /></p>
<p>其中ki代表K中的第i个频率成分，vj代表V中的第j个频率成分。我们还通过线性层将输入X转化为查询Q，并通过对固定大小的相似性核的交互作用获得分量增强的输出。</p>
<h3 id="dynamic-low-pass-filters-dlf"><a class="markdownIt-Anchor" href="#dynamic-low-pass-filters-dlf"></a> Dynamic Low-Pass Filters (DLF)</h3>
<p>低频成分占据了绝对图像中的大部分能量，代表了大部分的语义信息。一个低通滤波器允许低于截止频率的信号通过，而高于截止频率的信号则被阻断。因此，我们采用典型的平均集合作为低通滤波器。然而，不同图像的截止频率是不同的。为此，我们在多组中控制不同的内核和步长来产生动态低通滤波器。对于第m组，我们有：<img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230426142344156.png" alt="image-20230426142344156" /></p>
<p>其中B(-)表示双线性插值，Γs×s表示输出大小为s×s的自适应平均集合。</p>
<h3 id="dynamic-high-pass-filters-dhf"><a class="markdownIt-Anchor" href="#dynamic-high-pass-filters-dhf"></a> Dynamic High-Pass Filters (DHF)</h3>
<p>高频信息对于保留分割中的细节至关重要。作为一个典型的高通算子，卷积可以过滤掉不相关的低频冗余成分，保留有利的高频成分。高频成分决定了图像的质量，而每幅图像的高通的截止频率是不同的。因此，我们把数值V分成N组，得出vn。对于每一组，我们使用不同内核的卷积层来模拟不同高通滤波器中的截止频率。对于第n组，我们有：</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230426154858983.png" alt="image-20230426142410927" /></p>
<p>其中Λk×k表示深度卷积层，核大小为k×k。此外，我们使用查询和高频特征的Hadamard乘积来抑制物体内部的高频，这些高频是分割的噪声。</p>
<p>FFN有助于融合捕获的频率信息，但拥有大量的计算量，这在轻量级设计中经常被忽略。在这里，我们通过引入卷积层来减少隐藏层的维度，以弥补由于维度压缩而缺失的能力。</p>
<h3 id="discuss"><a class="markdownIt-Anchor" href="#discuss"></a> Discuss</h3>
<p>对于频率相似性核，其计算复杂度为O(hwC2)。每个动态高通滤波器的计算复杂度为O(hwCk2)，比频率相似性核的计算复杂度小得多。由于动态低通滤波器是通过每组的自适应均值池实现的，其计算复杂度约为O(hwC)。因此，一个模块的计算复杂度与分辨率呈线性关系，这对语义分割中的高分辨率是有利的。</p>
<h2 id="experiments"><a class="markdownIt-Anchor" href="#experiments"></a> Experiments</h2>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230426154941898.png" alt="image-20230426154858983" /></p>
<blockquote>
<p>表1：与ADE20K上分辨率为512×512的先进方法的比较。这里我们用Segformer作为基线，并报告增长百分比。MV2=MobileNetV2, EN=EfficientNet, SV2=ShuffleNetV2.</p>
</blockquote>
<h3 id="implementation-details"><a class="markdownIt-Anchor" href="#implementation-details"></a> Implementation Details</h3>
<p>我们在三个公开的数据集上验证了提议的AFFormer： ADE20K（Zhou等人，2017）、Cityscapes（Cordts等人，2016）和COCO-stuff（Caesar, Uijlings, and Ferrari，2018）。我们用基于MMSegmentation工具箱的PyTorch框架实现我们的AFFormer（贡献者2020）。按照以前的工作（Cheng, Schwing, and Kirillov 2021; Zhao et al. 2017），我们使用ImageNet-1k来预训练我们的模型。在语义分割训练期间，我们对所有数据集采用广泛使用的AdamW优化器来更新模型参数。为了进行公平的比较，我们的训练参数主要遵循以前的工作（Xie等人，2021）。对于ADE20K和Cityscapes数据集，我们采用Segformer中默认的训练迭代160K，其中mini-batchsize分别设置为16和8。对于COCO-stuff数据集，我们将训练迭代次数设置为80K，迷你批处理为16。此外，我们在ADE20K、Cityscapes、COCO-stuff的训练过程中分别通过随机水平翻转、随机调整大小的比例为0.5-2.0、随机裁剪为512×512、1024×1024、512×512来实现数据增强。我们用平均相交于联合（mIoU）的指标来评估结果。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230426153043215.png" alt="image-20230426154941898" /></p>
<blockquote>
<p>表2：与Cityscapes估值集上的最新方法的比较。FLOPs是在1024×2048的分辨率下测试的。同时，我们还报告了与Segformer相比增加的百分比。</p>
</blockquote>
<h3 id="comparisons-with-existing-works"><a class="markdownIt-Anchor" href="#comparisons-with-existing-works"></a> Comparisons with Existing Works</h3>
<h4 id="results-on-ade20k-dataset"><a class="markdownIt-Anchor" href="#results-on-ade20k-dataset"></a> Results on ADE20K Dataset.</h4>
<p>我们将我们的AFFormer与排名靠前的语义分割方法进行比较，包括基于CNN和基于视觉的Transformer模型。按照（Xie等人，2021）中的推理设置，我们在512×512的分辨率下测试FLOPs，并在表1中显示了单比例结果。我们的模型AFFormer-base在与Lite-ASPP相同的计算能力消耗下提高了5.2 mIoU，达到41.8 mIoU。同时，通过减少层数和通道数，我们得到了AFFormertiny和AFFormer-small版本，以适应不同的计算能力情况。对于轻量级和高效的Segformer（8.4GFLOPs），我们的基础版本（4.6GFLOPs）也用一半的计算能力获得了4.4mIoU，而微小版本（2.4GFLOPs）仅用1/4的计算能力提高了1.3mIoU。更轻的topformer只需要1.8GFLOPs，但我们的基本版本少了210万个参数（5.1M vs 3M），mIoU提高了4.0。</p>
<h4 id="results-on-cityscapes-dataset"><a class="markdownIt-Anchor" href="#results-on-cityscapes-dataset"></a> Results on Cityscapes Dataset.</h4>
<p>表2显示了我们的模型和前沿方法在Cityscapes上的结果。尽管Segformer足够高效，但由于其方形级别的复杂性，我们只用了30%的计算成本就达到了78.7 mIoU，在FLOPs减少70%的情况下，提高了2.5 mIoU。同时，我们在表3中报告了不同高分辨率下的结果。在{512, 640, 768, 1024}的短边上，我们模型的计算成本分别是Segformer的51.4%, 57.5%, 62.5%和72.5%。同时，mIoU分别提高了1.6、1.9、1.2和2.5。输入的分辨率越高，我们的模型在计算成本和准确性方面就越有优势。</p>
<h4 id="results-on-coco-stuff-dataset"><a class="markdownIt-Anchor" href="#results-on-coco-stuff-dataset"></a> Results on COCO-stuff Dataset.</h4>
<p>COCO-stuff数据集包含了大量在COCO中收集的困难样本。如表4所示，尽管复杂的解码器（如PSPNet、DeepLabV3+）可以取得比LR-ASPP（MV3）更好的结果，但它们带来了大量的计算成本。我们的模型实现了35.1 mIoU的精度，而只占用了4.5 GFLOPs，实现了最佳的权衡。</p>
<h4 id="ablation-studies"><a class="markdownIt-Anchor" href="#ablation-studies"></a> Ablation Studies</h4>
<p>除非另有说明，所有的消融研究都是在ADE20K数据集和AFFormer-base上进行的。</p>
<h4 id="rationalization-of-parallel-structures"><a class="markdownIt-Anchor" href="#rationalization-of-parallel-structures"></a> Rationalization of Parallel Structures.</h4>
<p>并行结构是去除解码器头并确保准确性和效率的关键。我们首先将提议的结构调整为天真的金字塔结构（表示为 “w/o PD”）和ViT结构（表示为 “w/o PL”），以说明并行结构的优势。具体来说，&quot;w/o PD &quot;意味着去除PD模块，只保留PL模块，而 &quot;w/o PL &quot;则相反。如表5所示，由于缺乏高分辨率的像素语义信息，&quot;w/o PD &quot;的设置减少了2.6 mIoU。没有金字塔结构的 &quot;w/o PL &quot;由于参数少，缺乏丰富的图像语义信息，准确率明显下降。这也证明了我们的并行架构可以有效地结合两种架构的优势。</p>
<h4 id="advantages-of-heterogeneous-structure"><a class="markdownIt-Anchor" href="#advantages-of-heterogeneous-structure"></a> Advantages of Heterogeneous Structure.</h4>
<p>采用异质方法的目的是为了进一步减少计算开销。采用PL模块学习聚类特征中的原型表示，然后用PD结合原始特征进行修复，这样就避免了对高分辨率原始特征的直接计算，降低了计算成本。从表6可以看出，当并行分支调整为像素描述模块（表示为 “全PD”）时，即通过PD模块学习原型表示。模型大小只有0.6M，FLOPs减少了2.5G，但精度却降低了14.3mIoU。这是由于PD缺乏学习伟大原型表征的能力。相比之下，当我们用PL模块（表示为 “所有PL”）取代PD模块后，FLOPs增加了2.4G，但准确性几乎没有差别。我们认为，PD模块实际上只是一个简单的恢复所学原型的方法，而相对复杂的PL模块使模型容量饱和。</p>
<h4 id="advantages-of-adaptive-frequency-filter"><a class="markdownIt-Anchor" href="#advantages-of-adaptive-frequency-filter"></a> Advantages of Adaptive Frequency Filter.</h4>
<p>我们使用两个差异较大的数据集，包括ADE20K和Cityscapes，来探索自适应频率滤波器模块的核心部件。主要原因是ADE20K数据集的上限只有40 mIoU，而Cityscapes的上限是80 mIoU。这两个数据集对不同频率有不同程度的敏感度。我们在表7中报告了每个内部组件的好处。我们发现，DHF单独优于DLF，特别是在Cityscapes数据集上优于2.6 mIoU，而FSK在ADE20K上明显高于DLF和DHF。这表明ADE20K可能更倾向于高频和低频之间的中间状态，而Cityscapes则更需要高频信息。综合实验表明，结合各部分的优势可以稳定地提高ADE20K和Cityscapes的结果。</p>
<h4 id="frequency-statistics-visualization"><a class="markdownIt-Anchor" href="#frequency-statistics-visualization"></a> Frequency Statistics Visualization.</h4>
<p>我们首先统计不同阶段的特征频率分布，如图5所示。可以发现，G2和F2的曲线几乎重合，说明聚类后的频率与原始特征中的频率非常相似。G3和F3的情况也是如此。而经过频率自适应滤波后，学习到的原型表示明显改善了所包含的频率信息。在PD修复后，不同的频率成分可以在不同阶段得到强调。如图6所示，我们也分析了AFF模块中核心成分的频率影响。正如预期的那样，DLF和DHF分别表现出强大的低通和高通能力，就像FSK一样。同时，我们还发现，FSK筛选和增强的重要频率成分主要集中在高频部分，但该频率信号比DHF更饱和。这也说明高频分量部分在语义分割任务中尤为重要，因为它更强调物体间的边界细节和纹理差异。同时，根据表7的分析（ADE20K和Cityscapes的效果得到了稳步提升），每个核心组件都有自己的优势，AFF模块在各种类型和复杂场景中表现出强大的鲁棒性。</p>
<h4 id="speed-and-memory-costs"><a class="markdownIt-Anchor" href="#speed-and-memory-costs"></a> Speed and Memory Costs.</h4>
<p>同时，我们在表8中报告了城市景观数据集的速度。我们可以发现，所提出的模型提高了10 FPS，在这种高分辨率的城市景观图像上的表现比Segformer好很多。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230426155135820.png" alt="image-20230426155119202" /></p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230426155119202.png" alt="image-20230426155135820" /></p>
<h2 id="conclusion"><a class="markdownIt-Anchor" href="#conclusion"></a> Conclusion</h2>
<p>在本文中，我们提出了AFFormer，一个无头的轻量级语义分割具体架构。其核心是从频率角度学习聚类原型的局部描述表示，而不是直接学习所有的像素嵌入特征。它去掉了复杂的解码器，同时拥有线性复杂度转化器，实现了语义分割与常规分类一样简单。各种实验表明，AFFormer在低计算成本的情况下拥有强大的准确性和极大的稳定性和鲁棒性。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">CA</div></div><div class="info-2"><div class="info-item-1"> Coordinate Attention for Efficient Mobile Network Design  Abstract 最近关于移动网络设计的研究已经证明了通道注意（如SE注意力机制）对于提高模型性能的显著效果，但他们通常忽略了位置信息，而位置信息对于生成空间选择性注意图是很重要的。在本文中，我们提出了一种新的移动网络注意机制，将位置信息嵌入到通道注意中，我们称之为...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BAM</div></div><div class="info-2"><div class="info-item-1"> BAM  Abstract 近来，小样本分割分割技术（FSS）得到了广泛的发展。然而，训练出来的模型偏向于所看到的类别，而不是理想的类别无关性，从而阻碍了对新内容的识别。本文提出了一种新颖而直接的见解来缓解这一问题。具体来说，我们在传统的 FSS 模型（元学习器）上增加了一个分支（基础学习器），以明确识别基础类别的目标，即不需要分割的区域。然后，对这两个学习器并行输出的粗略结果进行自适应整合，从而得出精确的分割预测结果。考虑到元学习器的敏感性，我们进一步引入了一个调整因子来估计输入图像对之间的场景差异，以促进模型的集合预测。在 PASCAL-5i 和 COCO-20i 上取得的显著性能提升验证了这一方法的有效性，而且令人惊讶的是，即使使用两个普通学习器，我们的多功能方案也创造了新的一流水平。此外，鉴于所提方法的独特性，我们还将其扩展到了更现实但更具挑战性的环境中，即广义 FSS，在这种环境中，基础类和新类别的像素都需要确定。  1. Introduction   得益于成熟的大规模数据集 [8, 9,...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#head-free-lightweight-semantic-segmentation-with-linear-transformer"><span class="toc-text"> Head-Free Lightweight Semantic Segmentation with Linear Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-text"> Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#related"><span class="toc-text"> Related</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#work-semantic-segmentation"><span class="toc-text"> Work Semantic Segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#efficient-vision-transformers"><span class="toc-text"> Efficient Vision Transformers</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#method"><span class="toc-text"> Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#parallel-heterogeneous-architecture"><span class="toc-text"> Parallel Heterogeneous Architecture</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#prototype-learning-by-adaptive-frequency-filter"><span class="toc-text"> Prototype Learning by Adaptive Frequency Filter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#frequency-similarity-kernel-fsk"><span class="toc-text"> Frequency Similarity Kernel (FSK)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dynamic-low-pass-filters-dlf"><span class="toc-text"> Dynamic Low-Pass Filters (DLF)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dynamic-high-pass-filters-dhf"><span class="toc-text"> Dynamic High-Pass Filters (DHF)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#discuss"><span class="toc-text"> Discuss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experiments"><span class="toc-text"> Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#implementation-details"><span class="toc-text"> Implementation Details</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#comparisons-with-existing-works"><span class="toc-text"> Comparisons with Existing Works</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#results-on-ade20k-dataset"><span class="toc-text"> Results on ADE20K Dataset.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#results-on-cityscapes-dataset"><span class="toc-text"> Results on Cityscapes Dataset.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#results-on-coco-stuff-dataset"><span class="toc-text"> Results on COCO-stuff Dataset.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ablation-studies"><span class="toc-text"> Ablation Studies</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#rationalization-of-parallel-structures"><span class="toc-text"> Rationalization of Parallel Structures.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#advantages-of-heterogeneous-structure"><span class="toc-text"> Advantages of Heterogeneous Structure.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#advantages-of-adaptive-frequency-filter"><span class="toc-text"> Advantages of Adaptive Frequency Filter.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#frequency-statistics-visualization"><span class="toc-text"> Frequency Statistics Visualization.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#speed-and-memory-costs"><span class="toc-text"> Speed and Memory Costs.</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusion"><span class="toc-text"> Conclusion</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA">CA</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>