<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>SegNeXt | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="SegNeXt  文章链接   Abstract 我们提出了SegNeXt，一个用于语义分割的简单卷积网络架构。由于自我注意在编码空间信息方面的效率，最近基于transformer的模型在语义分割领域占主导地位。在本文中，我们表明卷积注意是一种比transformer中的自我注意机制更有效率和效果的编码上下文信息的方式。通过重新审视成功的分割模型所拥有的特征，我们发现了导致分割模型性能提高的几个">
<meta property="og:type" content="article">
<meta property="og:title" content="SegNeXt">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/SegNeXt/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="SegNeXt  文章链接   Abstract 我们提出了SegNeXt，一个用于语义分割的简单卷积网络架构。由于自我注意在编码空间信息方面的效率，最近基于transformer的模型在语义分割领域占主导地位。在本文中，我们表明卷积注意是一种比transformer中的自我注意机制更有效率和效果的编码上下文信息的方式。通过重新审视成功的分割模型所拥有的特征，我们发现了导致分割模型性能提高的几个">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:03:42.640Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/SegNeXt/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'SegNeXt',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">SegNeXt</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">SegNeXt</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:03:42.640Z" title="更新于 2024-12-11 01:03:42">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>19分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="segnext"><a class="markdownIt-Anchor" href="#segnext"></a> SegNeXt</h1>
<blockquote>
<p><a href="zotero://open-pdf/library/items/HUI65XBR">文章链接</a></p>
</blockquote>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>我们提出了SegNeXt，一个用于语义分割的简单卷积网络架构。由于自我注意在编码空间信息方面的效率，最近基于transformer的模型在语义分割领域占主导地位。在本文中，我们表明卷积注意是一种比transformer中的自我注意机制更有效率和效果的编码上下文信息的方式。通过重新审视成功的分割模型所拥有的特征，我们发现了导致分割模型性能提高的几个关键成分。这促使我们设计了一个新颖的卷积注意力网络，它使用廉价的卷积操作。在没有任何附加条件的情况下，我们的SegNeXt大大改善了以前最先进的方法在流行基准上的性能，包括ADE20K、Cityscapes、COCO-Stuff、Pascal VOC、Pascal Context和iSAID。值得注意的是，SegNeXt的性能优于EfficientNet-L2 w/NAS-FPN，在Pascal VOC 2012测试排行榜上只用了它的1/10的参数就达到了90.6%的mIoU。与最先进的方法相比，SegNeXt在ADE20K数据集上以相同或更少的计算量平均实现了约2.0%的mIoU改进。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221128143048861.png" alt="image-20221128143048861" /></p>
<blockquote>
<p>表1 我们从成功的语义分割方法中观察到有利于提升模型性能的属性。这里，n指的是像素或token的数量。强编码器表示强骨干，如ViT[17]和VAN[24]。</p>
</blockquote>
<h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1 Introduction</h2>
<p>作为计算机视觉中最基本的研究课题之一，语义分割旨在为每个像素分配一个语义类别，在过去十年中引起了极大的关注。从早期基于CNN的模型，如FCN[53]和DeepLab系列[4, 6, 8]，到最近基于transformer的方法，如SETR[96]和SegFormer[80]，语义分割模型在网络结构方面经历了重大变革。</p>
<p>通过重新审视以前成功的语义分割工作，我们总结了不同模型所拥有的几个关键属性，如表1所示。1. 基于上述观察，</p>
<p><strong>我们认为一个成功的语义分割模型应该具有以下特点：</strong></p>
<p>(i) **强大的骨干网络作为编码器。**与之前基于CNN的模型相比，基于transformer的模型的性能提升主要来自于更强的骨干网络。</p>
<p>(ii) **多尺度信息交互。**与图像分类任务大多识别单一物体不同，语义分割是一个密集的预测任务，因此需要处理单一图像中不同大小的物体。</p>
<p>(iii) **空间注意力。**空间注意力允许模型通过对语义区域内的区域进行优先排序来进行分割。</p>
<p>(iv) **低计算复杂性。**在处理遥感和城市场景的高分辨率图像时，这一点尤其关键。</p>
<p>考虑到上述分析，在本文中，我们重新思考了卷积注意的设计，并提出了一个用于语义分割的高效而有效的编码器-解码器架构。与之前的基于transformer的模型不同，我们的方法是将解码器中的卷积作为特征细化器，倒置了transformer-卷积编码器-解码器架构。<strong>具体来说，对于我们编码器中的每个块，我们翻新了传统卷积块的设计，并利用多尺度的卷积特征，通过一个简单的元素明智的乘法来唤起空间注意力，遵循[24]。我们发现这样一种简单的建立空间注意力的方法比标准卷积和空间信息编码中的自我注意力都要有效。对于解码器，我们从不同的阶段收集多层次的特征，并使用Hamburger[21]来进一步提取全局背景。在这种情况下，我们的方法可以获得从局部到全局的多尺度背景，实现空间和通道维度的适应性，并从低级到高级的信息聚合。</strong></p>
<p>我们的网络被称为SegNeXt，除了解码器部分，大部分由卷积运算组成，其中包含一个基于分解的Hamburger模块[21]（Ham）用于全局信息提取。这使得我们的SegNeXt比以前严重依赖transformer的分割方法要高效得多。如图1所示，SegNeXt明显优于最近基于transformer的方法。特别是我们的SegNeXt-S在处理Cityscapes数据集中的高分辨率城市场景时，只用了大约1/6（124.6G vs. 717.1G）的计算成本和1/2的参数（13.9M vs. 27.6M）就超过了SegFormer-B2（81.3% vs. 81.0%）。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221128150139749.png" alt="image-20221128150139749" /></p>
<blockquote>
<p>图1 在Cityscapes（左）和ADE20K（右）验证集上的性能-计算曲线。计算FLOPs时，Cityscapes的输入尺寸为2，048×1，024，ADE20K为512×512。圆圈的大小表示参数的数量。更大的圆圈意味着更多的参数。我们可以看到，我们的SegNeXt实现了分割性能和计算复杂性之间的最佳权衡。</p>
</blockquote>
<p>我们的贡献可以概括为以下几点：</p>
<ul>
<li>我们确定了一个好的语义分割模型应该拥有的特征，并提出了一个新颖的定制网络架构，称为SegNeXt，它通过多尺度卷积特征唤起空间注意。</li>
<li>我们表明，一个具有简单而廉价的卷积的编码器仍然可以比视觉transformer表现得更好，特别是在处理物体细节时，而它所需要的计算成本要低得多。</li>
<li>我们的方法在各种分割基准上，包括ADE20K、Cityscapes、COCO-Stuff、Pascal VOC、Pascal Context和iSAID，将最先进的语义分割方法的性能提高了很多。</li>
</ul>
<h2 id="2-related-work"><a class="markdownIt-Anchor" href="#2-related-work"></a> 2 Related Work</h2>
<h3 id="21-semantic-segmentation"><a class="markdownIt-Anchor" href="#21-semantic-segmentation"></a> 2.1 Semantic Segmentation</h3>
<p>语义分割是一项基本的计算机视觉任务。自FCN[53]提出以来，卷积神经网络（CNN）[1, 64, 86, 94, 19, 87, 71, 20, 45]取得了巨大的成功，成为语义分割的流行架构。最近，基于transformer的方法[96, 80, 88, 65, 63, 44, 11, 10]已经显示出巨大的潜力，并超过了基于CNN的方法。</p>
<p>在深度学习时代，分割模型的结构可以大致分为两部分：编码器和解码器。对于编码器，研究人员通常采用流行的分类网络（如ResNet[27]、ResNeXt[81]和DenseNet[32]），而不是定制的架构。然而，语义分割是一种密集预测任务，它与图像分类不同。分类的改进可能不会出现在具有挑战性的分割任务中[28]。因此，出现了一些定制的编码器，包括Res2Net[20]、HRNet[71]、SETR[96]、SegFormer[80]、HRFormer[88]、MPViT[38]、DPT[63]，等等。对于解码器来说，它经常与编码器合作使用，以达到更好的效果。针对不同的目标，有不同类型的解码器，<strong>包括实现多尺度感受野[94, 7, 78]，收集多尺度语义[64, 80, 8]，扩大感受野[5, 4, 62]，加强边缘特征[95, 2, 16, 42, 90]，以及捕捉全局背景[19, 34, 89, 40, 23, 26, 91]。</strong></p>
<p>在本文中，我们总结了那些为语义分割设计的成功模型的特点，并提出了一个基于CNN的模型，命名为SegNeXt。与我们的论文最相关的工作是[62]，它将k×k卷积分解为一对k×1和1×k卷积。虽然这项工作表明大卷积核在语义分割中很重要，但它忽略了多尺度感受野的重要性，也没有考虑如何利用这些由大卷积核提取的多尺度特征，以注意力的形式进行分割。</p>
<h3 id="22-multi-scale-networks"><a class="markdownIt-Anchor" href="#22-multi-scale-networks"></a> 2.2 Multi-Scale Networks</h3>
<p>设计多尺度网络是计算机视觉中的一个流行方向。对于分割模型，多尺度块出现在编码器[71, 20, 67]和解码器[94, 86, 6]两部分。GoogleNet[67]是与我们的方法最相关的多尺度架构之一，它使用多分支结构来实现多尺度特征提取。另一项与我们方法相关的工作是HRNet[71]。在深层阶段，HRNet也保留了高分辨率的特征，与低分辨率的特征进行聚合，从而实现多尺度的特征提取。</p>
<p>与之前的方法不同，SegNeXt除了在编码器中捕获多尺度特征外，还引入了一个有效的注意力机制，并采用了更便宜和更大的内核卷积。这些使我们的模型能够达到比上述分割方法更高的性能。</p>
<h3 id="23-attention-mechanisms"><a class="markdownIt-Anchor" href="#23-attention-mechanisms"></a> 2.3 Attention Mechanisms</h3>
<p><strong>注意机制是一种自适应的选择过程，其目的是使网络集中于重要的部分。一般来说，它在语义分割中可以分为两类[25]，包括通道注意和空间注意。不同类型的注意起着不同的作用。例如，空间注意主要关心重要的空间区域[17, 14, 57, 51, 22]。不同的是，使用通道注意的目的是使网络有选择地注意那些重要的物体，这在以前的工作中已经被证明是很重要的[30, 9, 72]。说到最近流行的视觉transformer[17, 51, 82, 74, 73, 50, 80, 33, 49, 88]，它们通常忽略了通道维度的适应性。</strong></p>
<p><strong>视觉注意力网络（VAN）[24]是与SegNeXt最相关的工作</strong>，它也提出利用大核注意力（LKA）机制来建立通道和空间注意力。虽然VAN在图像分类中取得了很好的表现，但它在网络设计过程中忽略了多尺度特征聚合的作用，而这对于类似分割的任务是至关重要的。</p>
<h2 id="3-method"><a class="markdownIt-Anchor" href="#3-method"></a> 3 Method</h2>
<p>在本节中，我们将详细描述所提出的SegNeXt的结构。基本上，我们采用了一个编码器-解码器的架构，这与以前的大多数工作一样，简单易行。</p>
<h3 id="31-convolutional-encoder"><a class="markdownIt-Anchor" href="#31-convolutional-encoder"></a> 3.1 Convolutional Encoder</h3>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221128151220385.png" alt="image-20221128151220385" /></p>
<blockquote>
<p>图2 作者对提出的MSCA和MSCAN的说明。这里，d，k1×k2意味着使用内核大小为k1×k2的深度卷积（d）。我们使用卷积提取多尺度特征，然后利用它们作为注意力权重来重新权衡MSCA的输入。</p>
</blockquote>
<p>我们的编码器采用了金字塔结构，这与之前的大多数工作[80, 5, 19]相同。对于我们编码器中的构件，我们采用了与ViT[17, 80]类似的结构，但不同的是，我们没有使用自我注意机制，而是设计了一个新颖的多尺度卷积注意（MSCA）模块。如图2(a)所示，MSCA包含三个部分：一个深度卷积来聚集局部信息，多分支深度卷积来捕捉多尺度背景，以及一个1×1卷积来模拟不同通道之间的关系。1×1卷积的输出被直接用作注意力权重，以重新权衡MSCA的输入。在数学上，我们的MSCA可以写成：</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221128151537212.png" alt="image-20221128151537212" /></p>
<p>其中F代表输入特征。Att和Out分别为注意图和输出。⊗是逐元的矩阵乘法运算。DW-Conv表示深度卷积，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>c</mi><mi>a</mi><mi>l</mi><msub><mi>e</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">Scale_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，i∈{0，1，2，3}，表示图2（b）中的第i个分支。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>c</mi><mi>a</mi><mi>l</mi><msub><mi>e</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">Scale_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是身份连接。按照[62]，在每个分支中，我们使用两个深度带状卷积来近似于大核的标准深度卷积。这里，每个分支的核大小分别被设定为7、11和21。我们选择深度带状卷积的原因有两个方面。一方面，带状卷积是轻量级的。为了模仿核大小为7×7的标准二维卷积，我们只需要一对7×1和1×7的卷积。另一方面，在分割场景中存在一些条状物体，如人和电线杆。因此，条状卷积可以作为网格卷积的补充，有助于提取条状特征[62, 29]。</p>
<p>将一连串的构件堆叠在一起，就得到了提议的卷积编码器，命名为MSCAN。对于MSCAN，我们采用一个普通的分层结构，它包含四个阶段，空间分辨率依次为H 4 ×W 4,H 8 ×W 8,H 16 ×W 16和H 32 ×W 32。这里，H和W分别是输入图像的高度和宽度。如上所述，每个阶段都包含一个下采样块和一个堆叠的构建块。下采样块有一个跨度为2、核大小为3×3的卷积，然后是一个批量归一化层[35]。请注意，在MSCAN的每个构建块中，我们使用批量归一化而不是层归一化，因为我们发现批量归一化对分割性能有更大的好处。</p>
<p>我们设计了四个不同规模的编码器模型，分别命名为MSCAN-T、MSCAN-S、MSCAN-B和MSCAN-L。相应的整体分割模型分别称为SegNeXt-T, SegNeXt-S, SegNeXt-B, SegNeXt-L。详细的网络设置显示在Tab. 2.</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221128151941878.png" alt="image-20221128151941878" /></p>
<blockquote>
<p>表2 提出的SegNeXt的不同规模的详细设置。在这个表中，'e.r.'代表前馈网络中的扩展率。'C’和’L’分别是通道和构建块的数量。'解码器尺寸’表示解码器中的MLP尺寸。'参数’是在ADE20K数据集上计算的[98]。由于不同的数据集中类别的数量不同，参数的数量可能略有变化。</p>
</blockquote>
<h3 id="32-decoder"><a class="markdownIt-Anchor" href="#32-decoder"></a> 3.2 Decoder</h3>
<p>在分割模型[80, 96, 5]中，编码器大多是在ImageNet数据集上预训练的。为了捕捉高层次的语义，通常需要一个解码器，它被应用在编码器上。在这项工作中，我们研究了三种简单的解码器结构，如图3所示。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221128152147431.png" alt="image-20221128152147431" /></p>
<blockquote>
<p>图3 Three different decoder designs.</p>
</blockquote>
<p><strong>第一种，在SegFormer[80]中采用，是一种纯粹的基于MLP的结构。第二种是主要采用基于CNN的模型。在这种结构中，编码器的输出被直接用作重型解码器头的输入，如ASPP[5]、PSP[94]和DANet[19]。最后一种是我们的SegNeXt中采用的结构。我们将后三个阶段的特征聚集起来，并使用一个轻量级的Hamburger[21]来进一步建立全局背景模型。结合我们强大的卷积编码器，我们发现使用一个轻量级的解码器可以提高性能-计算效率。</strong></p>
<p>值得一提的是，与SegFormer不同的是，SegFormer的解码器聚合了第一阶段到第四阶段的特征，而我们的解码器只接收最后三个阶段的特征。这是因为我们的SegNeXt是基于卷积的。第一阶段的特征包含了太多的低层次信息，会影响性能。此外，对第一阶段的操作带来了沉重的计算开销。在实验部分，我们将展示我们的卷积SegNeXt比最近最先进的基于transformer的SegFormer[80]和HRFormer[88]表现得更好。</p>
<h3 id="4-experiments"><a class="markdownIt-Anchor" href="#4-experiments"></a> 4 Experiments</h3>
<h4 id="dataset"><a class="markdownIt-Anchor" href="#dataset"></a> Dataset.</h4>
<p>我们在七个流行的数据集上评估了我们的方法，包括ImageNet-1K [15], ADE20K [98], Cityscapes [13], Pascal VOC [18], Pascal Context [58], COCO-Stuff [3] 和iSAID [76]。ImageNet[15]是最著名的图像分类数据集，它包含1000个类别。与大多数分割方法类似，我们用它来预训练我们的MSCAN编码器。ADE20K[98]是一个具有挑战性的数据集，包含150个语义类别。它由20,210/2,000/3,352张图像组成的训练、验证和测试集。Cityscapes[13]主要关注城市场景，包含5,000张高分辨率图像和19个类别。在训练、验证和测试中分别有2,975/500/1,525张图像。Pascal VOC[18]涉及20个前景类和一个背景类。扩增后，它有10，582/1，449/1，456张图像，分别用于训练、验证和测试。Pascal Context[58]包含59个前景类和一个背景类。训练集和验证集分别包含4,996和5,104张图像。COCO-Stuff[3]也是一个具有挑战性的基准，它包含172个语义类别和总共164k张图像。iSAID[76]是一个大规模的航空图像分割基准，它包括15个前景类和一个背景类。其训练、验证和测试集分别涉及1,411/458/937张图像。</p>
<p>实施细节。我们通过使用Jittor[31]和Pytorch[61]进行实验。我们的实现是基于timm（Apache-2.0）[77]和mmsegmentation（Apache-2.0）[12]库，分别用于分类和分割。我们的分割模型的所有编码器都在ImageNet-1K数据集[15]上进行了预训练。我们分别采用Top-1准确率和平均交集大于联盟（mIoU）作为分类和分割的评价指标。所有的模型都是在一个有8个RTX 3090 GPU的节点上训练的。</p>
<p>对于ImageNet预训练，我们的数据增强方法和训练设置与DeiT[70]相同。对于分割实验，我们采用了一些常见的数据增强方法，包括随机水平翻转、随机缩放（从0.5到2）和随机裁剪。城市景观数据集的批量大小被设置为8，其他所有数据集的批量大小为16。AdamW[54]被用来训练我们的模型。我们将初始学习率设置为0.00006，并采用多学习率衰减策略。我们对ADE20K、Cityscapes和iSAID数据集进行了16万次迭代训练，对COCO-Stuff、Pascal VOC和Pascal Context数据集进行了8万次迭代。在测试过程中，我们同时使用了单尺度（SS）和多尺度（MS）的翻转测试策略来进行公平的比较。更多细节可以在我们的补充材料中找到。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221128155151998.png" alt="image-20221128155151998" /></p>
<h3 id="41-encoder-performance-on-imagenet"><a class="markdownIt-Anchor" href="#41-encoder-performance-on-imagenet"></a> 4.1 Encoder Performance on ImageNet</h3>
<p>ImageNet预训练是训练分割模型的一个常见策略[94, 6, 80, 88, 5]。在这里，我们将我们的MSCAN与最近流行的几个基于CNN和基于transformer的分类模型的性能进行比较。如表3所示，我们的MSCAN比最近最先进的基于CNN的方法ConvNeXt[52]取得了更好的结果，并且超过了流行的基于transformer的方法，如Swin Transformer[51]和SegFormer[80]的编码器MiT。</p>
<h3 id="42-ablation-study"><a class="markdownIt-Anchor" href="#42-ablation-study"></a> 4.2 Ablation study</h3>
<h4 id="ablation-on-msca-design"><a class="markdownIt-Anchor" href="#ablation-on-msca-design"></a> Ablation on MSCA design.</h4>
<p>我们在ImageNet和ADE20K数据集上进行MSCA设计的消融研究。K×K分支包含1×K的深度卷积和K×1的深度卷积。1×1 conv指的是通道混合操作。注意是指元素-明智的乘积，这使得网络获得自适应能力。结果显示在表6。6. 我们可以发现，每个部分都对最终的性能做出了贡献。</p>
<h4 id="global-context-for-decoder"><a class="markdownIt-Anchor" href="#global-context-for-decoder"></a> Global Context for Decoder.</h4>
<p>解码器在为分割模型整合来自多尺度特征的全局背景方面发挥着重要作用。在这里，我们研究了不同的全局背景模块对解码器的影响。正如大多数以前的工作[75，19]所示，基于注意力的解码器对CNN的性能比金字塔结构[94，5]更好，因此我们只展示使用基于注意力的解码器的结果。具体来说，我们展示了4种不同类型的基于注意力的解码器的结果，包括复杂度为O(n2)的非局部（NL）注意力[75]和复杂度为O(n)的CCNet[34]、EMANet[40]和HamNet[21]。如表5所示，Ham在复杂性和性能之间实现了最佳的权衡。因此，我们在解码器中使用Hamburger[21]。</p>
<h4 id="decoder-structure"><a class="markdownIt-Anchor" href="#decoder-structure"></a> Decoder Structure.</h4>
<p>与图像分类不同，分割模型需要高分辨率的输出。我们消减了三种不同的解码器设计用于分割，所有这些设计都已在图3中显示。相应的结果列于表7。我们可以看到SegNeXt©取得了最好的性能，计算成本也很低。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221128155415972.png" alt="image-20221128155415972" /></p>
<h4 id="importance-of-our-msca"><a class="markdownIt-Anchor" href="#importance-of-our-msca"></a> Importance of Our MSCA.</h4>
<p>在这里，我们进行实验来证明MSCA对于分割的重要性。作为比较，我们跟随VAN[24]，用一个大核的单一卷积来代替我们的MSCA中的多个分支。如表8和表9所示。8和Tab.3，我们可以观察到，虽然两个编码器在ImageNet分类中的性能接近，但SegNeXt w/MSCA产生的结果比设置w/o MSCA要好得多。这表明多尺度特征的聚合对编码器的语义分割至关重要。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221128155616645.png" alt="image-20221128155616645" /></p>
<h3 id="43-comparison-with-state-of-the-art-methods"><a class="markdownIt-Anchor" href="#43-comparison-with-state-of-the-art-methods"></a> 4.3 Comparison with state-of-the-art methods</h3>
<p>在本小节中，我们将我们的方法与最先进的基于CNN的方法，如HRNet[71]、ResNeSt[92]和EfficientNet[69]，以及基于transformer的方法，如Swin Transformer[51]、SegFormer[80]、HRFormer[88]、MaskFormer[11]和Mask2Former[10]进行比较。</p>
<h4 id="performance-computation-trade-off"><a class="markdownIt-Anchor" href="#performance-computation-trade-off"></a> Performance-computation trade-off.</h4>
<p>ADE20K和Cityscapes是语义分割中两个广泛使用的基准。如图1所示，我们绘制了不同方法在Cityscape和ADE20K验证集上的性能-计算曲线。显然，与其他最先进的方法，如SegFormer[80]、HRFormer[88]和MaskFormer[11]相比，我们的方法实现了性能和计算量之间的最佳平衡。</p>
<h4 id="comparison-with-state-of-the-art-transformers"><a class="markdownIt-Anchor" href="#comparison-with-state-of-the-art-transformers"></a> Comparison with state-of-the-art transformers.</h4>
<p>我们在ADE20K、Cityscapes、COCO-Stuff和Pascal Context基准上将SegNeXt与最先进的transformer模型进行比较。如表9所示，SegNeXt-L超过了Mask2Former。在 ADE20K 数据集上，SegNeXt-L 在参数和计算成本相似的情况下，以 3.3 mIoU (51.0 v. 47.7) 的优势超越了以 Swin-T 为骨干的 Mask2Former。此外，与SegFormer-B2相比，SegNeXt-B在ADE20K数据集上只用了56%的计算量就获得了2.0 mIoU的改善（48.5 v. 46.5）。特别是，由于SegFormer[80]中的自我关注是二次复杂的，而我们的方法使用卷积，这使得我们的方法在处理Cityscapes数据集的高分辨率图像时表现得非常好。例如，SegNeXt-B比SegFormer-B2增加了1.6 mIoU（81.0 v. 82.6），但使用的计算量减少了40%。在图4中，我们还展示了与SegFormer的定性比较。我们可以看到，由于提出了MSCA，我们的方法在处理物体细节时识别得很好。</p>
<h4 id="comparison-with-state-of-the-art-cnns"><a class="markdownIt-Anchor" href="#comparison-with-state-of-the-art-cnns"></a> Comparison with state-of-the-art CNNs.</h4>
<p>如表4所示。4, Tab.10和Tab.12，我们在Pascal VOC 2012、Pascal Context和iSAID数据集上将我们的SegNeXt与最先进的CNN如ResNeSt-269[92]、EfficientNet-L2[99]和HRNetW48[71]进行比较。SegNeXt-L在使用更少的参数和计算的情况下超过了流行的HRNet（OCR）[71, 87]模型（60.3 v. 56.3），它是为分割任务精心设计的。此外，SegNeXt-L在Pascal VOC 2012测试排行榜上的表现甚至优于EfficientNet-L2（NAS-FPN），后者在额外的3亿张不可用图像上进行了预训练。值得注意的是，EfficientNet-L2（NAS-FPN）有485M的参数，而SegNeXt-L只有48.7M的参数。</p>
<h4 id="comparison-with-real-time-methods"><a class="markdownIt-Anchor" href="#comparison-with-real-time-methods"></a> Comparison with real-time methods.</h4>
<p>除了最先进的性能外，我们的方法也适合于实时部署。即使没有任何特定的软件或硬件加速，SegNeXt-T在处理768×1,536尺寸的图像时，使用单个3090 RTX GPU实现了每秒25帧（FPS）。如表11所示，我们的方法创造了最先进的技术。如表11所示，我们的方法在Cityscapes测试集的实时分割方面创造了新的最先进的结果。</p>
<h3 id="5-conclusions-and-discussion"><a class="markdownIt-Anchor" href="#5-conclusions-and-discussion"></a> 5 Conclusions and Discussion</h3>
<p>在本文中，我们分析了以前成功的分割模型，并找到它们所拥有的良好特性。基于这些发现，我们提出了一个量身定做的卷积注意模块MSCA和一个CNN式网络SegNeXt。实验结果表明，SegNeXt在相当大的程度上超过了目前最先进的基于transformer的方法。</p>
<p>最近，基于transformer的模型在各种分割排行榜上占主导地位。相反，本文表明，当使用适当的设计时，基于CNN的方法仍然可以比基于transformer的方法表现得更好。我们希望本文能够鼓励研究人员进一步研究CNN的潜力。</p>
<p>我们的模型也有其局限性，例如，将此方法扩展到具有100M以上参数的大规模模型以及在其他视觉或NLP任务上的表现。这些将在我们未来的工作中得到解决。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/SegFormer/" title="SegFormer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">SegFormer</div></div><div class="info-2"><div class="info-item-1"> SegFormer  文章链接   摘要 我们提出了SegFormer，这是一个简单、高效而强大的语义分割框架，它将Transformer与轻量级多层感知器（MLP）解码器统一起来。SegFormer有两个吸引人的特点。1）SegFormer包括一个新颖的分层结构的Transformer编码器，输出多尺度特征。它不需要位置编码，从而避免了位置编码的插值，当测试分辨率与训练不同时，插值会导致性能下降。2)...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/SAN/" title="SAN"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">SAN</div></div><div class="info-2"><div class="info-item-1"> SAN  Abstract 本文提出了一种利用预先训练好的视觉语言模型（名为 “侧适配器网络”（SAN））进行开放词汇语义分割的新框架。我们的方法将语义分割任务建模为区域识别问题。一个侧边网络连接到一个冻结的 CLIP 模型，该模型有两个分支：一个用于预测mask建议，另一个用于预测注意力偏差，该注意力偏差应用于 CLIP 模型，以识别mask类别。这种解耦设计有利于 CLIP 识别mask建议类别。由于附加侧网络可以重复使用 CLIP 特征，因此可以非常轻便。此外，整个网络可以进行端到端训练，使侧网络适应冻结的 CLIP 模型，从而使预测的mask建议具有 CLIP 感知。我们的方法快速、准确，而且只增加了几个额外的可训练参数。我们在多个语义分割基准上评估了我们的方法。我们的方法明显优于其他同类方法，可训练参数减少了 18 倍，推理速度提高了 19 倍。图 1 显示了 ImageNet 上的一些可视化结果。我们希望我们的方法能成为一个坚实的基线，并为未来的开放词汇语义分割研究提供帮助。              图 1.ImageNet...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BAM</div></div><div class="info-2"><div class="info-item-1"> BAM  Abstract 近来，小样本分割分割技术（FSS）得到了广泛的发展。然而，训练出来的模型偏向于所看到的类别，而不是理想的类别无关性，从而阻碍了对新内容的识别。本文提出了一种新颖而直接的见解来缓解这一问题。具体来说，我们在传统的 FSS 模型（元学习器）上增加了一个分支（基础学习器），以明确识别基础类别的目标，即不需要分割的区域。然后，对这两个学习器并行输出的粗略结果进行自适应整合，从而得出精确的分割预测结果。考虑到元学习器的敏感性，我们进一步引入了一个调整因子来估计输入图像对之间的场景差异，以促进模型的集合预测。在 PASCAL-5i 和 COCO-20i 上取得的显著性能提升验证了这一方法的有效性，而且令人惊讶的是，即使使用两个普通学习器，我们的多功能方案也创造了新的一流水平。此外，鉴于所提方法的独特性，我们还将其扩展到了更现实但更具挑战性的环境中，即广义 FSS，在这种环境中，基础类和新类别的像素都需要确定。  1. Introduction   得益于成熟的大规模数据集 [8, 9,...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#segnext"><span class="toc-text"> SegNeXt</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-introduction"><span class="toc-text"> 1 Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-related-work"><span class="toc-text"> 2 Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-semantic-segmentation"><span class="toc-text"> 2.1 Semantic Segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-multi-scale-networks"><span class="toc-text"> 2.2 Multi-Scale Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#23-attention-mechanisms"><span class="toc-text"> 2.3 Attention Mechanisms</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-method"><span class="toc-text"> 3 Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-convolutional-encoder"><span class="toc-text"> 3.1 Convolutional Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-decoder"><span class="toc-text"> 3.2 Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-experiments"><span class="toc-text"> 4 Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#dataset"><span class="toc-text"> Dataset.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#41-encoder-performance-on-imagenet"><span class="toc-text"> 4.1 Encoder Performance on ImageNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-ablation-study"><span class="toc-text"> 4.2 Ablation study</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ablation-on-msca-design"><span class="toc-text"> Ablation on MSCA design.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#global-context-for-decoder"><span class="toc-text"> Global Context for Decoder.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#decoder-structure"><span class="toc-text"> Decoder Structure.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#importance-of-our-msca"><span class="toc-text"> Importance of Our MSCA.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-comparison-with-state-of-the-art-methods"><span class="toc-text"> 4.3 Comparison with state-of-the-art methods</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#performance-computation-trade-off"><span class="toc-text"> Performance-computation trade-off.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#comparison-with-state-of-the-art-transformers"><span class="toc-text"> Comparison with state-of-the-art transformers.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#comparison-with-state-of-the-art-cnns"><span class="toc-text"> Comparison with state-of-the-art CNNs.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#comparison-with-real-time-methods"><span class="toc-text"> Comparison with real-time methods.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-conclusions-and-discussion"><span class="toc-text"> 5 Conclusions and Discussion</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/11/%E5%B0%8F%E7%B1%B3/syzkaller/syzkaller01-%E9%85%8D%E7%BD%AE/" title="syzkaller01-配置">syzkaller01-配置</a><time datetime="2024-12-10T17:10:45.000Z" title="发表于 2024-12-11 01:10:45">2024-12-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM">BAM</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>