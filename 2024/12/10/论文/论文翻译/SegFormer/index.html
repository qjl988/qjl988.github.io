<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>SegFormer | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="SegFormer  文章链接   摘要 我们提出了SegFormer，这是一个简单、高效而强大的语义分割框架，它将Transformer与轻量级多层感知器（MLP）解码器统一起来。SegFormer有两个吸引人的特点。1）SegFormer包括一个新颖的分层结构的Transformer编码器，输出多尺度特征。它不需要位置编码，从而避免了位置编码的插值，当测试分辨率与训练不同时，插值会导致性能下">
<meta property="og:type" content="article">
<meta property="og:title" content="SegFormer">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/SegFormer/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="SegFormer  文章链接   摘要 我们提出了SegFormer，这是一个简单、高效而强大的语义分割框架，它将Transformer与轻量级多层感知器（MLP）解码器统一起来。SegFormer有两个吸引人的特点。1）SegFormer包括一个新颖的分层结构的Transformer编码器，输出多尺度特征。它不需要位置编码，从而避免了位置编码的插值，当测试分辨率与训练不同时，插值会导致性能下">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:03:47.787Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/SegFormer/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'SegFormer',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">SegFormer</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">SegFormer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:03:47.787Z" title="更新于 2024-12-11 01:03:47">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">7.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>25分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="segformer"><a class="markdownIt-Anchor" href="#segformer"></a> SegFormer</h1>
<blockquote>
<p><a href="zotero://open-pdf/library/items/VVUV6RPD">文章链接</a></p>
</blockquote>
<h2 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h2>
<p>我们提出了SegFormer，这是一个简单、高效而强大的语义分割框架，它将Transformer与轻量级多层感知器（MLP）解码器统一起来。SegFormer有两个吸引人的特点。1）SegFormer包括一个新颖的分层结构的Transformer编码器，输出多尺度特征。它不需要位置编码，从而避免了位置编码的插值，当测试分辨率与训练不同时，插值会导致性能下降。2) SegFormer避免了复杂的解码器。所提出的MLP解码器聚合了来自不同层的信息，从而结合了局部注意和全局注意，呈现出强大的表征。我们表明，这种简单和轻量级的设计是在Transformer上进行高效分割的关键。我们扩大了我们的方法，得到了从SegFormer-B0到SegFormer-B5的一系列模型，达到了比以前的同类模型明显更好的性能和效率。例如，SegFormer-B4在ADE20K上实现了50.3%的mIoU，参数为64M，比之前的最佳方法小5倍，好2.2%。我们最好的模型，SegFormer-B5，在Cityscapes验证集上实现了84.0%的mIoU，并在Cityscapes-C上显示了出色的零点稳定性。</p>
<h2 id="1-介绍"><a class="markdownIt-Anchor" href="#1-介绍"></a> 1 介绍</h2>
<p>语义分割是计算机视觉中的一项基本任务，可以实现许多下游应用。它与图像分类有关，因为它产生了每个像素的类别预测，而不是图像级别的预测。这一关系在一项开创性的工作中被指出并进行了系统的研究[1]，作者将全卷积网络（FCN）用于语义分割的任务。从那时起，FCN激发了许多后续工作，并成为密集预测的一个主要设计选择。</p>
<p>由于分类和语义分割之间有很强的关系，许多先进的语义分割框架是ImageNet上流行的图像分类架构的变体。因此，设计骨干架构仍然是语义分割的一个活跃领域。事实上，从使用VGG的早期方法[1, 2]开始，到具有明显更深、更强大的骨干的最新方法[3]，骨干的演变极大地推动了语义分割的性能边界。除了骨干架构，另一条工作路线是将语义分割作为一个结构化的预测问题，并专注于设计模块和运算符，这些模块和运算符可以有效地捕捉上下文信息。这个领域的一个代表性例子是扩张卷积[4，5]，它通过用孔 &quot;膨胀 &quot;内核来增加感受野。</p>
<p>见证了自然语言处理（NLP）的巨大成功，最近出现了将Transformer引入视觉任务的兴趣高潮。Dosovitskiy等人[6]提出了用于图像分类的视觉变换器（ViT）。遵循NLP中的Transformer设计，作者将一幅图像分割成多个线性嵌入的patches，并将它们送入一个带有位置嵌入（PE）的标准Transformer中，从而在ImageNet上取得了令人印象深刻的表现。在语义分割方面，Zheng等人[7]提出了SETR来证明在这个任务中使用Transformer的可行性。</p>
<p>SETR采用ViT作为主干，并结合了几个CNN解码器来扩大特征分辨率。尽管性能良好，但ViT也有一些限制。1）ViT输出单尺度的低分辨率特征，而不是多尺度的。2）它在大图像上有很高的计算成本。为了解决这些局限性，Wang等人[8]提出了金字塔视觉变换器（PVT），这是ViT的自然扩展，采用金字塔结构进行密集预测。PVT在物体检测和语义分割上比ResNet对应的方法有相当大的改进。然而，连同其他新兴的方法，如Swin Transformer[9]和Twins[10]，这些方法主要考虑Transformer编码器的设计，忽略了解码器对进一步改进的贡献。</p>
<p>本文介绍了SegFormer，这是一个用于语义分割的前沿Transformer框架，共同考虑了效率、准确性和鲁棒性。与以前的方法相比，我们的框架同时重新设计了编码器和解码器。我们方法的主要创新之处在于：</p>
<ul>
<li>
<p>一种新型的无位置编码和分层的Transformer编码器。</p>
</li>
<li>
<p>一个轻量级的全MLP解码器设计，在没有复杂和计算要求高的模块的情况下，产生了强大的表示。</p>
</li>
<li>
<p>如图1所示，SegFormer在三个公开的语义分割数据集中，在效率、准确性和稳健性方面都达到了新的水平。</p>
</li>
</ul>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20221127131039434.png" alt="image-20221127131039434" /></p>
<blockquote>
<p>图1: ADE20K上的性能与模型效率。所有的结果都是在单一模型和单一规模推理下报告的。SegFormer实现了新的最先进的51.0%的mIoU，同时比以前的方法明显更有效率。</p>
</blockquote>
<p>首先，提议的编码器在对分辨率与训练分辨率不同的图像进行推理时，避免了内插位置编码。因此，我们的编码器可以很容易地适应任意的测试分辨率而不影响其性能。此外，分层部分使编码器能够生成高分辨率的精细特征和低分辨率的粗略特征，这与ViT相反，后者只能生成固定分辨率的单一低分辨率特征图。其次，我们提出了一个轻量级的MLP解码器，其关键思想是利用Transformer引起的特征，即低层的注意力倾向于留在本地，而最高层的注意力是高度非本地的。通过聚合来自不同层的信息，MLP解码器结合了局部和全局的注意力。因此，我们得到了一个简单明了的解码器，它能演绎出强大的表征。</p>
<p>我们在三个公开可用的数据集上证明了SegFormer在模型大小、运行时间和准确性方面的优势。ADE20K、Cityscapes和COCO-Stuff。在Citysapces上，我们的轻量级模型SegFormer-B0，在没有TensorRT等加速实现的情况下，在48FPS下产生了71.9%的mIoU，**与ICNet[11]相比，在延迟和性能上分别有60%和4.2%的相对改善。**我们最大的模型，SegFormer-B5，产生了84.0%的mIoU，这代表了相对1.8%的mIoU改进，同时比SETR[7]快5倍。在ADE20K上，这个模型创造了一个新的最先进的51.8% mIoU，同时比SETR小4倍。此外，我们的方法对常见的损坏和扰动的鲁棒性明显高于现有的方法，因此适用于安全关键的应用。代码将被公开提供。</p>
<h2 id="2-相关工作"><a class="markdownIt-Anchor" href="#2-相关工作"></a> 2 相关工作</h2>
<h4 id="语义分割"><a class="markdownIt-Anchor" href="#语义分割"></a> 语义分割</h4>
<p>语义分割可以被看作是图像分类从图像层面到像素层面的延伸。在深度学习时代[12-16]，FCN[1]是语义分割的基础工作，它是一个完全卷积网络，以端到端的方式进行像素到像素的分类。之后，研究者们专注于从不同方面改进FCN，如：扩大感受野[17-19, 5, 2, 4, 20]；完善上下文信息[21-29]；引入边界信息[30-37]；设计各种注意模块[38-46]；或使用AutoML技术[47-51]。这些方法大大改善了语义分割的性能，但代价是引入了许多经验模块，使产生的框架在计算上要求很高，而且很复杂。最近的方法已经证明了基于Transformer的架构对于语义分割的有效性[7, 46]。然而，这些方法在计算上仍然要求很高。</p>
<h4 id="transformer-backbones"><a class="markdownIt-Anchor" href="#transformer-backbones"></a> Transformer backbones.</h4>
<p>ViT[6]是第一个证明纯Transformer可以在图像分类中取得最先进性能的工作。ViT将每张图像视为一个标记序列，然后将其送入多个Transformer层来进行分类。随后，DeiT[52]进一步探索了一种数据高效的训练策略和ViT的提炼方法。最近的方法如T2T ViT[53]、CPVT[54]、TNT[55]、CrossViT[56]和LocalViT[57]对ViT引入了量身定做的改变，以进一步提高图像分类性能。</p>
<p>在分类之外，PVT[8]是第一个在Transformer中引入金字塔结构的工作，证明了纯Transformer骨干与CNN对应的密集预测任务相比的潜力。之后，Swin[9]、CvT[58]、CoaT[59]、LeViT[60]和Twins[10]等方法增强了特征的局部连续性，去除固定大小的位置嵌入，提高了Transformer在密集预测任务中的性能。</p>
<h4 id="transformers-for-specific-tasks"><a class="markdownIt-Anchor" href="#transformers-for-specific-tasks"></a> Transformers for specific tasks.</h4>
<p>DETR[52]是第一个使用Transformer构建无非最大抑制（NMS）的端到端物体检测框架的工作。其他工作也在各种任务中使用Transformer，如跟踪[61, 62]、超分辨率[63]、ReID[64]、着色[65]、检索[66]和多模式学习[67, 68]。对于语义分割，SETR[7]采用ViT[6]作为骨干来提取特征，取得了令人印象深刻的性能。然而，这些基于Transformer的方法效率很低，因此，很难部署在实时应用中。</p>
<h2 id="3-method"><a class="markdownIt-Anchor" href="#3-method"></a> 3 Method</h2>
<p>本节介绍了SegFormer，我们高效、稳健、强大的分割框架，没有手工制作和计算要求高的模块。如图2所示，SegFormer由两个主要模块组成：</p>
<p>（1）一个分层的Transformer编码器，生成高分辨率的粗特征和低分辨率的细特征；</p>
<p>（2）一个轻量级的All-MLP解码器，融合这些多层次的特征，产生最终的语义分割掩码。</p>
<p>与使用16×16大小的patch的ViT相反，使用较小的patch有利于密集的预测任务。然后将这些patches作为分层Transformer编码器的输入，以获得原始图像分辨率的{1/4, 1/8, 1/16, 1/32}的多层次特征。然后，我们将这些多级特征传递给All-MLP解码器，以预测<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>H</mi><mn>4</mn></mfrac><mo>×</mo><mfrac><mi>W</mi><mn>4</mn></mfrac><mo>×</mo><msub><mi>N</mi><mrow><mi>c</mi><mi>l</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\frac{H}{4}×\frac{W}{4}×N_{cls}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>分辨率下的分割掩码，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mrow><mi>c</mi><mi>l</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">N_{cls}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是类别的数量。在本节的其余部分，我们详细介绍了提出的编码器和解码器设计，并总结了我们的方法和SETR之间的主要区别。</p>
<p><img src="/img/loading.gif" data-original="images/image-20221127195816201.png" alt="image-20221127195816201" /></p>
<blockquote>
<p>图2：拟议的SegFormer框架由两个主要模块组成。一个分层的Transformer编码器来提取粗略和精细的特征；一个轻量级的All-MLP解码器来直接融合这些多层次的特征并预测语义分割掩码。&quot;FFN &quot;表示前馈网络。</p>
</blockquote>
<h3 id="31-hierarchical-transformer-encoder"><a class="markdownIt-Anchor" href="#31-hierarchical-transformer-encoder"></a> 3.1 Hierarchical Transformer Encoder</h3>
<p>我们设计了一系列混合Transformer编码器（MiT），MiT-B0到MiT-B5，结构相同但尺寸不同。MiT-B0是我们的轻型模型，用于快速推理，而MiT-B5是最大的模型，用于最佳性能。我们对MiT的设计部分受到ViT的启发，但为语义分割进行了定制和优化。</p>
<h4 id="层次化特征表示"><a class="markdownIt-Anchor" href="#层次化特征表示"></a> 层次化特征表示。</h4>
<p>不像ViT只能生成单一分辨率的特征图，这个模块的目标是，给定一个输入图像，生成类似CNN的多层次特征。这些特征提供了高分辨率的粗略特征和低分辨率的细粒度特征，通常能提高语义分割的性能。更确切地说，给定一个分辨率为H×W×3的输入图像，我们进行patch合并，得到一个分辨率为H 2i+1×W 2i+1×Ci的层次特征图Fi，其中i∈{1，2，3，4}，Ci+1比Ci大。</p>
<h4 id="overlapped-patch-merging"><a class="markdownIt-Anchor" href="#overlapped-patch-merging"></a> Overlapped Patch Merging.</h4>
<p>给定一个图像patch，ViT中使用的patch合并过程将一个N×N×3的patch统一为一个1×1×C的向量。这可以很容易地扩展到将一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn><mo>×</mo><msub><mi>C</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">2×2×C_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的特征路径统一为一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><mo>×</mo><msub><mi>C</mi><mi>i</mi></msub><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1×1×C_i+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>的向量，以获得分层特征图。利用这一点，我们可以将我们的分层特征从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mfrac><mi>H</mi><mn>4</mn></mfrac><mo>×</mo><mfrac><mi>W</mi><mn>4</mn></mfrac><mo>×</mo><msub><mi>C</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F_1(\frac{H}{4}\times \frac{W}{4}\times C_1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>缩减到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mfrac><mi>H</mi><mn>8</mn></mfrac><mo>×</mo><mfrac><mi>W</mi><mn>8</mn></mfrac><mo>×</mo><msub><mi>C</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F_2(\frac{H}{8}\times \frac{W}{8}\times C_2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，然后对分层的任何其他特征图进行迭代。这个过程最初被设计为结合非重叠的图像或特征patches。因此，它未能保留这些patches周围的局部连续性。相反，我们使用了一个重叠的patch合并过程。为此，我们定义了K、S和P，其中K是patch大小，S是两个相邻patch之间的间距，P是填充大小。在我们的实验中，我们设定K=7，S=4，P=3，以及K=3，S=2，P=1来进行重叠patch合并，产生与非重叠过程相同大小的特征。</p>
<h4 id="efficient-self-attention"><a class="markdownIt-Anchor" href="#efficient-self-attention"></a> Efficient Self-Attention.</h4>
<p>编码器的主要计算瓶颈是自注意层。在原来的多头自注意力过程中，每个头Q、K、V都有相同的尺寸N×C，其中N=H×W是序列的长度，自注意力估计为。</p>
<p><img src="/img/loading.gif" data-original="images/image-20221127131504191.png" alt="image-20221127131504191" /></p>
<p>这个过程的计算复杂度是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，这对大图像分辨率来说是令人望而却步的。相反，我们使用[8]中介绍的序列减少过程。这个过程使用一个减少率R来减少序列的长度，如下所示。</p>
<p><img src="/img/loading.gif" data-original="images/image-20221127131512402.png" alt="image-20221127131512402" /></p>
<p>其中，K是要减少的序列，Reshape(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>N</mi><mi>R</mi></mfrac><mo separator="true">,</mo><mi>C</mi><mo separator="true">⋅</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">\frac{N}{R} , C · R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span>)(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>)是指将K重塑为具有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>N</mi><mi>R</mi></mfrac><mo>×</mo><mi>C</mi><mo separator="true">⋅</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">\frac{N}{R}\times C · R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span>形状的序列，Linear<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mo separator="true">⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C_{in}, C_{out})(·)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">(</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mclose">)</span></span></span></span>是指一个线性层将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>维的张量作为输入，生成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>维的张量作为输出。因此，新的K具有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>N</mi><mi>R</mi></mfrac><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">\frac{N}{R}\times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>的维度。因此，自我注意机制的复杂性从O（N^2）减少到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mfrac><msup><mi>N</mi><mn>2</mn></msup><mi>R</mi></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(\frac{N^2}{R})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.36292em;vertical-align:-0.345em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01792em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913142857142857em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>。在我们的实验中，从阶段1到阶段4，我们将R设置为[64, 16, 4, 1]。</p>
<h4 id="mix-ffn"><a class="markdownIt-Anchor" href="#mix-ffn"></a> Mix-FFN.</h4>
<p>ViT使用位置编码（PE）来引入位置信息。然而，PE的分辨率是固定的。因此，当测试分辨率与训练分辨率不同时，需要对位置编码进行内插，这往往导致精度下降。为了缓解这个问题，CPVT[54]使用3×3 Conv与PE一起实现了一个数据驱动的PE。我们认为，位置编码对于语义分割实际上是没有必要的。相反，我们引入了Mix-FFN，它考虑了零填充对泄露位置信息的影响[69]，直接在前馈网络（FFN）中使用3×3 Conv。Mix-FFN可以被表述为。<img src="/img/loading.gif" data-original="images/image-20221127131547577.png" alt="image-20221127131547577" /></p>
<p>其中Xin是来自自我注意模块的特征。Mix-FFN将一个3×3的卷积和一个MLP混合到每个FFN中。在我们的实验中，我们将表明3×3卷积足以为Transformers提供位置信息。特别是，我们使用深度卷积来减少参数的数量并提高效率。</p>
<h3 id="32-lightweight-all-mlp-decoder"><a class="markdownIt-Anchor" href="#32-lightweight-all-mlp-decoder"></a> 3.2 Lightweight All-MLP Decoder</h3>
<p>SegFormer包含了一个仅由MLP层组成的轻量级解码器，这就避免了其他方法中通常使用的手工制作和计算要求的组件。实现这样一个简单的解码器的关键是，我们的分层Transformer编码器比传统的CNN编码器具有更大的有效接收域（ERF）。</p>
<p>拟议的All-MLP解码器由四个主要步骤组成。首先，来自MiT编码器的多层次特征Fi经过MLP层来统一通道维度。然后，在第二步中，特征被上采样为1/4并串联在一起。第三，采用一个MLP层来融合串联的特征F。最后，另一个MLP层采用融合的特征来预测具有H 4×W 4×Ncls分辨率的分割掩码M，其中Ncls是分类的数量。这使我们可以将解码器表述为：</p>
<p><img src="/img/loading.gif" data-original="images/image-20221127131624816.png" alt="image-20221127131624816" /></p>
<p>其中，M指的是预测的掩码，Linear(Cin, Cout)(-)指的是线性层，Cin和Cout分别是输入和输出矢量尺寸。</p>
<h4 id="effective-receptive-field-analysis"><a class="markdownIt-Anchor" href="#effective-receptive-field-analysis"></a> Effective Receptive Field Analysis.</h4>
<p>**对于语义分割来说，保持大的感受野以包括背景信息一直是一个核心问题[5, 19, 20]。**在这里，我们使用有效感受野（ERF）[70]作为工具箱来可视化和解释为什么我们的MLP解码器设计对Transformer如此有效。在图3中，我们将DeepLabv3+和SegFormer的四个编码器阶段和解码器头的ERF可视化。我们可以做出以下观察。</p>
<ul>
<li>DeepLabv3+的ERF即使在阶段4，即最深的阶段也是相对较小的。</li>
<li>SegFormer的编码器在较低的阶段自然产生类似于卷积的局部注意，同时能够输出高度非局部的注意，有效地捕捉阶段4的上下文。</li>
<li>如图3中放大的patches所示，MLP头的ERF（蓝框）与Stage-4（红框）不同，除了非局部注意外，局部注意也明显加强。</li>
</ul>
<p><img src="/img/loading.gif" data-original="images/image-20221127203645702.png" alt="image-20221127203645702" /></p>
<blockquote>
<p>图3：城市景观的有效接收场（ERF）（100张图片的平均值）。最上面一行。Deeplabv3+。底排：Deeplabv3+。SegFormer。两个架构的四个阶段和解码器头的ERF都是可视化的。放大后观看效果最佳。</p>
</blockquote>
<p><strong>CNN中有限的接受域要求人们求助于上下文模块，如ASPP[18]，这些模块扩大了接受域，但不可避免地变得沉重。我们的解码器设计得益于Transformers中的非局部注意，导致了更大的接收区域而不复杂。然而，同样的解码器设计在CNN主干上并不能很好地工作，因为整体的感受野被阶段-4的有限感受野所限制，我们将在后面的表1d中验证这一点。</strong></p>
<p>更重要的是，我们的解码器设计基本上利用了Transformer诱导的特征，即同时产生高度局部和非局部的注意。通过统一它们，我们的MLP解码器通过增加少量的参数来渲染互补的和强大的表征。这是激励我们设计的另一个关键原因。正如表1d所验证的那样，仅仅从阶段4中获取非局部注意力并不足以产生好的结果。</p>
<h3 id="33-relationship-to-setr"><a class="markdownIt-Anchor" href="#33-relationship-to-setr"></a> 3.3 Relationship to SETR.</h3>
<p>与SETR[7]相比，SegFormer包含多个更高效和强大的设计。</p>
<ul>
<li>我们只使用ImageNet-1K进行预训练。SETR中的ViT是在较大的ImageNet-22K上预训练的。</li>
<li>SegFormer的编码器有一个分层结构，它比ViT小，可以同时捕捉高分辨率的粗略特征和低分辨率的精细特征。相比之下，SETR的ViT编码器只能生成单一的低分辨率特征图。</li>
<li>我们在编码器中去掉了位置嵌入，而SETR使用固定形状的位置嵌入，当推理时的分辨率与训练时的不同时，会降低准确性。</li>
<li>我们的MLP解码器比SETR中的解码器更紧凑，计算要求更低。这导致了可忽略不计的计算开销。相比之下，SETR需要有多个3×3卷积的重型解码器。</li>
</ul>
<h2 id="4-experiments"><a class="markdownIt-Anchor" href="#4-experiments"></a> 4 Experiments</h2>
<h3 id="41-experimental-settings"><a class="markdownIt-Anchor" href="#41-experimental-settings"></a> 4.1 Experimental Settings</h3>
<h4 id="datasets"><a class="markdownIt-Anchor" href="#datasets"></a> Datasets:</h4>
<p>我们使用了三个公开可用的数据集。Cityscapes [71], ADE20K [72] 和COCOStuff [73]。ADE20K是一个涵盖150个细粒度语义概念的场景解析数据集，由20210张图像组成。Cityscapes是一个用于语义分割的驱动数据集，包括5000张精细注释的高分辨率图像，有19个类别。COCO-Stuff涵盖172个标签，由16400张图像组成。118k用于训练，5k用于验证，20k用于测试开发，20k用于测试挑战。</p>
<h4 id="implementation-details"><a class="markdownIt-Anchor" href="#implementation-details"></a> Implementation details:</h4>
<p>我们使用mmsegmentation1代码库，在有8个Tesla V100的服务器上进行训练。我们在Imagenet-1K数据集上对编码器进行了预训练，并随机初始化了解码器。在训练过程中，我们通过比例为0.5-2.0的随机调整大小、随机水平翻转和随机裁剪，将ADE20K、Cityscapes和COCO-Stuff的数据分别增加到512×512、1024×1024和512×512。按照[9]，我们将ADE20K上最大的模型B5的裁剪尺寸设置为640×640。我们使用AdamW优化器在ADE20K和Cityscapes上对模型进行了16万次迭代，在COCO-Stuff上进行了8万次迭代。特殊情况下，在消融研究中，我们对模型进行了40K的迭代训练。我们对ADE20K和COCO-Stuff使用了16个批次，而对Cityscapes使用了8个批次。学习率被设置为0.00006的初始值，然后使用默认的系数为1.0的 &quot;聚 &quot;LR计划。为了简单起见，我们没有采用广泛使用的技巧，如OHEM、辅助损失或类平衡损失。在评估过程中，我们将图像的短边重新调整为训练剪裁尺寸，并保持ADE20K和COCO-Stuff的长宽比。对于Cityscapes，我们使用滑动窗口测试，通过裁剪1024×1024的窗口进行推理。我们使用平均相交于联合（mIoU）来报告语义分割的性能。</p>
<h2 id="42-ablation-studies"><a class="markdownIt-Anchor" href="#42-ablation-studies"></a> 4.2 Ablation Studies</h2>
<h4 id="influence-of-the-size-of-model"><a class="markdownIt-Anchor" href="#influence-of-the-size-of-model"></a> Influence of the size of model.</h4>
<p>我们首先分析了增加编码器的尺寸对性能和模型效率的影响。图1显示了ADE20K的性能与模型效率作为编码器尺寸的函数，并且，表1a总结了三个数据集的结果。这里首先要观察的是与编码器相比，解码器的大小。如图所示，对于轻型模型，解码器只有0.4M的参数。对于MiT-B5编码器，解码器只占模型中总参数数的4%。就性能而言，我们可以观察到，总的来说，增加编码器的大小在所有的数据集上都会产生一致的改进。我们的轻量级模型SegFormer-B0在保持有竞争力的性能的同时，也是紧凑和高效的，这表明我们的方法对于实时应用是非常方便的。另一方面，我们的SegFormer-B5，最大的模型，在所有三个数据集上都取得了最先进的结果，显示了我们Transformer编码器的潜力。</p>
<p><img src="/img/loading.gif" data-original="images/image-20221127204031661.png" alt="image-20221127204031661" /></p>
<h4 id="influence-of-c-the-mlp-decoder-channel-dimension"><a class="markdownIt-Anchor" href="#influence-of-c-the-mlp-decoder-channel-dimension"></a> Influence of C, the MLP decoder channel dimension.</h4>
<p>我们现在分析MLP解码器中通道维度C的影响，见第3.2节。在表1b中，我们显示了性能、flops和参数与该维度的关系。我们可以看到，设置C=256提供了一个非常有竞争力的性能和计算成本。性能随着C的增加而增加；然而，它导致了更大和更低效的模型。有趣的是，这种性能在通道尺寸大于768的情况下趋于平稳。鉴于这些结果，我们为我们的实时模型SegFormer-B0、B1选择了C=256，其余的选择C=768。</p>
<h4 id="mix-ffn-vs-positional-encoder-pe"><a class="markdownIt-Anchor" href="#mix-ffn-vs-positional-encoder-pe"></a> Mix-FFN vs. Positional Encoder (PE).</h4>
<p>在这个实验中，我们分析了去除Transformer编码器中的位置编码，而使用提议的Mix-FFN的效果。为此，我们用位置编码（PE）和提议的混合FFN训练Transformer编码器，并对两种不同图像分辨率的城市景观进行推理。768×768使用滑动窗口，1024×2048使用整个图像。</p>
<p>表1c显示了这个实验的结果。如图所示，在给定的分辨率下，我们使用Mix-FFN的方法明显优于使用位置编码。此外，我们的方法对测试分辨率的差异不太敏感：当使用较低分辨率的位置编码时，准确率下降了3.3%。相比之下，当我们使用提议的Mix-FFN时，性能下降仅为0.7%。从这些结果中，我们可以得出结论，使用拟议的Mix-FFN比使用位置编码的编码器产生更好、更稳健的编码器。</p>
<h4 id="effective-receptive-field-evaluation"><a class="markdownIt-Anchor" href="#effective-receptive-field-evaluation"></a> Effective receptive field evaluation.</h4>
<p>在第3.2节中，我们认为我们的MLP解码器得益于Transformer与其他CNN模型相比具有更大的有效接收场。为了量化这种效果，在这个实验中，我们比较了我们的MLP-解码器与基于CNN的编码器（如ResNet或ResNeXt）一起使用时的性能。如表1d所示，将我们的MLP-解码器与基于CNN的编码器结合使用，其准确度明显低于与拟议的Transformer编码器结合使用。直观地说，由于CNN的感受野比Transformer小（见第3.2节的分析），MLP-解码器不足以进行全局推理。相比之下，将我们的Transformer编码器与MLP解码器结合起来，就能获得最佳性能。此外，对于Transformer编码器来说，有必要将低层次的局部特征和高层次的非局部特征结合起来，而不是仅有高层次的特征。</p>
<h3 id="43-comparison-to-state-of-the-art-methods"><a class="markdownIt-Anchor" href="#43-comparison-to-state-of-the-art-methods"></a> 4.3 Comparison to state of the art methods</h3>
<p>We now compare our results with existing approaches on the ADE20K [72], Cityscapes [71] and COCO-Stuff [73] datasets.</p>
<h4 id="ade20k-and-cityscapes"><a class="markdownIt-Anchor" href="#ade20k-and-cityscapes"></a> ADE20K and Cityscapes:</h4>
<p>表2总结了我们的结果，包括ADE20K和Cityscapes的参数、FLOPS、延迟和精度。在表的上半部分，我们报告了实时方法，包括最先进的方法和我们使用MiT-B0轻量级编码器的结果。在下半部分，我们关注性能，并报告我们的方法和使用更强大的编码器的相关工作的结果。</p>
<p>如图所示，在ADE20K上，SegFormer-B0仅用3.8M的参数和8.4G的FLOPs就能产生37.4%的mIoU，在参数、flops和延迟方面优于其他所有的实时同行。例如，与DeeplabV3+（MobileNetV2）相比，SegFormer-B0为7.4FPS，速度更快，保持3.4%的mIoU。此外，SegFormer-B5超过了所有其他方法，包括之前最好的SETR，并建立了一个新的最先进的51.8%，比SETR好1.6%的mIoU，同时效率明显提高。</p>
<p>如表2所示，我们的结果也适用于Cityscapes。SegFormer-B0产生了15.2 FPS和76.2%的mIoU（输入图像的短边为1024），这意味着与DeeplabV3+相比有1.3%的mIoU改进和2倍的速度提升。此外，在输入图像的短边为512时，SegFormer-B0的运行速度为47.6FPS，产生71.9%的mIoU，比ICNet快17.3FPS，提高4.2%。SegFormer-B5的最佳IOU为84.0%，比所有现有的方法至少高出1.8%的mIoU，它的运行速度比SETR[7]快5倍，体积小4倍。</p>
<p>在Cityscapes测试集上，我们遵循常见的设置[20]，将验证图像合并到训练集上，并使用Imagenet-1K预训练和使用Mapillary Vistas[76]报告结果。如表3所示，仅使用Cityscapes精细数据和Imagenet-1K预训练，我们的方法就获得了82.2%的mIoU，超过了所有其他方法，包括使用ImageNet-22K预训练和额外的Cityscapes粗略数据的SETR。使用Mapillary预训练，我们的方法设定了一个新的最先进的结果，即83.1% mIoU。图4显示了Cityscapes的定性结果，SegFormer提供了比SETR更好的细节，比DeeplabV3+更平滑的预测。</p>
<h4 id="coco-stuff"><a class="markdownIt-Anchor" href="#coco-stuff"></a> COCO-Stuff.</h4>
<p>最后，我们在完整的COCO-Stuff数据集上评估SegFormer。为了进行比较，由于现有的方法没有在这个数据集上提供结果，我们重现了最具代表性的方法，如DeeplabV3+、OCRNet和SETR。在这种情况下，这个数据集上的flops与ADE20K的报告相同。如表4所示，SegFormer-B5仅用84.7M的参数就达到了46.7%的mIoU，比SETR好0.9%，小4倍。总之，这些结果证明了SegFormer在语义分割方面的准确性、计算成本和模型大小方面的优越性。</p>
<h3 id="44-robustness-to-natural-corruptions"><a class="markdownIt-Anchor" href="#44-robustness-to-natural-corruptions"></a> 4.4 Robustness to natural corruptions</h3>
<p>模型的鲁棒性对于许多安全关键任务（如自动驾驶）非常重要[77]。在这个实验中，我们评估了SegFormer对常见的损坏和扰动的鲁棒性。为此，我们遵循[77]并生成了Cityscapes-C，它用来自噪声、模糊、天气和数字类别的16种算法生成的损坏扩展了Cityscapes验证集。我们将我们的方法与DeeplabV3+的变体和[77]中报告的其他方法进行比较。这个实验的结果总结在表5中。</p>
<p>我们的方法明显优于以前的方法，在高斯噪声上产生了高达588%的相对改进，在雪天气上产生了高达295%的改进。这些结果表明SegFormer具有很强的鲁棒性，我们设想在鲁棒性很重要的安全关键型应用中受益。</p>
<h2 id="5-conclusion"><a class="markdownIt-Anchor" href="#5-conclusion"></a> 5 Conclusion</h2>
<p>在本文中，我们提出了SegFormer，一种简单、干净而强大的语义分割方法，它包含一个无位置编码的、分层的Transformer编码器和一个轻量级的AllMLP解码器。它避免了以往方法中常见的复杂设计，导致了高效率和高性能。SegFormer不仅在常见的数据集上取得了新的成果，而且还显示了强大的零点稳定性。我们希望我们的方法可以作为语义分割的一个坚实的基线，并激励进一步的研究。一个限制是，尽管我们最小的3.7M参数模型比已知的CNN模型要小，但它是否能在只有100K内存的边缘设备的芯片中很好地工作还不清楚。我们把它留给未来的工作。</p>
<h2 id="附录"><a class="markdownIt-Anchor" href="#附录"></a> 附录</h2>
<h3 id="a-details-of-mit-series"><a class="markdownIt-Anchor" href="#a-details-of-mit-series"></a> A Details of MiT Series</h3>
<p>在本节中，我们列出了我们的混合Transformer（MiT）编码器的一些重要的超参数。通过改变这些参数，我们可以轻松地将我们的编码器从B0扩展到B5。</p>
<p>综上所述，我们的MiT的超参数列举如下。</p>
<ul>
<li>Ki：第i阶段中重叠patch嵌入的patch尺寸。</li>
<li>Si：第i阶段中的重叠patch嵌入的跨度。</li>
<li>Pi：第i阶段中重叠patch嵌入的填充尺寸；</li>
<li>Ci：第i阶段输出的通道编号。</li>
<li>Li：第i阶段的编码器层数。</li>
<li>Ri：第i阶段中高效自注意力的还原率。</li>
<li>Ni：第i阶段的高效自适应的头数。</li>
<li>Ei：阶段i中前馈层[78]的扩展率。</li>
</ul>
<p>表6显示了我们MiT系列的详细信息。为了便于有效讨论，我们为MiT编码器分配了B0到B5的代号，其中B0是为实时设计的最小模型，而B5是为高性能设计的最大模型。</p>
<h3 id="b-more-qualitative-results-on-mask-predictions"><a class="markdownIt-Anchor" href="#b-more-qualitative-results-on-mask-predictions"></a> B More Qualitative Results on Mask Predictions</h3>
<p>在图5中，我们在Cityscapes、ADE20K和COCO-Stuff上展示了与SETR和DeepLabV3+相比的更多定性结果。</p>
<p>与SETR相比，我们的SegFormer预测的掩模在物体边界附近的细节明显更细，因为我们的Transformer编码器可以捕获比SETR高得多的分辨率特征，从而保留了更详细的纹理信息。与DeepLabV3+相比，SegFormer减少了长距离的错误，这得益于Transformer编码器比ConvNet更大的有效接收域。</p>
<h3 id="c-more-visualization-on-effective-receptive-field"><a class="markdownIt-Anchor" href="#c-more-visualization-on-effective-receptive-field"></a> C More Visualization on Effective Receptive Field</h3>
<p>在图6中，我们选择了一些有代表性的图像和DeepLabV3+和SegFormer的有效感受野（ERF）。除了更大的ERF之外，SegFormer的ERF对图像的背景更加敏感。我们看到SegFormer的ERF学会了道路、汽车和建筑物的模式，而DeepLabV3+的ERF显示出相对固定的模式。结果还表明，我们的Transformer编码器比ConvNets有更强的特征提取能力。</p>
<h3 id="d-deeplabv3和segformer在cityscapes-c上的更多比较"><a class="markdownIt-Anchor" href="#d-deeplabv3和segformer在cityscapes-c上的更多比较"></a> D DeeplabV3+和SegFormer在Cityscapes-C上的更多比较</h3>
<p>在这一节中，我们详细展示了与SegFormer和DeepLabV3+相比的零照度。按照[77]，我们对4种 &quot;噪声 &quot;测试了3种严重程度，对其余12种腐败和扰动测试了5种严重程度。</p>
<p>如图7所示，随着严重程度的增加，DeepLabV3+显示出相当大的性能下降。相比之下，SegFormer的性能则相对稳定。此外，SegFormer在所有腐败/扰动和所有严重程度上都比DeepLabV3+有明显的优势，显示了出色的零点稳健性。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/STDC/" title="STDC"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">STDC</div></div><div class="info-2"><div class="info-item-1"> STDC Rethinking BiSeNet For Real-time Semantic Segmentation  Abstract 事实证明，BiSeNet [28, 27] 是一种用于实时分割的流行双流网络。然而，其增加额外路径以编码空间信息的原理非常耗时，而且由于缺乏特定任务的设计，借用预训练任务（如图像分类）的骨干可能会导致图像分割效率低下。为了解决这些问题，我们通过消除结构冗余，提出了一种新颖高效的结构，命名为短期密集串联网络（STDC 网络）。具体来说，我们逐步降低了特征图的维度，并利用特征图的聚合进行图像表示，这构成了 STDC 网络的基本模块。在解码器中，我们提出了细节聚合模块，以单流方式将空间信息的学习整合到低层。最后，将低层特征和深层特征融合起来，预测最终的分割结果。在 Cityscapes 和 CamVid 数据集上进行的大量实验证明了我们方法的有效性，在分割准确性和推理速度之间实现了良好的权衡。在 Cityscapes 数据集上，我们在测试集上实现了 71.9% 的 mIoU，在 NVIDIA GTX 1080Ti 上的推理速度为 250.4...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/SegNeXt/" title="SegNeXt"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">SegNeXt</div></div><div class="info-2"><div class="info-item-1"> SegNeXt  文章链接   Abstract 我们提出了SegNeXt，一个用于语义分割的简单卷积网络架构。由于自我注意在编码空间信息方面的效率，最近基于transformer的模型在语义分割领域占主导地位。在本文中，我们表明卷积注意是一种比transformer中的自我注意机制更有效率和效果的编码上下文信息的方式。通过重新审视成功的分割模型所拥有的特征，我们发现了导致分割模型性能提高的几个关键成分。这促使我们设计了一个新颖的卷积注意力网络，它使用廉价的卷积操作。在没有任何附加条件的情况下，我们的SegNeXt大大改善了以前最先进的方法在流行基准上的性能，包括ADE20K、Cityscapes、COCO-Stuff、Pascal VOC、Pascal Context和iSAID。值得注意的是，SegNeXt的性能优于EfficientNet-L2 w/NAS-FPN，在Pascal VOC...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BAM</div></div><div class="info-2"><div class="info-item-1"> BAM  Abstract 近来，小样本分割分割技术（FSS）得到了广泛的发展。然而，训练出来的模型偏向于所看到的类别，而不是理想的类别无关性，从而阻碍了对新内容的识别。本文提出了一种新颖而直接的见解来缓解这一问题。具体来说，我们在传统的 FSS 模型（元学习器）上增加了一个分支（基础学习器），以明确识别基础类别的目标，即不需要分割的区域。然后，对这两个学习器并行输出的粗略结果进行自适应整合，从而得出精确的分割预测结果。考虑到元学习器的敏感性，我们进一步引入了一个调整因子来估计输入图像对之间的场景差异，以促进模型的集合预测。在 PASCAL-5i 和 COCO-20i 上取得的显著性能提升验证了这一方法的有效性，而且令人惊讶的是，即使使用两个普通学习器，我们的多功能方案也创造了新的一流水平。此外，鉴于所提方法的独特性，我们还将其扩展到了更现实但更具挑战性的环境中，即广义 FSS，在这种环境中，基础类和新类别的像素都需要确定。  1. Introduction   得益于成熟的大规模数据集 [8, 9,...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#segformer"><span class="toc-text"> SegFormer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text"> 摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-text"> 1 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-text"> 2 相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2"><span class="toc-text"> 语义分割</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#transformer-backbones"><span class="toc-text"> Transformer backbones.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#transformers-for-specific-tasks"><span class="toc-text"> Transformers for specific tasks.</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-method"><span class="toc-text"> 3 Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-hierarchical-transformer-encoder"><span class="toc-text"> 3.1 Hierarchical Transformer Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%82%E6%AC%A1%E5%8C%96%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA"><span class="toc-text"> 层次化特征表示。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#overlapped-patch-merging"><span class="toc-text"> Overlapped Patch Merging.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#efficient-self-attention"><span class="toc-text"> Efficient Self-Attention.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mix-ffn"><span class="toc-text"> Mix-FFN.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-lightweight-all-mlp-decoder"><span class="toc-text"> 3.2 Lightweight All-MLP Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#effective-receptive-field-analysis"><span class="toc-text"> Effective Receptive Field Analysis.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-relationship-to-setr"><span class="toc-text"> 3.3 Relationship to SETR.</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-experiments"><span class="toc-text"> 4 Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-experimental-settings"><span class="toc-text"> 4.1 Experimental Settings</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#datasets"><span class="toc-text"> Datasets:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#implementation-details"><span class="toc-text"> Implementation details:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#42-ablation-studies"><span class="toc-text"> 4.2 Ablation Studies</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#influence-of-the-size-of-model"><span class="toc-text"> Influence of the size of model.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#influence-of-c-the-mlp-decoder-channel-dimension"><span class="toc-text"> Influence of C, the MLP decoder channel dimension.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mix-ffn-vs-positional-encoder-pe"><span class="toc-text"> Mix-FFN vs. Positional Encoder (PE).</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#effective-receptive-field-evaluation"><span class="toc-text"> Effective receptive field evaluation.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-comparison-to-state-of-the-art-methods"><span class="toc-text"> 4.3 Comparison to state of the art methods</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ade20k-and-cityscapes"><span class="toc-text"> ADE20K and Cityscapes:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#coco-stuff"><span class="toc-text"> COCO-Stuff.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#44-robustness-to-natural-corruptions"><span class="toc-text"> 4.4 Robustness to natural corruptions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-conclusion"><span class="toc-text"> 5 Conclusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-text"> 附录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#a-details-of-mit-series"><span class="toc-text"> A Details of MiT Series</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#b-more-qualitative-results-on-mask-predictions"><span class="toc-text"> B More Qualitative Results on Mask Predictions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#c-more-visualization-on-effective-receptive-field"><span class="toc-text"> C More Visualization on Effective Receptive Field</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#d-deeplabv3%E5%92%8Csegformer%E5%9C%A8cityscapes-c%E4%B8%8A%E7%9A%84%E6%9B%B4%E5%A4%9A%E6%AF%94%E8%BE%83"><span class="toc-text"> D DeeplabV3+和SegFormer在Cityscapes-C上的更多比较</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/11/%E5%B0%8F%E7%B1%B3/syzkaller/syzkaller01-%E9%85%8D%E7%BD%AE/" title="syzkaller01-配置">syzkaller01-配置</a><time datetime="2024-12-10T17:10:45.000Z" title="发表于 2024-12-11 01:10:45">2024-12-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM">BAM</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>