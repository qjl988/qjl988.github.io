<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Dental Lesion Segmentation Using an Improved ICNet Network with Attention | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Dental Lesion Segmentation Using an Improved ICNet Network with Attention 文章链接  Abstract 牙齿病变的精确分割是建立智能牙齿病变检测系统的关键。为了解决牙齿病变与正常牙齿组织相似而难以分割的问题，我们提出了一种改进的图像级联网络（ICNet）网络分割方法来分割各种病变类型，如牙结石、牙龈炎和牙石。首先，利用IC">
<meta property="og:type" content="article">
<meta property="og:title" content="Dental Lesion Segmentation Using an Improved ICNet Network with Attention">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Dental%20Lesion%20Segmentation%20Using%20an%20Improved%20ICNet%20Network%20with%20Attention/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="Dental Lesion Segmentation Using an Improved ICNet Network with Attention 文章链接  Abstract 牙齿病变的精确分割是建立智能牙齿病变检测系统的关键。为了解决牙齿病变与正常牙齿组织相似而难以分割的问题，我们提出了一种改进的图像级联网络（ICNet）网络分割方法来分割各种病变类型，如牙结石、牙龈炎和牙石。首先，利用IC">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T16:45:56.792Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Dental%20Lesion%20Segmentation%20Using%20an%20Improved%20ICNet%20Network%20with%20Attention/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Dental Lesion Segmentation Using an Improved ICNet Network with Attention',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">Dental Lesion Segmentation Using an Improved ICNet Network with Attention</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Dental Lesion Segmentation Using an Improved ICNet Network with Attention</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T16:45:56.792Z" title="更新于 2024-12-11 00:45:56">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>24分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="dental-lesion-segmentation-using-an-improved-icnet-network-with-attention"><a class="markdownIt-Anchor" href="#dental-lesion-segmentation-using-an-improved-icnet-network-with-attention"></a> Dental Lesion Segmentation Using an Improved ICNet Network with Attention</h1>
<p><a href="zotero://open-pdf/library/items/QFRTXXGG">文章链接</a></p>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>牙齿病变的精确分割是建立智能牙齿病变检测系统的关键。为了解决牙齿病变与正常牙齿组织相似而难以分割的问题，我们提出了一种改进的图像级联网络（ICNet）网络分割方法来分割各种病变类型，如牙结石、牙龈炎和牙石。首先，利用ICNet网络模型实现对病变的实时分割。其次，将卷积块注意模块（CBAM）整合到ICNet网络结构中，将空间注意模块中的大尺寸卷积替换为分层扩张卷积，在增强相关特征的同时抑制无用特征，解决病变分割不准确的问题。最后，网络模型中的部分卷积被替换为不对称卷积，以减少注意力模块所增加的计算量。实验结果表明，与全卷积网络（FCN）、U-Net、SegNet等分割算法相比，我们的方法在分割效果上有明显改善，而且图像处理频率更高，满足了对牙齿病变分割精度的实时要求。</p>
<h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1. Introduction</h2>
<p>计算机视觉领域的不断进步，推动了在线智能诊断和治疗系统的研究。随着人类生活条件的改善，人们对牙齿病变的关注度越来越高，加之疫情的影响，医院的诊断也变得很不方便。 在临床上出现了以下问题：（1）有些病变与牙齿结构相似，医生容易漏诊或误诊。(2)随着就诊人数的增加，复查众多片子增加了医生的工作量，导致复查时间变慢，无法及时反馈给患者。由于医疗资源分配不均，偏远地区的患者缺乏获得深入治疗的机会。因此，在线诊所行业不断涌现，以满足额外的需求。牙科病变鉴定系统可以发挥预诊和辅助诊断的作用，使病情较轻的患者可以节省看病时间，而病情严重的患者可以得到彻底的诊断，并可以随时随地获得牙科病变诊断结果。这就减少了疫情发生时必要的现场诊断次数，防止了COVID-19病毒的传播。对牙齿病变的实时分割算法的研究已成为开发智能牙齿病变检测系统的关键。</p>
<p>牙科诊断技术主要是对X光片和光学相干断层扫描（OCT）图像进行增强或分割，以协助医生进行诊断。Lee等人[1]提出了垂直强度变换函数（VIFT）来解决减少基于光照的牙齿灰度差异的问题，然后使用K-means算法和Markov随机场来指定检测范围，最后分割出牙结石的候选块。然而，这些传统方法对于特定的牙科诊断任务的准确性仍有改进的余地，对于有多个病变的单一图像，传统方法无法识别和分割多个类别。</p>
<p>目前，许多学者也在尝试将深度学习方法用于牙科诊断。Kreis [<a href="#_bookmark2">2</a>] 使用卷积神经网络（CNN）来检测全景牙科X射线上的牙周骨质流失（PBL）。Casalegno等人[<a href="#_bookmark3">3</a>]使用近红外光透射（NILT）图像进行龋齿分割。 Jae-Hong Lee [<a href="#_bookmark4">4</a>] 评估了深度CNN算法在根尖周的X射线上检测和诊断龋齿的有效性。Yu[<a href="#_bookmark5">5</a>]等人也评估了CNN在通过侧头测量对骨骼进行分类的性能。最近，Wen等人[<a href="#_bookmark6">6</a>]使用深度学习方法来检测牙齿病变。他们建立了一个多任务网络结构，该模型由三个子网组成。FNet（特征提取子网），LNet（位置子网）和CNet（分类子网）。 这项工作主要是检测牙结石、牙龈炎和软沉积物。他们用不同颜色的候选帧来识别牙结石和牙龈炎，而对于软沉积物，只进行了图像级别的分类。这项工作的问题是，牙齿病变的形状是不规则的。矩形标签法通过网络学习周围正常组织的特征，检测结果范围大于病变的实际像素范围。此外，网络模型中的LNet子网是一个两级定位网络，所以模型的实时性是一个问题。与以往工作不同的是，我们使用口内摄像头采集牙齿病变的RGB图像，并使用深度学习方法在像素层面对多个病变进行分割，以更准确地检测病变的范围。</p>
<p>许多诊断方法都是基于目前流行的三维成像技术，如CT、3DMD等[7]。因此，三维点云数据分割也是热点研究课题之一。Karatas等人介绍了基本的三维图像分割方法，并总结了三维成像技术的现状，评估了它们在正畸中的应用。 为了解决点云数据的稀疏性问题，Graham等人提出了新的稀疏卷积运算SSCNs[<a href="#bookmark8">8</a>]，以更有效地处理稀疏数据。Liu等人提出了一个卷积BEACon网络[<a href="#_bookmark9">9</a>]，该网络具有嵌入式注意力边界，用于点云实例分割，该网络基于人类对几何和颜色的感知，将几何和颜色结合到注意力权重中，用于物体识别动机。 SMU-Net[<a href="#_bookmark10">10</a>]使用显著性映射引导主网络和次网络分别学习前景显著性和背景显著性表征，获得良好的分割结果，但其在小规模数据集上对模糊病变的边缘分割的有效性有待提高。</p>
<p>深度学习使用端到端的训练来预测复杂的模型，能够完成复杂场景下的病变分割。Long等人[<a href="#_bookmark11">11</a>]提出了一个基于CNN的全卷积网络，首次实现了像素级分类。他们巧妙地用卷积层取代了包含复杂计算的全连接层，并在卷积后使用解旋来恢复原始尺寸。随后，编码器-解码器被广泛用于语义分割中。所提出的SegNet[<a href="#_bookmark12">12</a>]网络与FCN类似，不同的是在解码操作中的最大池化操作中记录了最大值的位置，然后在解码时通过相应的池化指数实现非线性上采样。上采样后得到一个稀疏的特征图，然后通过普通卷积得到一个密集的特征图，再重复上采样的过程。这就减少了编码阶段的计算量。Ronneberger等人提出的U-Net[<a href="#_bookmark13">13</a>]是对编码器-解码器结构的改进，它通过连接编码器和解码器的相应层，将低级和高级特征合并起来，以消除差距。</p>
<p>在实际的分割场景中，我们要在保证精度的前提下追求实时性能。ENet[14]认为，解码结构只用于上采样编码的输出，只用于细化边缘细节，所以不需要特别深。此外，全卷积过程非常耗时，所以ENet只使用一层全卷积，并利用更少的参数，获得更快的速度。 然而，ENet在保证实时性的同时，放弃了一定的准确率，导致分割精度较低。ICNet[15]使用PSPNet[16]的金字塔池模块来融合多尺度的上下文信息，将网络结构分为三个分支：低分辨率、中分辨率和高分辨率。它使用低分辨率来完成语义分割，用高分辨率策略来重新确定分割结果，提高了模型的分割精度。此外，它使用级联标签来指导每个分支的训练，加快了模型的收敛和预测，并提高了实时性能。</p>
<p>上述分割网络在遥感图像[<a href="#_bookmark17">17</a>]、街景图像[<a href="#_bookmark18">18</a>]、病变图像[<a href="#_bookmark19">19</a>]的分割中表现良好，但到目前为止还没有在牙齿病变中应用。我们发现在用FCN、SegNet、ENet等分割算法测试时，存在分割过度和分割不足的情况。分析认为，牙齿病变和正常的牙齿组织在纹理和颜色上相似，具有相似的特征，这使得正确分割牙齿病变及其边缘变得困难。此外，牙齿病变的分割需要在合理的时间内保证准确性，从而满足诊断设备和海量数据的需要。在这篇文章中，我们提出将注意力机制整合到ICNet网络中，以解决上述两个问题。本文的突出观点和贡献如下。</p>
<ol>
<li>自建的牙齿病变数据集包括四种类型的病变：牙结石、牙龈炎、牙垢和磨损的表面，并用ACE色彩均衡算法对光源造成的过曝图像进行预处理。</li>
<li>轻量级的卷积块注意模块（CBAM）注意模块被整合到图像级联网络（ICNet）网络的中低层分支中，这样高分辨率的分支可以更好地引导中低层分支的特征，空间注意的大尺寸卷积使用堆叠的空心体积进行产品替换。</li>
<li>低分辨率和中分辨率分支中的常规卷积被替换为非对称卷积，以减少计算量。</li>
</ol>
<h2 id="2-related-work"><a class="markdownIt-Anchor" href="#2-related-work"></a> 2. Related Work</h2>
<p>在本节中，我们将介绍与我们的方法相关的语义分割架构和模型。 这些架构和模型在图像分割任务中被广泛使用。</p>
<h3 id="21-encoderdecoder-network"><a class="markdownIt-Anchor" href="#21-encoderdecoder-network"></a> 2.1 Encoder–Decoder Network</h3>
<p>我们的分割网络是基于编码器-解码器的网络结构。2015年，Long等人提出的FCN使用了一个卷积层来代替整个网络的完整卷积层，并使用去卷积层进行上采样来还原分割结果。这种完全卷积网络被称为编码器-解码器网络 。此后，大多数图像分割网络都采用了编解码网络的形式。在FCN的基础上，U-Net建立了一个更复杂的去编码器，在相应的层次上增加补偿，以补偿局部信息。SegNet是U-Net的进一步扩展，它在编码器模型中实现了最大像素池操作，减少了解码阶段的计算量。ICNet首先将输入图像的大小改为原始图像的二分之一和四分之一，并将原始图像组合成输入图像，输入到低、中、高像素分支。在低、中分辨率下有许多网络层，但图像分辨率很低，这就节省了计算时间。虽然高分辨率分支的图像较大，但输入的网络层数量较少，导致时间开销相对较小，从而实现了实时目标。通过CFF模块对各分支提取的特征图进行融合，最后通过解码得到分割结果。</p>
<h3 id="22-attention-mechanism"><a class="markdownIt-Anchor" href="#22-attention-mechanism"></a> 2.2 Attention Mechanism</h3>
<p>CNN的一个局限性是它很难有效地学习全局信息，而且对图像细节的分割也不完美。近年来，研究人员将特征融合和注意力机制整合到网络模型中，以提高网络学习全局特征的能力，更好地分割细节。PspNet是通过金字塔池模块提出的，它根据不同的区域聚合上下文信息，并将像素级特征扩展到专门设计的全局金字塔池。 局部和全局的线索共同作用，使最终的预测更加可靠。 近年来，大量的研究证明，在网络模型中引入注意力模块可以有效提高性能。SA-UNet[<a href="#_bookmark23">26</a>]引入了空间注意模块SE[<a href="#_bookmark24">27</a>]，可以沿空间维度推断出注意图，并将注意图与输入特征图相乘，进行自适应特征重构。 这项工作对视网膜血管分割的影响超过了Ronneberger等人的原始U-Net[<a href="#_bookmark25">24</a>]提出了一个注意力门结构。AG模块连接到每一跳连接的末端，并对提取的特征实施注意机制。在我们提出的模型中，我们使用了卷积块注意力模块CBAM，它将输入图像和注意力序列应用于通道，然后应用于空间维度。CBAM的结果是一个加权的特征图，它考虑到了输入图像的通道和空间区域。</p>
<h3 id="23-multiple-forms-of-convolution"><a class="markdownIt-Anchor" href="#23-multiple-forms-of-convolution"></a> 2.3 Multiple Forms of Convolution</h3>
<p>许多研究提出了各种形式的卷积来提高分割网络的性能。Szegedy等人[<a href="#_bookmark26">28</a>]首先提出了1×1卷积的概念，其主要功能是降低维度和节省计算成本。就卷积方法而言，1×1卷积与传统的卷积没有区别，主要区别在于应用场景和功能。他们还提出了一种非对称卷积；也就是说，n*n的卷积可以被1×n后的n×1的卷积所取代，这样得到的效果与传统卷积相同，计算量也减少了。深度可分离卷积[<a href="#_bookmark27">29</a>]是基于1×1卷积的创新，它包括两个部分，深度卷积和1×1卷积。卷积的目的是使用卷积核对每个输入进行单独卷积；也就是说，通道被分离，然后合并。Koltun等人[<a href="#_bookmark28">30</a>]首次将扩张卷积用于图像分割。扩张卷积是通过在卷积核的元素之间增加一些空间来扩大卷积核的过程，这在不增加参数数量的情况下增加了感受场。Papandreou等人提出的DeepLabV2算法[<a href="#_bookmark29">31</a>]使用扩张卷积来提取特征。许多研究已经使用扩张卷积[<a href="#_bookmark29">31</a>,<a href="#_bookmark30">32</a>]来取代传统卷积。</p>
<h2 id="3-methods"><a class="markdownIt-Anchor" href="#3-methods"></a> 3. Methods</h2>
<h3 id="31-adding-the-cbam-attention-module-to-low-and-medium-resolution-branches"><a class="markdownIt-Anchor" href="#31-adding-the-cbam-attention-module-to-low-and-medium-resolution-branches"></a> 3.1 Adding the CBAM Attention Module to Low- and Medium-Resolution Branches</h3>
<p>本文的网络框架主要是一个ICNet网络，它由三个网络分支组成：低、中、高分辨率分支，如图1所示。在我们设计的网络中，低分辨率网络是根据Resnet50的前11个卷积块设计的，使用扩张卷积来增强特征感知场，使用非对称卷积来减少计算量。中分辨率分支是根据Resnet50的前5个卷积块设计的，在最后两个卷积中使用非对称卷积，两个分支网络头都包括一个7×7卷积和最大平均池。高分辨率分支被设计为轻量级网络，由三个卷积块组成，包括3×3卷积、Relu激活函数、BN层、1×1卷积和Relu激活函数，依次进行。</p>
<p><img src="/img/loading.gif" data-original="images/202306142019089.png" alt="" /></p>
<p>牙齿病变和正常牙齿之间存在着形态上的相似性，边缘难以区分。 此外，虽然ICNet在低分辨率分支中进行特征提取时保留了大部分语义信息，但会损失细节和边缘信息，导致分割精度不理想。在本文中，CBAM模块被添加到低分辨率分支中，如图1所示。通过通道和空间注意力的串行组合，重要的特征可以被增强，不重要的特征可以被抑制，从而改善网络的性能，提高模型学习细节的能力。</p>
<p>CBAM将注意力应用于通道和空间两个维度，如图2所示。CBAM和SE模块一样，可以嵌入到目前大多数主流网络中。它可以提高网络模型的特征提取能力，而不会明显增加计算和参数的数量。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230624152747420.png" alt="image-20230624152747420" /></p>
<p>CBAM包括两块内容，通道注意力模块和空间注意力模块；也就是通道注意力CAM和空间注意力SAM。</p>
<p>嵌入CBAM机制的网络首先使用通道注意力映射对构成生成的特征图F进行全局汇集和最大汇集。汇总结果被连接到多层感知器进行加法运算，通道权重系数Mc是通过sigmoid激活函数生成的。最后，该权重系数与原始特征图F相乘，得到通道权重调整后的特征图F\，如图3所示。通道注意力映射过程如公式（1）所示。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230624152729494.png" alt="image-20230624152729494" /></p>
<p>方程（1）中，MLP代表多层感知器；6是激活函数。之后，由通道注意产生的特征图被送到空间注意进行处理。空间注意映射对加权特征图F/进行全局最大池化和平均池化的串联，用卷积法将维度降低为单通道特征图，用sigmoid函数激活空间特征矩阵Ms，对权重矩阵和特征图F进行点乘运算，得到最终需要的空间特征图F，空间注意映射过程如公式（2）。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230624152221722.png" alt="image-20230624152221722" /></p>
<p>式中，f7x7表示卷积核为7x7的卷积层；δ为激活函数；表示串联。</p>
<p>在本文中，我们将空间注意模块中的7×7大卷积核改为两个3×3的扩张卷积，扩张率为2，减少了空间注意模块中的参数数量。修改后的CBAM模型如图4所示，其中X和Y分别代表输入和输出特征矩阵，C、W和H以及Cl、Wl和Hl分别代表X和Y的三维信息。</p>
<p>为了抑制无用特征对模型的影响，CBAM注意力模型与四分之一分辨率分支的最后三个卷积块和二分之一分辨率的最后两个卷积块的外部卷积相连，以提高分割精度。 低分辨率和中分辨率分支前面的卷积块主要是提取图像特征，然后用注意力层来增强特征提取，使得到的特征更加准确，高分辨率分支可以更好地指导我们在低分辨率和中分辨率下更好地达到分割效果。</p>
<p>最后，通过CFF模块将三个分支产生的特征图进行融合，如图5所示。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230624152306265.png" alt="image-20230624152306265" /></p>
<p>CFF模块有三个输入，即特征图F1、F2和标签。对于F1，它被放大两倍，使其与F2的大小相同，然后F1的特征通过卷积3×3大小的孔和2的扩张率进行重构。对于F2，它被1×1卷积，使其与F1的通道数相同，然后使用BN层将特征归一化。然后将其加入到上面得到的F1特征中，得到F2\。为了加强对特征F1的学习，对1的上采样特征使用辅助标签进行引导，优化损失。其中，对于第一个CFF模块，F1和F2分别是由低分辨率和中分辨率分支得到的特征。</p>
<h3 id="32-asymmetric-convolution-replaces-regular-convolution"><a class="markdownIt-Anchor" href="#32-asymmetric-convolution-replaces-regular-convolution"></a> 3.2 Asymmetric Convolution Replaces Regular Convolution</h3>
<p>一个大的卷积核可以创造一个更大的接受场，但这也意味着有更多的参数。非对称卷积可以大大减少卷积阶段的计算量而不降低精度，从而减少模型的大小，提高模型的实时分割能力。本文用非对称卷积取代了部分常规卷积，进一步减少了ICNet的计算数量。</p>
<p>在ICNet网络中加入CBAM注意力模块，使模型的计算量略有增加。与传统卷积相比，非对称卷积可以大大减少卷积阶段的计算量而不损失精度。 首先进行n×1卷积，然后进行1×n卷积，如图6所示，这与直接进行n×n卷积的结果一致，但乘法运算的规模从n×n变为2×n；所以，n越大，非对称卷积减少计算量的效果越明显。在本文中，二分之一和四分之一分辨率分支用3×1卷积代替3×3卷积，然后再加入1×3卷积，如图6所示，从而减少了计算量。</p>
<h2 id="4-experiment"><a class="markdownIt-Anchor" href="#4-experiment"></a> 4. Experiment</h2>
<h3 id="41-datasets"><a class="markdownIt-Anchor" href="#41-datasets"></a> 4.1. Datasets</h3>
<h4 id="411-data-collection"><a class="markdownIt-Anchor" href="#411-data-collection"></a> 4.1.1 Data Collection</h4>
<p>使用口内照相机收集牙结石、牙龈炎、牙石和磨损表面的图像。通过从外部、内部和顶部三个角度拍摄，获得了200张牙齿病变的图像。表<a href="#_bookmark38">1</a>显示了所收集图像中各种病变的出现次数。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230624235320775.png" alt="image-20230624235320775" /></p>
<h4 id="412-data-augmentation"><a class="markdownIt-Anchor" href="#412-data-augmentation"></a> 4.1.2. Data Augmentation</h4>
<p>为了扩大样本之间的差异，保证后期模型训练的泛化能力，对收集到的牙齿病变图片进行随机裁剪，恢复原始尺寸并翻拍，然后调整对比度、亮度、饱和度等。然后，根据比例不变的原则，将原始牙齿病变图片缩放为512×512像素，通过人工筛选去除扭曲的图片，选取400张牙齿病变图片作为原始数据集。图<a href="#_bookmark39">7</a>为该数据集实例，图<a href="#_bookmark40">8</a>为数据增强前后的各种病变数量。最后，使用Label Me标签工具模仿PASCAL VOC2012数据集的格式，对病变图像中的牙结石、牙龈炎、牙石和磨损面四类进行手工标注，并将数据集按7:2:1的比例分为训练集、验证集和测试集。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230624235356270.png" alt="image-20230624235356270" /></p>
<p><img src="/img/loading.gif" data-original="images/image-20230624235408783.png" alt="image-20230624235408783" /></p>
<p>牙垢数据很小，但它有更突出的特征，如颜色和纹理。为了应对少量的牙垢数据，我们在训练过程中加入了一些训练技术，如对牙垢数据集进行预训练以初始化权重，然后用预训练的模型来训练我们的多类病变分割模型。在训练过程中，我们还通过随机复制对牙垢数据进行过量取样，以便对少量的牙垢进行更准确的预测，我们还在网络中加入了dropout，以防止牙垢数据的过度过滤。</p>
<p><strong>此外，牙齿病变数据集被过度曝光，导致病变图像的边缘不清晰，细节不明确。 正常牙齿图像的直方图比较平衡，而曝光过度的牙齿图像有太多的高亮度像素，这导致直方图向右移动，如图9所示。</strong></p>
<p><img src="/img/loading.gif" data-original="images/image-20230624235419209.png" alt="image-20230624235419209" /></p>
<p><strong>本文使用ACE自动色彩均衡算法对数据集中的不清晰图像进行色彩平衡。 该算法考虑了图像中颜色和亮度之间的空间位置关系，对局部特征进行自适应调整，对图像亮度和颜色进行调整，对局部和非线性特征进行对比度调整，并满足灰色世界理论假说和白斑假说。ACE算法包括两个步骤。第一步是调整图像的颜色/空间域，完成图像的色差校正，得到空间重建的图像，如公式（3）所示。</strong></p>
<p><img src="/img/loading.gif" data-original="images/image-20230624235431061.png" alt="image-20230624235431061" /></p>
<p>式中，Rc是中间结果，Ic(p入_Ic(j入是两个不同点的亮度差，d(p，j)是距离度函数，r()是度性能函数，必须是奇数函数。这一步可以适应局部图像的对比度，r可以放大小差异，丰富大差异，根据局部内容扩大或压缩动态范围。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230624235440899.png" alt="image-20230624235440899" /></p>
<h3 id="42-metrics"><a class="markdownIt-Anchor" href="#42-metrics"></a> 4.2 Metrics</h3>
<p>为了比较我们方法的性能，我们使用训练模型对测试集进行分割，并将分割后的图像与屏蔽后的标签进行比较。在分割精度方面，我们的指标主要包括像素精度（Acc）、平均交互比（mIoU）和F1得分。在实时性能方面，我们主要比较单张图片的计算量和推理时间。我们对牙齿病变进行语义分割，这是一个像素级的分割。在计算机视觉的深度学习图像分割领域，mIoU值是衡量图像分割准确性的一个重要指标。假设有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>个类别，$ p_{ij} $代表实际类别为i类但预测结果为j类的像素数。 mIoU的计算公式如公式（5）所示。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230624235453766.png" alt="image-20230624235453766" /></p>
<h3 id="43-loss-function"><a class="markdownIt-Anchor" href="#43-loss-function"></a> 4.3. Loss Function</h3>
<p>ICNet在每个分支训练中加入损失权重，并对加权SoftMax交叉熵进行优化，其损失函数L可表示为：</p>
<p><img src="/img/loading.gif" data-original="images/image-20230624235551996.png" alt="image-20230624235551996" /></p>
<p>其中L1、L2和L3分别为低、中、高分辨率分支的损失，W1、W2和W3分别为低、中、高分辨率分支的损失函数的权重。通常情况下，如果高分辨率分支的权重W1设置为1，中分辨率和低分辨率分支的权重W2和W3分别为0.4和0.16。</p>
<h3 id="44-experimental-details"><a class="markdownIt-Anchor" href="#44-experimental-details"></a> 4.4. Experimental Details</h3>
<p>我们在一个自建的牙齿病变数据集上评估了我们的方法。在训练过程中，我们使用Resnet50作为骨干网络，当验证集的损失在20个epochs内没有减少时，我们使用SGD优化方法进行训练。训练过程中损失和acc的数据转换情况如图10所示。我们的方法具有更快的收敛率。我们替换了CBAM中空间注意模块中的大尺寸卷积，并进行消融实验。最后，我们增强和降低了原始图像的亮度，测试了不同亮度下我们模型的分割结果。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230624235650149.png" alt="image-20230624235650149" /></p>
<p>本文的实验都是基于TensorFlow深度学习框架，在Bit hub云服务器上完成，显卡信息为gtx1080。</p>
<h2 id="5-results"><a class="markdownIt-Anchor" href="#5-results"></a> 5. Results</h2>
<h3 id="51-contrast-test-with-other-segmentation-algorithms"><a class="markdownIt-Anchor" href="#51-contrast-test-with-other-segmentation-algorithms"></a> 5.1. Contrast Test with Other Segmentation Algorithms</h3>
<p>目前，随着卷积神经网络的发展，越来越多的深度学习方法被用于语义图像分割。然而，不同的任务和方法的分割性能是显著不同的。为了进一步检验该方法在牙列图像实时语义分割中的利弊，上述四个模型的训练参数与改进后的ICNet的参数相同。它们都是根据自动保存最优模型的策略进行训练，然后在验证集上进行测试。测试指标主要从分割精度和时间性能两方面入手，包括Acc、mloUthe F1 score和单张图片的推理时间。从表2可以看出，我们的方法具有最高的Acc、mloU和F1得分，得分分别为0.8897、78.67%和0.8890。与ICNet相比，改进后的ICNet分割精度、mIoU和F1得分分别高出0.0384、3.91%和0.0397。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230627001646694.png" alt="image-20230627001646694" /></p>
<p>此外，从图11的可视化结果来看，U-Net和SegNet有过度分割的现象。也就是说，没有病变的部分被分割了，而ENet和ICNet则存在分割不足的问题，这使得我们很难正确识别正常的牙齿组织和病变。改进后的ICNet大大改善了过度分割的情况，结果更接近标签。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230627001729756.png" alt="image-20230627001729756" /></p>
<p>在实时分割方面，ENet、UNet、SegNet、ICNet和改进的ICNet对一张图像的分割分别需要833毫秒、696毫秒、739毫秒、805毫秒、307毫秒和395毫秒。与ENet、U-Net和SegNet相比，改进后的ICNet分别缩短了34.63%、38.43%和43.47%的时间，与FCN相比缩短了近一半的时间，与ICNet相比增加的时间更少。从表3可以看出，改进后的ICNet比ICNet的计算成本略高。</p>
<p>从以上评价指标可以看出，我们的方法具有最好的分割效果，当分割时间接近ICNet时，我们方法的三个指标都有不同程度的提高。这些指标都有不同程度的提高。在所有方法的可视化结果中，我们的分割效果是最接近真实标签的。图12的横轴是分割的频率，纵轴是MIOU，图的右上角是最佳方法。图12显示，我们的方法在准确性和时间上都是最优的。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230627001820716.png" alt="image-20230627001820716" /></p>
<p>在四种病变类型中，牙结石和磨损面与正常牙齿组织特征最相似。图13显示了我们的方法和其他算法对牙结石和磨损面的分割可视化结果。FCNU-Net和SegNet对牙结石进行了分割。ENet和ICNet有一定的分割不足问题。我们的方法的可视化效果最接近于标签。在穿戴表面分割的可视化结果中，ENet分割的e最差。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230627001833245.png" alt="image-20230627001833245" /></p>
<h3 id="52-segmentation-under-different-brightness"><a class="markdownIt-Anchor" href="#52-segmentation-under-different-brightness"></a> 5.2. Segmentation under Different Brightness</h3>
<p>在采集口腔图像时，每张图像的亮度很难保持，亮度成为影响分割质量的重要因素之一。因此，本文对不同亮度下的分割和牙齿病变图像进行了比较。我们将原始图像的亮度设置为P，将图像亮度降低到原始图像的0.7倍，并将其提高到原始图像的1.3倍进行图像分割。</p>
<p>从表4可以看出，当亮度被调整到0.7P或1.3P时，分割的准确性会降低。分析认为，在太亮和太暗的图像中识别病变的特征是比较困难的，ICNet的分割精度分别下降了2.33%和4.5%，而我们方法的分割精度分别下降了2.68%和5.34%。根据可视化的结果，在图14中，ICNet在0.7P的时候有一个分割不足的问题，在1.3P的时候有一个分割过度的问题，我们的方法在0.7和1.3P的时候更接近真实标签。</p>
<h3 id="53-ablation-experiment-of-cbam"><a class="markdownIt-Anchor" href="#53-ablation-experiment-of-cbam"></a> 5.3. Ablation Experiment of CBAM</h3>
<p>我们用大小为3×3、稀释率为2的卷积核代替CBAM中空间注意中的7×7大尺寸卷积核，如表4所示，卷积核大小为7×7的标准卷积用ICnet+CBAM7×7表示；卷积核大小为3×3，空隙率为2的空隙卷积，用ICNet+CBAM3×3表示。我们使用扩张卷积，卷积核大小为3×3，扩张率为2倍，用CBAM2×3×3表示。与作为基准的ICNet+CBAM3×3的参数量相比，我们将ICNet+CBAM3×3的参数增量设为0。</p>
<p>从表5可以看出，用两个3×3的卷积代替空间注意中的7×7卷积后，参数数量减少后，mIoU增加了0.55%。实验表明，稀释率为2的3×3稀释卷积与7×7的标准卷积具有相同的知觉领域，两者的效果是近似的。此外，对比第三组实验结果，发现用稀释层堆叠多个卷积层也能提高空间注意模块的特征表达能力。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230831003939313.png" alt="image-20230831003939313" /></p>
<h2 id="6-conclusions"><a class="markdownIt-Anchor" href="#6-conclusions"></a> 6. Conclusions</h2>
<p>在本文中，对于一小部分牙科病变数据集，通过随机裁剪和ﬂipping对数据进行了扩充。ACE自动色彩均衡算法解决了由光源引起的病变图像模糊的问题。将增强后的数据和处理后的图像与原始数据集结合成一个新的数据集，使所得到的模型更具有通用性。</p>
<p>牙齿病变的边缘与正常牙齿高度相似，边缘难以细分。此外，虽然ICNet在低、中分辨率分支进行特征提取时获得了大部分语义信息，但细节容易丢失，导致病变分割不准确。因此，本文在特征提取阶段增加了一个轻量级的CBAM模块，它可以更好地获取图像的语义信息，使高分辨率分支可以更好地指导低分辨率生成的特征图，从而提高分割的准确性。</p>
<p>在低、中分辨率卷积阶段，我们用非对称卷积代替了3×3卷积，这进一步降低了模型的计算复杂性。我们还确保提高模型精度所需的时间不会明显增加。</p>
<p>尽管我们的方法极大地提高了各种牙齿病变的分割效果，但光照强度对分割效果有很大影响。此外，病变图像的标注也需要大量的努力。在未来的研究中，我们将建立一个高度通用的弱监督网络，以解决标注困难和光线影响大的问题。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Danet/" title="Dual Attention Network for Scene Segmentation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Dual Attention Network for Scene Segmentation</div></div><div class="info-2"><div class="info-item-1"> Dual Attention Network for Scene Segmentation  Abstract 在本文中，我们通过捕捉基于自我注意机制的丰富的上下文依赖来解决场景分割任务。与之前通过多尺度特征融合来捕捉上下文的工作不同，我们提出了一个双注意网络（DANet）来适应性地整合局部特征和它们的全局依赖关系。具体来说，我们在扩张的FCN之上附加了两种注意力模块，它们分别在空间和通道维度上模拟语义的相互依赖性。位置注意模块通过所有位置上的特征的加权和，选择性地聚合每个位置上的特征。类似的特征会相互关联，而不考虑它们的距离。同时，通道注意模块通过整合所有通道图中的相关特征，选择性地强调相互依赖的通道图。我们将两个注意力模块的输出相加，进一步改善特征表示，这有助于获得更精确的分割结果。我们在三个具有挑战性的场景分割数据集，即Cityscapes、PASCAL Context和COCO Stuff数据集上实现了新的最先进的分割性能。特别是，在不使用粗略数据的情况下，在Cityscapes测试集上取得了81.5%的平均IoU得分。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/DAPPM(DDRNet)/" title="Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes</div></div><div class="info-2"><div class="info-item-1"> 用于实时和准确道路场景语义分割的深度双分辨率网络 Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes  Abstract 语义分割是自动驾驶汽车理解周围场景的关键技术。当代模型的迷人性能通常是以繁重的计算和漫长的推理时间为代价的，这对于自动驾驶来说是无法忍受的。利用轻量级架构（编码器-解码器或双通道）或在低分辨率图像上进行推理，最近的方法实现了非常快速的场景解析，甚至可以在单个1080Ti GPU上以超过100...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">CA</div></div><div class="info-2"><div class="info-item-1"> Coordinate Attention for Efficient Mobile Network Design  Abstract 最近关于移动网络设计的研究已经证明了通道注意（如SE注意力机制）对于提高模型性能的显著效果，但他们通常忽略了位置信息，而位置信息对于生成空间选择性注意图是很重要的。在本文中，我们提出了一种新的移动网络注意机制，将位置信息嵌入到通道注意中，我们称之为...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#dental-lesion-segmentation-using-an-improved-icnet-network-with-attention"><span class="toc-text"> Dental Lesion Segmentation Using an Improved ICNet Network with Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-introduction"><span class="toc-text"> 1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-related-work"><span class="toc-text"> 2. Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-encoderdecoder-network"><span class="toc-text"> 2.1 Encoder–Decoder Network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-attention-mechanism"><span class="toc-text"> 2.2 Attention Mechanism</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#23-multiple-forms-of-convolution"><span class="toc-text"> 2.3 Multiple Forms of Convolution</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-methods"><span class="toc-text"> 3. Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-adding-the-cbam-attention-module-to-low-and-medium-resolution-branches"><span class="toc-text"> 3.1 Adding the CBAM Attention Module to Low- and Medium-Resolution Branches</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-asymmetric-convolution-replaces-regular-convolution"><span class="toc-text"> 3.2 Asymmetric Convolution Replaces Regular Convolution</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-experiment"><span class="toc-text"> 4. Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-datasets"><span class="toc-text"> 4.1. Datasets</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#411-data-collection"><span class="toc-text"> 4.1.1 Data Collection</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#412-data-augmentation"><span class="toc-text"> 4.1.2. Data Augmentation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-metrics"><span class="toc-text"> 4.2 Metrics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-loss-function"><span class="toc-text"> 4.3. Loss Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#44-experimental-details"><span class="toc-text"> 4.4. Experimental Details</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-results"><span class="toc-text"> 5. Results</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#51-contrast-test-with-other-segmentation-algorithms"><span class="toc-text"> 5.1. Contrast Test with Other Segmentation Algorithms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#52-segmentation-under-different-brightness"><span class="toc-text"> 5.2. Segmentation under Different Brightness</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#53-ablation-experiment-of-cbam"><span class="toc-text"> 5.3. Ablation Experiment of CBAM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-conclusions"><span class="toc-text"> 6. Conclusions</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA">CA</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>