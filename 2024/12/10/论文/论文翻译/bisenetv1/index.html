<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="BiSeNet BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation  Abstract. 语义分割需要丰富的空间信息和相当大的感受野。然而，现代方法通常会牺牲空间分辨率来实现实时推理速度，这导致了性能不佳。在本文中，我们用一个新颖的双边分割网络（BiSeNet）来解决这个难题。我们首先设计了一个小">
<meta property="og:type" content="article">
<meta property="og:title" content="BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/bisenetv1/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="BiSeNet BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation  Abstract. 语义分割需要丰富的空间信息和相当大的感受野。然而，现代方法通常会牺牲空间分辨率来实现实时推理速度，这导致了性能不佳。在本文中，我们用一个新颖的双边分割网络（BiSeNet）来解决这个难题。我们首先设计了一个小">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:04:04.655Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/bisenetv1/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:04:04.655Z" title="更新于 2024-12-11 01:04:04">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">6.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>21分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="bisenet"><a class="markdownIt-Anchor" href="#bisenet"></a> BiSeNet</h1>
<p><a href="zotero://open-pdf/library/items/FUYHF4SI">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a></p>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract.</h2>
<p>语义分割需要丰富的空间信息和相当大的感受野。然而，现代方法通常会牺牲空间分辨率来实现实时推理速度，这导致了性能不佳。在本文中，我们用一个新颖的双边分割网络（BiSeNet）来解决这个难题。我们首先设计了一个小步幅的空间路径，以保留空间信息并产生高分辨率的特征。同时，采用快速下采样策略的Context Path，以获得足够的感受野。在这两条路径的基础上，我们引入了一个新的特征融合模块来有效地结合特征。在Cityscapes、CamVid和COCO-Stuff数据集上，提议的架构在速度和分割性能之间取得了适当的平衡。具体来说，对于2048×1024的输入，我们在Cityscapes测试数据集上实现了68.4%的平均IOU，在一块NVIDIA Titan XP卡上的速度为105FPS，这比现有的具有可比性的方法要快得多。</p>
<h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1 Introduction</h2>
<p>语义分割的研究，相当于给每个像素分配语义标签，是计算机视觉中的一项基本任务。它可以广泛地应用于增强现实设备、自动驾驶和视频监控等领域。这些应用对高效的推理速度有很高的要求，以实现快速交互或响应。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230329193657587.png" alt="image-20230329193657587" /></p>
<blockquote>
<p>**Fig. 1.**加速的架构和我们提出的方法的说明。(a)介绍了对输入图像的裁剪或调整大小的操作，以及带有修剪通道或放弃阶段的轻量级模型。(b)表示U型结构。©展示了我们提出的双侧分割网络（BiSeNet）。黑色破折线代表破坏空间信息的操作，而红色破折线代表缩小感受野的操作。绿色块是我们提出的空间路径（SP）。在网络部分，每个区块代表不同下采样大小的特征图。块的长度代表空间分辨率，而厚度则代表通道的数量。</p>
</blockquote>
<p>最近，实时语义分割的算法[1,17,25,39]表明，主要有三种方法来加速模型。1）[34,39]试图通过裁剪或调整大小来限制输入尺寸以降低计算的复杂性。虽然该方法简单有效，但空间细节的损失破坏了预测，特别是在边界周围，导致指标和可视化的准确性下降。2) 有些工作没有调整输入图像的大小，而是修剪网络的通道以提高推理速度[1, 8, 25]，特别是在基础模型的早期阶段。然而，它削弱了空间容量。3) 对于最后一种情况，ENet[25]提出放弃模型的最后阶段，以追求一个极其紧密的框架。然而，这种方法的缺点是显而易见的：由于ENet放弃了最后阶段的下采样操作，模型的接受域不足以覆盖大型物体，导致辨别能力差。总的来说，上述所有的方法都在准确性上向速度妥协，这在实践中是比较差的。图1（a）给出了说明。</p>
<p>为了弥补上述空间细节的损失，研究人员广泛利用U型结构[1,25,35]。通过融合骨干网络的层次特征，U型结构逐渐提高了空间分辨率，填补了一些缺失的细节。然而，这种技术有两个弱点。1）完整的U型结构会降低模型的速度，因为引入了高分辨率特征图的额外计算。2）更重要的是，如图1（b）所示，在修剪或裁剪中丢失的大部分空间信息不能通过涉及浅层而轻易恢复。换句话说，U型技术最好被看作是一种救济，而不是一种基本的解决方案。</p>
<p>基于上述观察，我们提出了双侧分割网络（BiSeNet），包括两部分： 空间路径（SP）和背景路径（CP）。顾名思义，这两个部分分别是为了应对空间信息的损失和感受野的萎缩而设计的。这两个路径的设计理念是明确的。对于空间路径，我们只堆叠三个卷积层来获得1/8的特征图，它保留了丰富的空间细节。对于Context Path，我们在Xception[8]的尾部附加了一个全局平均池化层，这里的感受野是骨干网络的最大值。图1（c）显示了这两个部分的结构。</p>
<p>为了追求更好的准确性而不损失速度，我们还研究了两条路径的融合和最终预测的细化，并分别提出了特征融合模块（FFM）和注意细化模块（ARM）。正如我们下面的实验所示，这两个额外的组件可以进一步提高Cityscapes[9]、CamVid[2]和COCO-Stuff[3]两个基准的整体语义分割精度。</p>
<p>我们的主要贡献总结如下：</p>
<ul>
<li><strong>我们提出了一种新的方法，将空间信息保存和感受野提供的功能解耦为两条路径。具体来说，我们提出了一个具有空间路径（SP）和背景路径（CP）的双侧分割网络（BiSeNet）。</strong></li>
<li>我们设计了两个特定的模块，即特征融合模块（FFM）和注意精化模块（ARM），以可接受的成本进一步提高准确性。</li>
<li>我们在Cityscapes、CamVid和COCO-Stuff等基准测试中取得了令人印象深刻的结果。更具体地说，我们在Cityscapes测试数据集上取得了68.4%的结果，速度为105 FPS。</li>
</ul>
<h2 id="2-related-work"><a class="markdownIt-Anchor" href="#2-related-work"></a> 2 Related Work</h2>
<p>最近，许多基于FCN的方法[22]在语义分割任务的不同基准上取得了最先进的性能。这些方法中的大多数都是为了编码更多的空间信息或扩大接受领域。</p>
<p><strong>空间信息：</strong> 卷积神经网络（CNN）[16]通过连续的下采样操作对高级语义信息进行编码。然而，在语义分割任务中，图像的空间信息对预测详细输出至关重要。现有的现代方法致力于编码富裕的空间信息。DUC[32]、PSPNet[40]、DeepLab v2[5]和Deeplab v3[6]使用扩张卷积来保留特征图的空间大小。全局卷积网络[26]利用 &quot;大核 &quot;来扩大感受野。</p>
<p><strong>U形方法：</strong> U型结构[1,10,22,24,27]可以恢复一定程度的空间信息。原始的FCN[22]网络通过一个跳过连接的网络结构来编码不同层次的特征。一些方法采用其特定的细化结构，变成U型网络结构。[1, 24]通过使用去卷积层创建了一个U型网络结构。U-net[27]为这个任务引入了有用的跳过连接网络结构。全局卷积网络[26]将U型结构与 &quot;大核 &quot;相结合。LRR[10]采用了拉普拉斯金字塔重构网络。RefineNet[18]增加了多路径细化结构来细化预测。DFN[36]设计了一个通道关注块来实现特征选择。然而，在U型结构中，一些丢失的空间信息不容易恢复。</p>
<p><strong>语境信息：</strong> 语义分割需要语境信息来产生高质量的结果。大多数常见的方法都是放大感受野或融合不同的语境信息。[5, 6, 32, 37]在卷积层中采用不同的扩张率来捕捉不同的背景信息。在图像金字塔的驱动下，语义分割网络结构中总是采用多尺度特征组合。在[5]中，提出了一个 &quot;ASPP &quot;模块来捕捉不同感受野的语境信息。PSPNet[40]应用了一个 &quot;PSP &quot;模块，其中包含几个不同尺度的平均集合层。[6]设计了一个带有全局平均池的 &quot;ASPP &quot;模块来捕捉图像的全局背景。[38]通过一个尺度自适应卷积层改进了神经网络，以获得自适应的场背景信息。DFN[36]在U型结构的顶部增加了全局池，以编码全局背景。</p>
<p><strong>注意机制：</strong> 注意机制可以使用高层信息来指导前馈网络[23，31]。在[7]中，CNN的注意力取决于输入图像的比例。在[13]中，他们将通道注意力应用于识别任务，并达到了最先进的水平。像DFN[36]一样，他们学习全局背景作为注意力并修改特征。</p>
<p>**实时分割：**实时语义分割算法需要一个快速的方法来生成高质量的预测结果。SegNet[1]利用一个小的网络结构和跳过连接的方法来实现快速的速度。E-Net[25]从头开始设计了一个轻量级网络，并提供了极高的速度。ICNet[39]利用图像级联来加速语义分割方法。[17]采用了级联网络结构来减少 &quot;容易区域 &quot;的计算量。[34]设计了一个新颖的双柱网络和空间稀疏性来减少计算成本。不同的是，我们提出的方法采用了一个轻量级模型来提供足够的感受野。此外，我们设置了一个浅而宽的网络来捕捉足够的空间信息。</p>
<h2 id="3-bilateral-segmentation-network"><a class="markdownIt-Anchor" href="#3-bilateral-segmentation-network"></a> 3 Bilateral Segmentation Network</h2>
<p>在这一节中，我们首先详细说明了我们提出的双边分割网络（BiSeNet）的空间路径和背景路径。此外，我们还详细阐述了这两条路径的相应效果。最后，我们展示了如何将这两条路径的特征与特征融合模块和我们BiSeNet的整个架构结合起来。</p>
<h3 id="31-spatial-path"><a class="markdownIt-Anchor" href="#31-spatial-path"></a> 3.1 Spatial path</h3>
<p>在语义分割的任务中，一些现有的方法[5, 6, 32, 40]试图保留输入图像的分辨率，用扩张卷积来编码足够的空间信息，而少数方法[5, 6, 26, 40]试图用金字塔集合模块、阿特拉斯空间金字塔集合或 &quot;大内核 &quot;来捕捉足够的接受域。这些方法表明，空间信息和感受野是实现高精确度的关键。然而，要同时满足这两个要求是很难的。特别是在实时语义分割的情况下，现有的现代方法[1, 25, 39]利用小型输入图像或轻量级基础模型来加速。小尺寸的输入图像失去了原始图像的大部分空间信息，而轻量级的模型在通道修剪时损害了空间信息。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230413153312229.png" alt="image-20230413153312229" /></p>
<blockquote>
<p>图2.双边分割网络的概述。(a) 网络结构。块的长度表示空间大小，而厚度代表通道的数量。(b) 注意力细化模块（ARM）的组成部分。© 特征融合模块（FFM）的组成部分。读取线代表我们只在测试时采取这一过程。</p>
</blockquote>
<p>基于这一观察，我们提出了一个空间路径，以保留原始输入图像的空间大小，并对富裕的空间信息进行编码。空间路径包含三层。每层包括一个stride=2的卷积，然后是BN[15]和ReLU[11]。因此，该路径提取的输出特征图是原始图像的1/8。由于特征图的空间尺寸较大，它编码了丰富的空间信息。图2(a)展示了结构的细节。</p>
<h3 id="32-context-path"><a class="markdownIt-Anchor" href="#32-context-path"></a> 3.2 Context path</h3>
<p>**空间路径编码丰富的空间信息，而Context Path则是为了提供足够的感受野。**在语义分割任务中，接受域对性能具有重要意义。为了扩大感受野，一些方法利用了金字塔集合模块[40]、无轨空间金字塔集合[5,6]或 “大核”[26]。然而，这些操作对计算的要求很高，也很耗费内存，导致速度很低。</p>
<p>考虑到大接收场和高效计算的同时，我们提出了 “Context Path”。**Context Path利用轻量级模型和全局平均池[5, 6, 21]来提供大的感受野。**在这项工作中，轻量级模型，如Xception[8]，可以快速对特征图进行降样，以获得大的感受野，它编码了高水平的语义背景信息。然后，我们在轻量级模型的尾部添加一个全局平均池，它可以提供具有全局语境信息的最大感受野。最后，我们将全局池的上采样输出特征和轻量级模型的特征结合起来。在轻量级模型中，我们部署了U型结构[1, 25, 35]来融合后两个阶段的特征，这是一个不完整的U型结构风格。图2（c）显示了Context Path的整体视角。</p>
<p><strong>注意力细化模块：</strong> 在语境路径中，我们提出了一个特定的注意力细化模块（ARM）来细化每个阶段的特征。如图2(b)所示，ARM采用全局平均池来捕捉全局背景，并计算出一个注意力向量来指导特征学习。这种设计可以细化Context Path中每个阶段的输出特征。它可以很容易地整合全局上下文信息，而不需要任何向上采样的操作。因此，它要求的计算成本可以忽略不计。</p>
<h3 id="33-network-architecture"><a class="markdownIt-Anchor" href="#33-network-architecture"></a> 3.3 Network architecture</h3>
<p>通过空间路径和Context Path，我们提出了BiSeNet，用于实时语义分割，如图2（a）所示。</p>
<p><strong>我们使用预先训练好的Xception模型作为 &quot;语境路径 &quot;的骨干，使用三个卷积层作为 “空间路径”</strong>。<strong>然后我们融合这两条路径的输出特征</strong>，进行最终的预测。它可以同时实现实时性能和高精确度。首先，我们把重点放在实际计算方面。虽然空间路径有很大的空间尺寸，但它只有三个卷积层。因此，它的计算量不大。至于 “Context Path”，我们使用一个轻量级的模型来快速下采样。此外，这两条路径同时进行计算，这大大增加了效率。第二，我们讨论这个网络的准确性方面。在我们的论文中，空间路径编码了丰富的空间信息，而Context Path提供了大的感受野。它们相互补充，以获得更高的性能。</p>
<p><strong>特征融合模块：</strong> 两条路径的特征在特征表示水平上是不同的。因此，我们不能简单地将这些特征相加。空间路径捕捉到的空间信息主要编码了丰富的细节信息。此外，&quot;语境路径 &quot;的输出特征主要是编码语境信息。换句话说，空间路径的输出特征是低层次的，而语境路径的输出特征是高层次的。因此，我们提出一个特定的特征融合模块来融合这些特征。</p>
<p>考虑到特征的不同层次，我们首先将空间路径和Context Path的输出特征连接起来。然后，我们利用批量归一化[15]来平衡这些特征的尺度。接下来，我们将串联的特征汇集成一个特征向量，并计算出一个权重向量，如SENet[13]。这个权重向量可以对特征进行重新加权，这相当于特征选择和组合。图2©显示了这种设计的细节。</p>
<p><strong>损失函数：</strong> 在本文中，我们还利用了辅助损失函数来监督我们提出的方法的训练。我们使用主损失函数来监督整个BiSeNet的输出。此外，我们增加了两个特定的辅助损失函数来监督Context Path的输出，就像深度监督一样[35]。所有的损失函数都是Softmax损失，如公式1所示。此外，我们使用参数α来平衡主损失和辅助损失的权重，如公式2所示。本文中的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>等于1。联合损失使优化器在优化模型时更加舒适。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munder><mo>∑</mo><mi>i</mi></munder><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo fence="true">(</mo><mfrac><msup><mi>e</mi><msub><mi>p</mi><mi>i</mi></msub></msup><mrow><munder><mo>∑</mo><mi>j</mi></munder><msup><mi>e</mi><msub><mi>p</mi><mi>j</mi></msub></msup></mrow></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">loss=\frac1N\sum_iL_i=\frac1N\sum_i-log\left(\frac{e^{p_i}}{\sum_je^{p_j}}\right)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.599109em;vertical-align:-1.277669em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord">1</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.027669em;vertical-align:-1.277669em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord">1</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.341392em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6064620000000001em;"><span style="top:-3.0050700000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1218180000000002em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span></span></span></span></span></p>
<p>其中p是网络的输出预测值，</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">;</mo><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>l</mi><mi>p</mi></msub><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">;</mo><mi>W</mi><mo stretchy="false">)</mo><mo>+</mo><mi>α</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>K</mi></munderover><msub><mi>l</mi><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo separator="true">;</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(X;W)=l_p(X;W)+\alpha\sum_{i=2}^Kl_i(X_i;W)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:3.106005em;vertical-align:-1.277669em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">2</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span></span></span></span></span></p>
<p>其中lp是串联输出的主损失。Xi是Xception模型第i阶段的输出特征。li是第i阶段的辅助损失。K在我们的论文中等于3。L是联合损失函数。这里，我们只在训练阶段使用辅助损失。</p>
<h2 id="4-experimental-results"><a class="markdownIt-Anchor" href="#4-experimental-results"></a> 4 Experimental Results</h2>
<p>我们在实时语义分割任务中采用了改进的Xception模型[8]，即Xception39。我们的实现代码将公开提供。</p>
<p>我们在Cityscapes[9]、CamVid[2]和COCOStuff[3]基准上评估了我们提出的BiSeNet。我们首先介绍了数据集和实施协议。接下来，我们详细描述了我们与其他方法相比的速度策略。然后，我们研究了我们提出的方法的每个组成部分的效果。我们在Cityscapes验证集上评估了所有的性能结果。最后，我们报告了在Cityscapes、CamVid和COCO-Stuff数据集上与其他实时语义分割算法相比的准确性和速度结果。</p>
<p>城市景观： Cityscapes[9]是一个来自汽车视角的大型城市街道场景数据集。它包含2,975张用于训练的精细注释的图像和另外500张用于验证的图像。在我们的实验中，我们只使用精细数据集。对于测试，它提供了1,525张没有地面真相的图像，以便进行公平比较。这些图像的分辨率都是2,048×1,024，其中每个像素都被注释为预先定义的19类。</p>
<p>COCO-Stuff： COCO-Stuff[3]增加了流行的COCO[20]数据集的全部164000张图片，其中118000张图片用于训练，5000张图片用于验证，20000张图片用于测试-开发，20000张图片用于测试-挑战。它涵盖了91个东西类和1个 &quot;未标记 &quot;类。</p>
<h3 id="41-implementation-protocol"><a class="markdownIt-Anchor" href="#41-implementation-protocol"></a> 4.1 Implementation protocol</h3>
<p>在本节中，我们将详细阐述我们的实施协议。网络： 我们应用三个卷积作为空间路径和<strong>Xception39模型作为背景路径</strong>。然后我们使用特征融合模块来结合这两条路径的特征来预测最终结果。空间路径的输出分辨率和最终的预测结果是原始图像的1/8。</p>
<p>训练细节： 我们在训练中使用迷你批次随机梯度下降法（SGD）[16]，批次大小为16，动量0.9，权重衰减1e-4。与[5, 6, 21]类似，我们采用 &quot;聚 &quot;学习率策略，其中初始学习率乘以(1 - iter max iter )幂，每次迭代的幂为0.9。最初的学习率是2.5e-2。</p>
<p>数据增量： 在训练过程中，我们对输入图像采用平均减法、随机水平翻转和随机比例来增加数据集。标度包含{0.75, 1.0, 1.5, 1.75, 2.0}。最后，我们随机地将图像裁剪成固定的尺寸，用于训练。</p>
<h3 id="42-ablation-study"><a class="markdownIt-Anchor" href="#42-ablation-study"></a> 4.2 Ablation study</h3>
<p>在本小节中，我们一步一步地详细研究了我们提出的BiSeNet中每个组件的效果。在接下来的实验中，我们使用Xception39作为基础网络，在Cityscapes验证数据集[9]上评估我们的方法。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230412160452141.png" alt="image-20230412160452141" /></p>
<p>基准线： 我们使用在ImageNet数据集[28]上预训练的Xception39网络作为Context Path的主干。然后我们像FCN[22]一样，直接对网络的输出进行上采样，作为原始输入图像。我们将基础模型的性能作为基线来评估，如表1所示。</p>
<p>对U型的消融： 我们提出了 “Context Path”，以提供足够的感受野。我们使用一个轻量级的模型，Xception39，作为 &quot;Context Path &quot;的骨干，以快速取样。同时，我们使用U型结构[1, 25, 35]来结合Xception39网络中最后两个阶段的特征，称为U型8s，而不是标准的U型结构，称为U型4s。这个数字代表了输出特征的下采样系数，如图2所示。使用U-shape-8s结构的原因有两个方面。首先，U型结构可以恢复一定程度的空间信息和空间大小。第二，与U形-4s相比，U形-8s结构的速度更快，如表2所示。因此，我们使用U-形-8s结构，它的性能从60.79%提高到66.01%，如表2所示。</p>
<p>空间路径的消融： 如第一节所述，现有的实时语义分割任务的现代方法面临着失去空间信息的挑战。因此，我们提出了一个空间路径，以保留空间大小并捕获丰富的空间信息。空间路径包含三个跨度=2的卷积，然后是批量归一化[15]和ReLU[11]。这使性能从66.01%提高到67.42%，如表3所示。空间路径编码了丰富的空间信息细节。图3显示，BiSeNet可以获得更详细的空间信息，例如一些交通标志。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230412160521923.png" alt="image-20230412160521923" /></p>
<p><img src="/img/loading.gif" data-original="images/image-20230412160532823.png" alt="image-20230412160532823" /></p>
<p>注意力细化模块的消融： 为了进一步提高性能，我们专门设计了一个注意力细化模块（ARM）。该模块包含一个全局平均池，将输出特征编码为一个向量。然后我们利用卷积、批量归一化[15]和ReLU单元[11]来计算注意力向量。原始特征将被注意力向量重新加权。对于原始特征，不需要复杂的上采样操作，就可以很容易地捕捉到全局的环境信息。ARM的效果如表3所示。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230412160550434.png" alt="image-20230412160550434" /></p>
<p><strong>特征融合模块的消融：</strong> 基于空间路径和背景路径，我们需要融合这两条路径的输出特征。考虑到不同层次的特征，低层次的是空间路径的特征，高层次的是Context Path的特征，我们提出了特征融合模块来有效结合这些特征。首先，我们评估了这些特征与我们提出的特征融合模块直接相加的效果，如表3所示。比较性能的差距说明这两条路径的特征依次属于不同的层次。</p>
<p><strong>全局平均汇集的消融：</strong> 我们期望Context Path能提供足够的感受野。虽然原始的Xception39模型在理论上可以覆盖输入图像的大部分区域，但我们还是通过全局平均池化[21]来进一步扩大感受野。这可以确保有效的感受区足够大。在本文中，我们在Xception39模型的尾部加入了全局平均池。然后，我们对全局平均池的输出进行上采样，并将此特征与Xception39模型中最后一个阶段的输出相加，如DFN[36]。这样就把性能从67.42%提高到了68.42%，这表明了这种设计的效果，如表3所示。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230412160611742.png" alt="image-20230412160611742" /></p>
<h3 id="43-speed-and-accuracy-analysis"><a class="markdownIt-Anchor" href="#43-speed-and-accuracy-analysis"></a> 4.3 Speed and Accuracy Analysis</h3>
<p>在本节中，我们首先分析了我们算法的速度。然后，我们报告了我们在Cityscapes[9]、CamVid[2]和COCO-Stuff[3]基准上与其他算法相比的最终结果。</p>
<p>速度分析： 速度是一个算法的重要因素，特别是当我们在实践中应用它时。我们在不同的设置上进行实验以进行彻底的比较。首先，我们在表4中显示了FLOPS和参数的情况。FLOPS和参数表示处理该分辨率的图像所需的操作数。为了公平比较，我们选择640×360作为输入图像的分辨率。同时，表5列出了我们的方法与其他方法在不同分辨率的输入图像和不同硬件基准上的速度比较。最后，我们报告了我们在Cityscapes测试数据集上的速度和相应的准确性结果。从表6中，我们可以发现我们的方法在速度和准确率方面都比其他方法取得了明显的进步。在评估过程中，我们首先将2048×1024分辨率的输入图像缩放为1536×768的分辨率，以测试其速度和准确度。同时，我们用在线引导策略计算损失函数，如[33]中所述。在这个过程中，我们没有采用任何测试技术，如多尺度或多作物测试。</p>
<p>准确度分析： 实际上，与其他非实时语义分割算法相比，我们的BiSeNet也可以达到更高的准确率。这里，我们将展示在Cityscapes[9]、CamVid[2]和COCO-Stuff[3]基准上的准确性结果。同时，为了确保我们方法的有效性，我们还在不同的基础模型上采用了该方法，如标准的ResNet18和ResNet101[12]。接下来，我们将详细介绍一些训练细节。<img src="/img/loading.gif" data-original="images/image-20230412160642856.png" alt="image-20230412160642856" /></p>
<p><img src="/img/loading.gif" data-original="images/image-20230412160652513.png" alt="image-20230412160652513" /></p>
<p>城市景观： 如表7所示，我们的方法在不同模型上也取得了令人印象深刻的结果。为了提高准确性，我们随机抽取1024×1024的作物作为输入。图4展示了我们结果的一些视觉例子。</p>
<p>CamVid： 表8显示了CamVid数据集的统计精度结果。对于测试，我们使用训练数据集和验证数据集来训练我们的模型。这里，我们使用960×720的分辨率进行训练和评估。</p>
<p>COCO-Stuff： 我们还在表9中报告了我们在COCO-Stuff验证数据集上的准确性结果。在训练和验证过程中，我们将输入的数据裁剪成640×640的分辨率。为了进行公平的比较，我们没有采用多尺度测试。<img src="/img/loading.gif" data-original="images/image-20230412160720872.png" alt="image-20230412160720872" /></p>
<h2 id="5-结论"><a class="markdownIt-Anchor" href="#5-结论"></a> 5 结论</h2>
<p>本文提出了双侧分割网络（BiSeNet），以同时提高实时语义分割的速度和准确性。我们提出的BiSeNet包含两条路径： 空间路径（SP）和语境路径（CP）。空间路径的设计是为了保留原始图像的空间信息。Context Path利用轻量级模型和全局平均池[6, 21, 40]来迅速获得可观的感受野。有了丰富的空间细节和大的接受域，我们在105FPS下对Cityscapes[9]测试数据集取得了68.4%的平均IOU的结果。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Transformers-in-Medical-Image-Analysis-A-Review/" title="Transformers in Medical Image Analysis A Review"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Transformers in Medical Image Analysis A Review</div></div><div class="info-2"><div class="info-item-1"> Transformers in Medical Image Analysis: A Review  Abstract Transformer 在自然语言处理领域占据了主导地位，最近又对计算机视觉领域产生了影响。在医学图像分析领域，Transformer 也已成功应用于全栈临床应用，包括图像合成/重建、配准、分割、检测和诊断。我们的论文旨在促进医学图像分析领域对 Transformer 的认识和应用。具体来说，我们首先概述了内置于 Transformer 和其他基本组件中的注意力机制的核心概念。其次，我们回顾了为医学影像应用量身定制的各种 Transformer 架构，并讨论了它们的局限性。在这篇综述中，我们围绕 Transformer 在不同学习范式中的使用、提高模型效率以及与其他技术的耦合等方面的关键挑战进行了研究。我们希望这篇综述能让医学图像分析领域的读者对 Transformer 有一个全面的了解。  1 INTRODUCTION Transformer [1] 在自然语言处理（NLP）领域占据主导地位，包括语音识别 [2]、合成 [3]、文本到语音的翻译 [4]...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/TopFormer/" title="TopFormer Token Pyramid Transformer for Mobile Semantic Segmentation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">TopFormer Token Pyramid Transformer for Mobile Semantic Segmentation</div></div><div class="info-2"><div class="info-item-1"> TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation  文章链接   Abstract 尽管vision transformers（ViTs）在计算机视觉领域取得了巨大的成功，但沉重的计算成本阻碍了它们在密集预测任务中的应用，如移动设备上的语义分割。在本文中，我们提出了一个名为Token Pyramid Vision Transformer（TopFormer）的移动友好架构。所提出的TopFormer将不同尺度的Token作为输入，以产生尺度感知的语义特征，然后将其注入到相应的Token中以增强表示。实验结果表明，我们的方法在几个语义分割数据集上明显优于基于CNN和ViT的网络，并且在准确性和延迟之间实现了良好的权衡。在ADE20K数据集上，TopFormer在基于ARM的移动设备上以较低的延迟实现了比MobileNetV3高5%的mIoU精度。此外，TopFormer的微小版本在基于ARM的移动设备上实现了具有竞争力的实时推理结果。  1. Introduction vision...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BAM</div></div><div class="info-2"><div class="info-item-1"> BAM  Abstract 近来，小样本分割分割技术（FSS）得到了广泛的发展。然而，训练出来的模型偏向于所看到的类别，而不是理想的类别无关性，从而阻碍了对新内容的识别。本文提出了一种新颖而直接的见解来缓解这一问题。具体来说，我们在传统的 FSS 模型（元学习器）上增加了一个分支（基础学习器），以明确识别基础类别的目标，即不需要分割的区域。然后，对这两个学习器并行输出的粗略结果进行自适应整合，从而得出精确的分割预测结果。考虑到元学习器的敏感性，我们进一步引入了一个调整因子来估计输入图像对之间的场景差异，以促进模型的集合预测。在 PASCAL-5i 和 COCO-20i 上取得的显著性能提升验证了这一方法的有效性，而且令人惊讶的是，即使使用两个普通学习器，我们的多功能方案也创造了新的一流水平。此外，鉴于所提方法的独特性，我们还将其扩展到了更现实但更具挑战性的环境中，即广义 FSS，在这种环境中，基础类和新类别的像素都需要确定。  1. Introduction   得益于成熟的大规模数据集 [8, 9,...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#bisenet"><span class="toc-text"> BiSeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract.</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-introduction"><span class="toc-text"> 1 Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-related-work"><span class="toc-text"> 2 Related Work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-bilateral-segmentation-network"><span class="toc-text"> 3 Bilateral Segmentation Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-spatial-path"><span class="toc-text"> 3.1 Spatial path</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-context-path"><span class="toc-text"> 3.2 Context path</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-network-architecture"><span class="toc-text"> 3.3 Network architecture</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-experimental-results"><span class="toc-text"> 4 Experimental Results</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-implementation-protocol"><span class="toc-text"> 4.1 Implementation protocol</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-ablation-study"><span class="toc-text"> 4.2 Ablation study</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-speed-and-accuracy-analysis"><span class="toc-text"> 4.3 Speed and Accuracy Analysis</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E7%BB%93%E8%AE%BA"><span class="toc-text"> 5 结论</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/11/%E5%B0%8F%E7%B1%B3/syzkaller/syzkaller01-%E9%85%8D%E7%BD%AE/" title="syzkaller01-配置">syzkaller01-配置</a><time datetime="2024-12-10T17:10:45.000Z" title="发表于 2024-12-11 01:10:45">2024-12-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM">BAM</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>