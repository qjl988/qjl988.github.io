<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>BAM | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="BAM  Abstract 近来，小样本分割分割技术（FSS）得到了广泛的发展。然而，训练出来的模型偏向于所看到的类别，而不是理想的类别无关性，从而阻碍了对新内容的识别。本文提出了一种新颖而直接的见解来缓解这一问题。具体来说，我们在传统的 FSS 模型（元学习器）上增加了一个分支（基础学习器），以明确识别基础类别的目标，即不需要分割的区域。然后，对这两个学习器并行输出的粗略结果进行自适应整合，从">
<meta property="og:type" content="article">
<meta property="og:title" content="BAM">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="BAM  Abstract 近来，小样本分割分割技术（FSS）得到了广泛的发展。然而，训练出来的模型偏向于所看到的类别，而不是理想的类别无关性，从而阻碍了对新内容的识别。本文提出了一种新颖而直接的见解来缓解这一问题。具体来说，我们在传统的 FSS 模型（元学习器）上增加了一个分支（基础学习器），以明确识别基础类别的目标，即不需要分割的区域。然后，对这两个学习器并行输出的粗略结果进行自适应整合，从">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:04:06.475Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'BAM',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">BAM</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">BAM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:04:06.475Z" title="更新于 2024-12-11 01:04:06">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">7.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>23分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="bam"><a class="markdownIt-Anchor" href="#bam"></a> BAM</h1>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>近来，小样本分割分割技术（FSS）得到了广泛的发展。然而，训练出来的模型偏向于所看到的类别，而不是理想的类别无关性，从而阻碍了对新内容的识别。本文提出了一种新颖而直接的见解来缓解这一问题。具体来说，我们在传统的 FSS 模型（元学习器）上增加了一个分支（基础学习器），以明确识别基础类别的目标，即不需要分割的区域。然后，对这两个学习器并行输出的粗略结果进行自适应整合，从而得出精确的分割预测结果。考虑到元学习器的敏感性，我们进一步引入了一个调整因子来估计输入图像对之间的场景差异，以促进模型的集合预测。在 PASCAL-5i 和 COCO-20i 上取得的显著性能提升验证了这一方法的有效性，而且令人惊讶的是，即使使用两个普通学习器，我们的多功能方案也创造了新的一流水平。此外，鉴于所提方法的独特性，我们还将其扩展到了更现实但更具挑战性的环境中，即广义 FSS，在这种环境中，基础类和新类别的像素都需要确定。</p>
<h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1. Introduction</h2>
<ul>
<li>
<p>得益于成熟的大规模数据集 [8, 9, 29]，大量基于卷积神经网络（CNN）的计算机视觉技术在过去几年中得到了快速发展 [15-17, 27, 28, 35, 4345, 48]。然而，收集足够的标注数据耗时耗力是众所周知的，尤其是对于密集预测任务，如实例分割 [2,3,15,21,59] 和语义分割 [1,25,35,40,45]。与机器学习范式形成鲜明对比的是，人类可以很容易地从少量实例中识别出新的内容或模式，这极大地激发了社区的研究兴趣[39,52,53]。因此，为了解决这一问题，有人提出了少量学习（FSL），即建立一个网络，将其泛化到缺少注释样本的未知领域 [7,42,54,57]。</p>
<p>在本文中，我们将在语义分割领域应用 FSL，即小样本分割（FSS），模型仅利用极少量的标注训练数据从原始图像中分割出特定语义类别的目标[46]。在寥寥几个分类成功的推动下，大多数现有的 FSS 方法都在努力通过元学习框架实现泛化[23, 30-34, 36-38, 47, 55, 56, 58, 61, 62, 64-67]。从基础数据集中抽取一系列学习任务（情节），以模拟新类别的少量场景，即匹配训练和测试条件。然而，这种方法远远不够，而且能力不足。在具有丰富注释样本的基础数据集上进行元训练，不可避免地会偏向所见的类，而不是理想的类无关性，从而阻碍了对新概念的识别[10]。值得注意的是，在对抗与基础数据具有相似类别的困难查询样本时，泛化性能可能会濒临崩溃。</p>
<p><img src="/img/loading.gif" data-original="images/image-20231202231630936.png" alt="image-20231202231630936" /></p>
<blockquote>
<p>图 1. 我们的 BAM 与之前工作的比较。(a) 传统方法通常采用元学习框架来训练 FSS 模型，这不可避免地会偏向于基类，而不是理想的类无关性，从而阻碍了对新类别目标对象（如猫（-））的识别。(b) 我们的 BAM 引入了一个额外的分支，即基类学习器，以明确预测基类区域。这样，查询图像中的干扰对象（如人（-）和沙发（-））就能在集合模块之后被显著抑制。© 我们的 BAM 在广义 FSS 设置下的扩展，其中基础类和新类别的像素都需要确定。细化后的结果与基础学习器的输出再次合并，生成综合预测结果。</p>
</blockquote>
<p>我们认为，除了设计更强大的特征提取模块[23, 60, 61]，调整使用包含足够训练样本的基础数据集也是缓解上述偏差问题的另一种方法，而这一问题在以往的研究中一直被忽视。为此，我们在传统的 FSS 模型（元学习器）中引入了一个额外的分支（基础学习器），以明确预测基础类的目标（见图 1）。然后，对这两个学习器并行输出的粗略结果进行自适应整合，以生成准确的预测结果。这种操作背后的核心理念是通过在传统范例中训练的高容量分割模型来识别查询图像中的可混淆区域，从而进一步促进对新物体的识别。顺便提一下，所提出的方案被命名为 BAM，因为它由两个独特的学习器组成，即基础学习器和元学习器。</p>
<p>此外，我们注意到元学习器通常对支持图像的质量很敏感，输入图像对之间的巨大差异会导致性能严重下降。相反，基础学习器往往能提供高度可靠的分割结果，并且由于输入的是单一查询图像，因此性能稳定。基于这一观察结果，我们进一步建议利用查询-支持图像对之间的场景差异评估结果来调整元学习器得出的粗略预测结果。受在图像风格转换领域被广泛采用的风格损失（style loss）[12, 13, 20]的启发，我们首先计算两幅输入图像的格兰矩阵差，然后利用弗罗贝尼斯规范（Frobenius norm）获得整体指标，用于指导调整过程。如图 1(b)所示，经过组合模块后，查询图像中的基类（如人和沙发）干扰物被明显抑制，实现了对新物体（如猫）的准确定位。此外，鉴于所提方法的独特性，我们还将当前任务扩展到了一个更现实但更具挑战性的环境（即广义 FSS）中，要求同时确定基础类和新类别的像素，如图 1©所示。总之，我们的主要贡献可以总结如下：</p>
</li>
<li>
<p>我们提出了一个简单而高效的方案，通过引入一个额外的分支来明确预测查询图像中的基类区域，从而解决偏差问题，这为未来的工作提供了启示。</p>
</li>
<li>
<p>我们建议通过格拉姆矩阵来估计查询-支持图像对之间的场景差异，以减轻元学习器的灵敏度带来的不利影响。</p>
</li>
<li>
<p>我们的多用途计划为所有环境下的家庭支助服务设定了新的艺术基准，即使是两名普通学员也不例外。</p>
</li>
<li>
<p>我们将提出的方法扩展到更具挑战性的环境中，即广义 FSS，它可以同时识别基础类和新类别的目标。</p>
</li>
</ul>
<h2 id="2-related-works"><a class="markdownIt-Anchor" href="#2-related-works"></a> 2. Related Works</h2>
<p><strong>语义分割：</strong> 语义分割是一项基本的计算机视觉任务，旨在根据一组预定义的语义类别识别给定图像中的每个像素[45]。最近，得益于全卷积网络（FCN）[35] 的优势，这一领域取得了巨大进步。人们相继提出了各种稳健的网络设计，同时也带来了一些基本技术，如扩张卷积[63]、编码器-解码器结构[45]、多级特征聚合[26]、注意力机制[18]等。然而，传统的分割模型需要足够多的注释样本才能产生令人满意的结果，而且在没有微调的情况下很难泛化到未见过的类别，因此在一定程度上阻碍了其实际应用。在本研究中，元学习器中引入了基于扩张卷积的空间金字塔池化（ASPP）模块[4]，以扩大接受区，并以 PSPNet[68] 作为基础学习器，预测基础类别中的分心对象。</p>
<p>**小样本学习：**多年来，计算机视觉领域一直在努力开发一种能够泛化到新类别的网络。目前，小样本学习（FSL）领域的大多数方法都遵循 [54] 中提出的元学习框架，即从基础数据集中抽取一组学习任务（集）来模拟小样本场景。在此基础上，FSL 方法可进一步细分为三个分支：(i) 基于度量的 [24, 49, 50]；(ii) 基于优化的 [11, 19, 42]；(iii) 基于增强的 [5, 6]。我们的工作与基于度量的方法密切相关，这种方法通过特定的距离度量（如欧氏距离和余弦距离）来确定支持原型 [49] 与查询特征之间的亲和性。受 FSL [22] 中广义设置的启发，我们试图通过预测查询图像中的基类区域来帮助识别新目标，低数据机制下的分割任务也扩展到了这一设置中。</p>
<p>**小样本分割：**小样本分割（FSS）是 FSL 技术在密集预测任务中的自然应用，近年来受到越来越多的关注。以往的方法通常采用双分支结构（即支持分支和查询分支）来传输注释信息并在提取的特征之间进行交互。Shaban 等人[46]提出了这一领域的开创性工作–OSLSM，其中支持（条件）分支被用来生成分类器权重，用于查询分支预测。后来，Zhang 等人[67] 利用掩蔽平均池操作获得了具有代表性的支持特征，这也是后续工作的基础技术。最近，一些相关研究放弃了重骨干网络的训练过程，转而在固定骨干网络上构建功能强大的块来提高性能，如 CANet [66]、PFENet [51]、ASGNet [23]、SAGNN [60] 和 MM-Net [58]。</p>
<p>然而，这些方法的泛化性能在很大程度上取决于元学习框架，即使经过微调过程也可能很脆弱。更具体地说，由于不平衡的数据分布和较大的领域偏移，训练好的 FSS 模型会偏向于基类。我们注意到，该领域很少有作品明确研究泛化退化问题，而是专注于设计两个分支之间的高容量交互模块。Tian 等人[51]利用从固定骨干网络中提取的高级特征来评估相似性，为查询图像提供了重要的分割线索。这种无参数方法可以帮助网络学习捕捉更多通用模式，从而提高泛化能力。本文则从更根本的角度出发，通过明确识别基类的可混淆区域来解决偏差问题。</p>
<h2 id="3-problem-definition"><a class="markdownIt-Anchor" href="#3-problem-definition"></a> 3. Problem Definition</h2>
<p>小样本分割的目的是仅使用少量标记数据进行分割。目前的方法通常是在元学习范式（也称为情节训练）中训练模型。具体来说，给定两个在对象类别上互不关联的图像集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{train}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{test}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，模型有望在有足够标注样本的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{train}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 上学习到可迁移的知识，从而在标注样本较少的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{test}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 上表现出良好的泛化能力。具体来说，这两个集合都由许多情节组成，每个情节都包含一个小的支持集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><msubsup><mi>x</mi><mi>i</mi><mi>S</mi></msubsup><mo separator="true">,</mo><msubsup><mi>m</mi><mi>i</mi><mi>S</mi></msubsup><mo stretchy="false">)</mo><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">S=\{(x_{i}^{S},m_{i}^{S})\}_{i=1}^K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0999949999999998em;vertical-align:-0.258664em;"></span><span class="mopen">{</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span></span></span></span>和一个查询集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><msup><mi>x</mi><mi>q</mi></msup><mo separator="true">,</mo><msup><mi>m</mi><mi>q</mi></msup><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">Q = \{(x^q, m^q)\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">}</span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mtext>∗</mtext></msup></mrow><annotation encoding="application/x-tex">x^∗</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.688696em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>m</mi><mtext>∗</mtext></msup></mrow><annotation encoding="application/x-tex">m^∗</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.688696em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span> 分别代表原始图像及其对应的特定类别<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">c</span></span></span></span> 的二进制掩码。在每次训练过程中都会对模型进行优化，以便在支持集$ S$ 的条件下对查询图像 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>q</mi></msup></mrow><annotation encoding="application/x-tex">x^q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span></span></span></span></span></span></span></span> 做出预测。训练完成后，我们将评估它们在所有测试集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{test}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 上的少帧分割性能，而无需进一步优化。</p>
<h2 id="4-proposed-method"><a class="markdownIt-Anchor" href="#4-proposed-method"></a> 4. Proposed Method</h2>
<p><img src="/img/loading.gif" data-original="images/image-20231203004741938.png" alt="image-20231203004741938" /></p>
<blockquote>
<p>图 2. 拟议 BAM 的整体架构，由三个基本组件组成：基础学习器、元学习器和集合模块。在每次训练中，两个学习器使用共享编码器提取输入图像对（xs，xq）的特征，并分别对特定基础类别 c（注意，c 表示元测试阶段的新类别）和其余基础类别进行预测。然后，粗略预测结果连同调整因子ψ一起被送入集合模块，以抑制基础类别的错误激活区域，从而进一步产生精确的分割结果。为便于理解，我们以分割掩码的形式呈现概率图，但它们实际上是二维浮点矩阵，即 p∈[0, 1]H×W 。MAP 表示屏蔽平均池化操作 [67]。</p>
</blockquote>
<p>为了缓解当前 FSS 方法的偏差问题，我们建议建立一个额外的网络，明确预测查询图像中的基础类别区域，从而促进新物体的分割。在不失一般性的前提下，我们展示了单镜头设置下模型的整体架构（见图 2）。拟议的 BAM 由三个主要部分组成，包括两个互补学习器（即基础学习器和元学习器）和一个集合模块。两个具有共享骨干的学习器分别用于识别基础类和新类别。然后，集合模块接收它们的粗略预测和调整因子ψ，以抑制基础类的错误激活区域，从而进一步产生精确的分割。此外，我们还建议根据ψ来学习 K-shot 设置下不同支持图像的融合权重，旨在为查询分支提供更好的指导。</p>
<h3 id="41-base-learner"><a class="markdownIt-Anchor" href="#41-base-learner"></a> 4.1. Base Learner</h3>
<p>如第 2 章所述，当前的 FSS 模型偏重于已见类别，这阻碍了对新内容的识别。基于这一观点，我们建议引入一个额外的分支，即基类学习器，以明确预测查询图像中的基类区域。具体来说，给定查询图像 xq∈R3×H×W 时，我们首先应用编码器网络 E 和卷积块提取其中间特征图 f q b，可表述为</p>
<p><img src="/img/loading.gif" data-original="images/image-20231130144732222.png" alt="image-20231130144732222" /></p>
<p><img src="/img/loading.gif" data-original="images/image-20231130144756290.png" alt="image-20231130144756290" /></p>
<p>其中，Fconv 表示连续卷积运算*。c、h 和 w 分别为信道维度、高度和宽度，h × w 表示所有提取的特征图中的最小分辨率。</p>
<p>然后，解码器网络 Db 逐步放大中间特征图 f q b 的空间尺度，最终得到预测结果，其定义如下</p>
<p><img src="/img/loading.gif" data-original="images/image-20231130144818004.png" alt="image-20231130144818004" /></p>
<p>其中，nbs 是每批训练样本的数量。</p>
<p><strong>为什么不联合训练两个学习器？</strong> 预测查询图像中基础类别区域的一种自然方法是遵循标准语义分割网络，如 PSPNet [68]、DeepLab [4]等。然而，在原有的几帧模型基础上额外构建如此庞大的网络是不现实的，这会引入过多的参数，降低推理速度。因此，我们尝试设计一个统一的框架，让两个学习者共享同一个骨干网络。然而，我们注意到，先进的 FSS 方法 [23, 51, 66] 通常会在训练过程中冻结主干网络，以增强泛化效果。这种操作与标准分割模型的学习方案不一致，无疑会影响基础学习器的性能。更重要的是，基础学习器能否在偶发学习范式下得到很好的训练还是个未知数，因此我们最终采用了两阶段训练策略。在第 5.3 节中，我们将讨论不同训练方法和网络设计对分割准确性的影响。</p>
<h3 id="42-meta-learner"><a class="markdownIt-Anchor" href="#42-meta-learner"></a> 4.2. Meta Learner</h3>
<p>给定支持集 S = {xs、ms} 和查询图像 xq，元学习器的目标是在 S 的指导下，分割 xq 中与注释掩码 ms 具有相同类别的对象。然后，应用 1×1 卷积来降低通道维度并生成中间特征图：</p>
<p><img src="/img/loading.gif" data-original="images/image-20231130144855301.png" alt="image-20231130144855301" /></p>
<p>其中，E 是基础学习器和元学习器共享的编码器网络，F1×1 表示将输入特征编码为 256 维的 1×1 卷积。此外，我们通过掩码平均池化（MAP）[67]计算原型（f s m, ms），以提供关键的类别相关线索：</p>
<p><img src="/img/loading.gif" data-original="images/image-20231130144910079.png" alt="image-20231130144910079" /></p>
<p>其中，Fpool 是平均池化运算，代表哈达玛乘积，I 是一个函数，通过插值和扩展技术将 ms 重塑为与 f s m 相同的形状，使 I : RH×W → Rc×h×w。之后，在 vs 的指导下激活 f q m 中的目标区域，并通过解码器网络生成最终预测结果，可概括为：<img src="/img/loading.gif" data-original="images/image-20231130144928622.png" alt="image-20231130144928622" /></p>
<p>其中 Dm 表示元学习器的解码器网络。Fguidance 是 FSS 的一个重要模块，它将注释信息从支持分支传递到查询分支，以提供特定的分割线索。在我们的工作中，它代表了 &quot;扩展与连接 &quot;操作 [66]。同样，我们计算 pm 和 mq 之间的 BCE 损失，以更新元学习器的所有参数：</p>
<p><img src="/img/loading.gif" data-original="images/image-20231201002116222.png" alt="image-20231201002116222" /></p>
<p>其中，ne 表示每个批次的训练集数。</p>
<h3 id="43-ensemble"><a class="markdownIt-Anchor" href="#43-ensemble"></a> 4.3. Ensemble</h3>
<p>考虑到元学习器通常对支持图像的质量很敏感，我们进一步建议利用查询-支持图像对之间场景差异的评估结果来调整元学习器得出的粗略预测。具体来说，我们首先整合基础学习器生成的前景概率图，以获得相对于小样本分割任务的背景区域预测：</p>
<p><img src="/img/loading.gif" data-original="images/image-20231201002147732.png" alt="image-20231201002147732" /></p>
<p>其中 pf b 的上标代表前景，下标 &quot;b &quot;代表基础学习者。</p>
<p>然后，我们利用从固定骨干网络中提取的低层次特征 f s low、f q low∈ RC1×H1×W1 分别计算支持图像和查询图像的革兰氏矩阵（见图 3）。请注意，这两种输入图像的相关运算是相似的，支持图像的运算可概括为</p>
<p><img src="/img/loading.gif" data-original="images/image-20231201002205432.png" alt="image-20231201002205432" /></p>
<p>其中 N =H1×W1，Freshape 将输入张量的大小调整为 C1×N。利用计算出的格兰矩阵，对它们的差值进行弗罗贝尼斯规范（Frobenius norm）评估，从而得到整体指标ψ，用于指导调整过程：</p>
<p><img src="/img/loading.gif" data-original="images/image-20231201002225323.png" alt="image-20231201002225323" /></p>
<p>其中，‖-‖F 表示输入矩阵的 Frobenius 准则。之后，在调整因子ψ的指导下，对两个学习器的粗略结果进行整合，进一步得出最终的分割预测结果 pf：</p>
<p><img src="/img/loading.gif" data-original="images/image-20231201002232436.png" alt="image-20231201002232436" /></p>
<p>其中 pm、pb 分别表示元学习器和基础学习器的预测结果。上标 &quot;0 &quot;和 &quot;1 &quot;分别代表背景和前景。Fψ 和 Fensemble 都是具有特定初始参数的 1×1 卷积运算。前者的目标是调整元学习器的粗略结果，而后者的目标是整合两个学习器。⊕ 表示沿通道维度的连接操作。最后，元训练阶段的总体损失可以通过以下方式进行评估：</p>
<p><img src="/img/loading.gif" data-original="images/image-20231201002251246.png" alt="image-20231201002251246" /></p>
<p>其中，λ 在所有实验中均设为 1.0，Lmeta 是式 (8) 所定义的元学习器的损失函数。</p>
<h3 id="44-k-shot-setting"><a class="markdownIt-Anchor" href="#44-k-shot-setting"></a> 4.4. K-Shot Setting</h3>
<p>当任务扩展到 K-shot（K&gt;1）设置时，就会有多幅注释（支持）图像可用。目前的 FSS 方法通常是将从支持分支提取的原型平均化，然后利用平均化后的特征来指导后续的分割过程，这种方法假定每个样本的贡献是相同的 [51, 56]。然而，这种方法可能并不理想，因为与查询图像有显著场景差异的样本无法提供更有针对性的指导。因此，我们进一步建议根据调整因子ψ来自适应地估计每张支持图像的权重，ψ值越小表示贡献越大，反之亦然。</p>
<p>具体来说，给定每个支持样本的调整因子ψi，我们首先通过连接操作将它们整合为一个统一的向量ψt∈RK。然后，应用两个全连接（FC）层来生成支持图像的融合权重η：</p>
<p><img src="/img/loading.gif" data-original="images/image-20231201002324426.png" alt="image-20231201002324426" /></p>
<p>其中，w1∈ RK× K r ，w2∈ R K r ×K 是 FC 层的权重，r 代表降维系数。最后，我们进行加权求和，得到最终的集合ψ。</p>
<h3 id="45-extension-to-generalized-fss"><a class="markdownIt-Anchor" href="#45-extension-to-generalized-fss"></a> 4.5. Extension to Generalized FSS</h3>
<p>所提出的 BAM 最初是为标准 FSS 任务而设计的，但它可以很容易地扩展到需要确定查询图像中基础类别和新类别区域的通用设置中。在这项工作中，我们只需根据预定义的阈值 τ 将基础学习器的结果和集合后的最终结果进行融合，即可得到整体分割预测结果 ˆ mg，可表述为</p>
<p><img src="/img/loading.gif" data-original="images/image-20231201002349668.png" alt="image-20231201002349668" /></p>
<p>其中 (x, y) 表示空间位置。ˆ mb 表示基本分割掩码，其计算公式为</p>
<p><img src="/img/loading.gif" data-original="images/image-20231201002402519.png" alt="image-20231201002402519" /></p>
<p>其中 arg max(-) 沿通道维度执行。</p>
<h2 id="5-experiments"><a class="markdownIt-Anchor" href="#5-experiments"></a> 5. Experiments</h2>
<h3 id="51-setup"><a class="markdownIt-Anchor" href="#51-setup"></a> 5.1. Setup</h3>
<p>**我们在两个广泛使用的 FSS 数据集（即 PASCAL-5i [46] 和 COCO-20i [38]）上评估了我们方法的性能。PASCAL-5i 由 Shaban 等人提出，基于 PASCAL VOC 2012 [9]，并添加了 SDS [14] 的注释；COCO-20i 由 MSCOCO [29]提出，载于 [38] 中。这两个数据集的对象类别被平均分为四个褶皱，实验以交叉验证的方式进行。对于每个褶皱，我们随机抽取 1,000 对支持和查询图像进行验证。</p>
<p><strong>评价指标：</strong> 参照前人的研究成果[34, 51, 61]，我们采用平均交集-重叠-联合（mIoU）和前景-背景 IoU（FB-IoU）作为实验的评价指标。</p>
<p><strong>实施细节</strong>* 拟议方法的训练过程可分为两个阶段，即预训练和元训练。在第一阶段，我们采用标准的监督学习范式，在 FSS 数据集的每个褶上训练基础学习器，该数据集由 PASCAL5i/COCO-20i 的 16/61 个类别（包括背景）组成。在我们的工作中，PSPNet [68] 被用作基础学习器，它在 PASCAL-5i 上进行了 100 次历时训练，在 COCO-20i 上进行了 20 次历时训练。使用初始学习率为 2.5e-3 的 SGD 优化器更新参数，训练批次大小设为 12。在第二阶段，我们以偶发学习的方式联合训练元学习器和集合模块，基础学习器的参数在这一阶段是固定的。需要注意的是，两个学习器共用同一个编码器来提取输入图像的特征，这也不利于泛化。其余网络层使用 SGD 优化器在 PASCAL-5i 和 COCO-20i 上分别训练 200 次和 50 次。在这两个数据集上，批量大小和学习率分别设置为 8 和 5e-2。我们采用 [51] 中的数据增强技术进行训练。在我们的工作中，PFENet [51] 的变体被用作元学习器，其中 FEM 被 ASPP [4] 代替，以降低复杂度。我们使用不同的随机种子对 5 条路径的结果进行了平均。提议的模型在 PyTorch 中实现，并在英伟达 RTX 2080Ti GPU 上运行。</p>
<h3 id="52-comparison-with-state-of-the-arts"><a class="markdownIt-Anchor" href="#52-comparison-with-state-of-the-arts"></a> 5.2. Comparison with State-of-the-Arts</h3>
<p>**表 1 和表 2 列出了不同方法在 PASCAL-5i 和 COCO-20i 基准上的 mIoU 结果。可以发现，我们的 BAM 以相当大的优势超过了先进的 FSS 模型，并在所有设置下都创造了新的技术水平。使用 VGG16 主干网时，与 PASCAL-5i 先前的最佳结果相比，拟议方法的 mIoU 分别提高了 4.71%p（1-shot）和 4.66%p（5-shot）。至于 COCO-20i，在使用 ResNet50 主干网的情况下，我们的 1shot 和 5-shot 结果分别比最佳竞争对手 HSNet 高出 7.03%p 和 4.26%p mIoU，证明了其处理复杂任务的卓越能力。此外，我们还在 PASCAL-5i 上将我们的模型与其他先进方法的 FBIoU 进行了比较（见表 3）。我们提出的 BAM 再次实现了大幅改进，尤其是在使用 ResNet50 主干网的单次搜索结果方面。</p>
<p><strong>定量结果</strong> 为了更好地分析和理解所提出的模型，我们进一步选取了元测试阶段的几个事件，并将相应的分割结果可视化，如图 4 所示。从我们的结果（第 4 行）中可以发现，与基线方法（第 3 行）相比，基类的假激活目标明显减少，这验证了基类学习器和集合模块的有效性。</p>
<h3 id="53-ablation-study"><a class="markdownIt-Anchor" href="#53-ablation-study"></a> 5.3. Ablation Study</h3>
<p>我们进行了一系列消融研究，以调查每个组件对分割性能的影响。请注意，除非另有说明，本节中的实验都是在使用 VGG16 骨干的 PASCAL-5i 数据集上进行的。</p>
<p>对两个学习者的消减研究。如第 4.1 节所述，两个学习器可以联合训练，也可以分开训练。在我们的实验中，后一种方案表现出更好的性能，如表 4 的前两行所示。4. 我们将这一现象归因于两个学习器对骨干网的不同利用。具体来说，一个学习者倾向于固定参数以增强泛化能力，而另一个学习者则倾向于更新参数以提取更具区分性的特征，这在端到端训练范式中很难取得平衡。此外，我们注意到，不使用 Lmeta 的模型性能会略微变差，这表明有必要对元学习器的预测结果进行约束。</p>
<p><strong>对集合模块的消融研究</strong> 模型的初始权重对训练过程乃至最终结果都有重大影响。因此，我们对合集模块的这一环节进行了相关的消减研究，它可以说是 BAM 的一个重要组成部分。在我们的实验中，元学习器和基础学习器的初始权重分别为1和0的集合模块明显优于其他方案，比随机初始化权重的模块提高了2.73%的mIoU，如表4第3行和第4行所示。4. 此外，我们还研究了调整因子ψ对性能的影响，结果表明，根据ψ调整元学习器的粗预测在模型集合预测中起着至关重要的作用。图 5 显示了使用不同低级特征估计ψ的方法之间的比较结果，其中使用 B2 特征的情况在分割准确性和计算复杂性之间显示出更好的权衡。</p>
<p>K 射线融合方案的消融研究。如第 4.4 节所述，我们建议根据ψ的值自适应调整每个支持样本的融合权重。与其他方案相比，所提出的方案在 5 次拍摄设置下获得了可观的增益（见表 5），进一步证明了衡量图像差异的因子对于 FSS 任务的重要性。</p>
<p>**为了评估 BAM 在复杂场景中的性能，我们使用不同的支持注释进行了实验。具体来说，除了标准的密集掩膜注释外，我们还引入了边界框注释进行比较。从表 6 中可以看出，使用边界框注释的模型在复杂场景中的表现更好。从表 6 中可以看出，与采用高成本像素注释的模型相比，采用边界框注释的模型结果更有竞争力，这表明所提出的方案具有很强的鲁棒性。</p>
<h3 id="54-generalized-few-shot-segmentation"><a class="markdownIt-Anchor" href="#54-generalized-few-shot-segmentation"></a> 5.4. Generalized Few-Shot Segmentation</h3>
<p>鉴于所提方法的独特性，我们将其扩展到更现实但更具挑战性的环境中，即广义 FSS。我们只需根据预定义阈值 τ 将最终输出与基础学习器的输出合并，即可生成整体分割结果，而无需任何可学习参数（公式 (18)）。受到与少拍分类和检测相关的工作[10, 22]的启发，我们还定义了三个指标来评估广义设置下的性能：mIoUn、mIoUb 和 mIoUa，分别表示新类别、基础类别和所有类别的 mIoU 分数。如表 7 所示 如表 7 所示，使用集合模块后，分割模型的性能得到了全面提升，而不仅仅是新类别。此外，图 6 中的定性结果也说明了该模型在处理通用 FSS 任务方面令人满意的能力。</p>
<h4 id="6-conclusion"><a class="markdownIt-Anchor" href="#6-conclusion"></a> 6. Conclusion</h4>
<p>我们提出了一种新颖的方案来缓解 FSS 模型对所见内容的偏差问题。我们方案的核心思想是利用基础学习器来识别查询图像中的可混淆（基础）区域，并进一步完善元学习器的预测。令人惊讶的是，即使只有两个普通学习器，我们的方案也在 FSS 基准上创造了新的技术水平。此外，我们还将当前任务扩展到了更具挑战性的广义环境中，并取得了优异的基线结果。我们希望我们的工作能为未来解决偏差或语义混淆问题的研究提供启示。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CANet/" title="CANet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">CANet</div></div><div class="info-2"><div class="info-item-1"> CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning (Chi Zhang 等。, 2019)  摘要 深度卷积神经网络和大规模标记的图像数据集推动了语义分割的最新进展。然而，用于像素分割的数据标签是繁琐而昂贵的。此外，一个训练有素的模型只能在一组预先定义的类别中进行预测。在本文中，我们提出了CANet，一个与类别无关的分割网络，在只有小样本注释图像的情况下对新的类别进行小样本的分割。我们的网络由一个双分支密集比较模块和一个迭代优化模块组成，前者在支持图像和查询图像之间进行多层次的特征比较，后者对预测结果进行迭代改进。此外，我们还引入了一种注意力机制，在k-shot学习的设定下，有效地融合来自多个支持实例的信息。在PASCAL VOC...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#bam"><span class="toc-text"> BAM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-introduction"><span class="toc-text"> 1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-related-works"><span class="toc-text"> 2. Related Works</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-problem-definition"><span class="toc-text"> 3. Problem Definition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-proposed-method"><span class="toc-text"> 4. Proposed Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-base-learner"><span class="toc-text"> 4.1. Base Learner</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-meta-learner"><span class="toc-text"> 4.2. Meta Learner</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-ensemble"><span class="toc-text"> 4.3. Ensemble</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#44-k-shot-setting"><span class="toc-text"> 4.4. K-Shot Setting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#45-extension-to-generalized-fss"><span class="toc-text"> 4.5. Extension to Generalized FSS</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-experiments"><span class="toc-text"> 5. Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#51-setup"><span class="toc-text"> 5.1. Setup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#52-comparison-with-state-of-the-arts"><span class="toc-text"> 5.2. Comparison with State-of-the-Arts</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#53-ablation-study"><span class="toc-text"> 5.3. Ablation Study</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#54-generalized-few-shot-segmentation"><span class="toc-text"> 5.4. Generalized Few-Shot Segmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-conclusion"><span class="toc-text"> 6. Conclusion</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/11/%E5%B0%8F%E7%B1%B3/syzkaller/syzkaller01-%E9%85%8D%E7%BD%AE/" title="syzkaller01-配置">syzkaller01-配置</a><time datetime="2024-12-10T17:10:45.000Z" title="发表于 2024-12-11 01:10:45">2024-12-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/BAM/" title="BAM">BAM</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>