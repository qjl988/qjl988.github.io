<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Interactive Few-shot Learning Limited Supervision, Better Medical Image Segmentation | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Interactive Few-shot Learning: Limited Supervision, Better Medical Image Segmentation  摘要 许多已知的用于医学图像分割的有监督的深度学习方法，在模型训练中承受着昂贵的数据注释负担。最近，人们提出了小样本分割方法来减轻这一负担，但这种方法往往对目标任务的适应性很差。通过审慎地将交互式学习引入小样本学习策略，我们">
<meta property="og:type" content="article">
<meta property="og:title" content="Interactive Few-shot Learning Limited Supervision, Better Medical Image Segmentation">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Interactive-Few-shot-Learning-Limited-Supervision,-Better-Medical-Image-Segmentation/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="Interactive Few-shot Learning: Limited Supervision, Better Medical Image Segmentation  摘要 许多已知的用于医学图像分割的有监督的深度学习方法，在模型训练中承受着昂贵的数据注释负担。最近，人们提出了小样本分割方法来减轻这一负担，但这种方法往往对目标任务的适应性很差。通过审慎地将交互式学习引入小样本学习策略，我们">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:04:06.226Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Interactive-Few-shot-Learning-Limited-Supervision,-Better-Medical-Image-Segmentation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Interactive Few-shot Learning Limited Supervision, Better Medical Image Segmentation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">Interactive Few-shot Learning Limited Supervision, Better Medical Image Segmentation</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Interactive Few-shot Learning Limited Supervision, Better Medical Image Segmentation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:04:06.226Z" title="更新于 2024-12-11 01:04:06">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">13.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>41分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="interactive-few-shot-learning-limited-supervision-better-medical-image-segmentation"><a class="markdownIt-Anchor" href="#interactive-few-shot-learning-limited-supervision-better-medical-image-segmentation"></a> Interactive Few-shot Learning: Limited Supervision, Better Medical Image Segmentation</h1>
<h2 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h2>
<p>许多已知的用于医学图像分割的有监督的深度学习方法，在模型训练中承受着昂贵的数据注释负担。最近，人们提出了小样本分割方法来减轻这一负担，但这种方法往往对目标任务的适应性很差。通过审慎地将交互式学习引入小样本学习策略，我们开发了一种新的小样本分割方法，称为交互式小样本学习（IFSL），它不仅解决了医学图像分割模型的注释负担，还解决了已知小样本分割方法的共同问题。首先，我们设计了一种新的小样本分割结构，称为基于医学先验的小样本学习网络（MPrNet），它只使用少数注释过的样本（如10个样本）作为支持图像来指导查询图像的分割，而无需任何预训练。然后，我们提出了一个基于交互式学习的测试时间优化算法（IL-TOA），以交互的方式在飞行中加强我们的MPrNet，以完成目标任务。据我们所知，我们的IFSL方法是第一个允许在目标任务上以互动和可控的方式优化和加强小样本分割模型。在四项小样本分割任务上的实验表明，我们的IFSL方法在DSC指标上优于最先进的方法20%以上。具体来说，交互式优化算法（IL-TTOA）对小样本的分割模型的DSC进一步改进贡献了10%。</p>
<h2 id="介绍"><a class="markdownIt-Anchor" href="#介绍"></a> 介绍</h2>
<p>对医学图像中的物体进行分割是医学图像分析中最基本的任务之一，有助于临床诊断和治疗计划。最近，深度学习方法已被证明对器官/组织的分割非常有效[1]。然而，这些方法大多是以完全监督的方式进行的，需要大量的训练数据和详细的注释。为分割任务注释大量的像素级基础事实是很耗时和昂贵的。此外，在实际场景中，医疗任务可能会面临这样的困境：只有很少的图像可用于模型训练（例如，对于一些罕见的疾病）。因此，研究如何减轻深度学习模型沉重的注释负担依赖，解决这些问题非常重要。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210251423517.png" alt="" /></p>
<p>小样本学习是一种从有限的图像和监督中学习任务的机器学习方法[2]，它有助于解决前述的问题。基于小样本学习的分割是一个新兴的研究课题。许多现有的小样本分割模型沿用了第一个工作[3]的网络结构，包括一个支持分支[4]-[7]或一个原型学习器[8]，[9]，和一个查询分支。<strong>图1</strong>说明了这些模型的共同想法：支持分支（或原型学习者）接收支持图像，而查询分支接收查询图像；这两个分支之间的具体连接传达支持信息，以指导特征提取和查询图像的分割。因此，当把这种模型用于小样本的分割任务时，研究人员一般遵循一个特定的机制[3]：通过采用接收注释的支持-查询图像对的模式，在多个任务上训练一个模型；然后把它用于一个未见过的目标任务的查询分割，用很少的注释样本作为支持输入。</p>
<p>尽管小样本分割模型在减少注释负担和监督方面取得了明显的成功，但它们仍然有相当大的缺点，特别是缺乏对目标任务的进一步优化。如上所述，小样本的分割模型通常是在多个任务中训练的，在这些任务中，模型积累了利用支持图像来指导查询分割的一般能力。这种能力使小样本的分割模型在一个未见过的目标任务上达到一定的性能，即使只有很少的注释样本可用[9]。然而，一个关键的问题是，这种普遍的能力可能不足以处理目标任务的一些独特特征。</p>
<p>**当模型用于自然图像时，这个问题可能不那么严重，因为它们可以在各种任务（如ImageNet[10]）上进行预训练，以获得大量有用的特征和强大的先验知识。但是，这个问题在医学图像上更为严重，主要有两个原因。1）缺乏专门针对医学图像的预训练模型[11]，以及2）自然图像和医学图像之间的实质性差异（例如，特定的器官定位和医学图像的独特特征[12]）可能会损害简单地将自然图像的知识转移到医学成像任务的有效性[13]，[14]。**面对这样的挑战，作者在[11]中提出了第一个小样本的分割模型，SSENet，用于处理小样本的医学图像分割任务，而不需要任何预训练。这种结构在一些小样本的医学图像分割任务中取得了最先进的性能。然而，其分割性能并不令人满意。此外，这种方法仍然存在上述现有小样本分割模型的共同想法的缺点，即缺乏对目标任务的进一步优化。</p>
<p>在本文中，我们提出了一种新颖的交互式小样本学习（IFSL）方法，用于医学图像分割（如图2所示），以解决注释负担和上述缺点的挑战。我们的IFSL方法旨在解决以下具体问题。如何获得更强的利用有限的支持信息来指导分割的一般能力；如何在有限的监督支持下加强获得的目标任务的能力。对于前一个问题，我们开发了一个新的小样本分割结构，称为基于医疗先验的小样本学习网络（MPrNet）。对于后一个问题，我们将交互式学习的理念引入到小样本学习策略中，并提出了基于交互式学习的测试时间优化算法（IL-TTOA）。</p>
<p>我们的MPrNet有一个编码器-解码器结构。在编码器中，两个分支（一个空间分支和一个纹理分支）被构造出来以提取支持图像的特征，一个查询分支被构造用来查询图像。空间分支的设计是为了捕捉目标的更多空间特征，具有大的感受野。然后，这些特征通过引入 “通道挤压和空间激励”（sSE）块被传达给查询分支以获得空间关注。纹理分支关注支持图像的详细纹理特征，并通过分权操作与查询分支建立强大的互动关系。解码器由两个分支（解码器-S和解码器-Q）组成，沿用了[11]中的网络。在这种具有多分支和强交互的结构中，考虑了一些医学图像的两个先验因素（相对固定的器官位置和医学图像的一般纹理性质），以弥补缺乏预训练的缺点，这在以前的研究中很少被探讨。当使用MPrNet来完成目标任务时，我们首先应用我们的IL-TOA来交互式地优化MPrNet（在其他任务上训练的），由极少的额外人类监督和修正来指导。在这个过程中，我们在一个小的目标任务数据集上进行了几个梯度下降步骤，并在一个特定的获得能力感知损失函数（LO）的指导下进行。损失函数的目的是平衡训练期间获得的能力和目标任务的额外监督的影响。</p>
<p>通过这种方式，一方面，我们的MPrNet可以直接利用极少数注释样本的有限监督，将这些样本作为目标任务的支持输入（就像大多数已知的少数学习模型[11], [15]）。另一方面，我们的IL-TOA可以动态地利用额外的人类监督来加强MPrNet，以便在目标任务上有更好的表现。在临床应用中，我们的IFSL方法可以帮助医生以非常轻的负担来注释/分割物体（只需要少数具有注释完整像素的样本），并通过交互式修正实现动态监督和修正（额外注释少量的像素）。此外，我们的IL-TOA可以适应于增强任何训练有素的模型，以获得更好的性能，并使人类能够被纳入基于深度学习的临床管道，进行分割以获得改进。</p>
<p>总而言之，我们的工作有四个主要贡献：</p>
<ul>
<li>一种新的结构，即MPrNet，由具有强连接的多个分支组成，用于医学图像的少量分割。</li>
<li>一种新的算法，IL-TTOA，通过人与人之间的互动修正来解决已知的小样本分割方法的不足之处。</li>
<li>实验表明，我们的IFSL方法在四项小样本分割任务上取得了可喜的成绩。我们的IFSL方法的主要思想（通过IL-TOA在目标任务上交互式地优化小样本的分割模型）进一步产生了明显的改进，并显示出加强任何小样本分割方法的巨大潜力。</li>
<li>我们的IFSL方法，特别是IL-TOA，具有临床意义：（1）减轻医学专家在注释图像上的负担，（2）使人类能够被纳入基于深度学习的临床管道，以加强目标任务上的模型（不限于少数学习模型）。</li>
</ul>
<h2 id="相关工作"><a class="markdownIt-Anchor" href="#相关工作"></a> 相关工作</h2>
<p>最近，大量的研究工作集中在减轻深度学习方法的数据和注释负担的问题上（例如，如[16]，[17]中的回顾）。这些研究处理了不同的具体问题，并主要在深度学习管道的不同阶段工作（这些阶段，如[18]，[19]中总结的，包括样本采集，数据解释和注释，模型训练等）。在本节中，我们回顾了这些研究，并根据他们所关注的具体问题和主要工作阶段，将其大致分为三组。</p>
<p><strong>1） 样本获取负担的方法：</strong></p>
<p>在一些医疗场景中，人们可能会面临这样的困境：只能收集到非常少的目标任务的医疗图像（例如，对于一些罕见的疾病）。一方面，一些方法（如零、一或少数几个镜头的学习，以及元学习方法）主要集中在解决这一困境，并广泛地旨在从少数样本中学习新目标任务的特征。当只能获得很少的样本作为训练样本时，这些方法是可取的，包括极端情况下，标记的例子只有一个或没有。然而，这些方法在医学图像上的表现仍然很差（特别是对于分割任务[11]），尽管它们在自然图像上的表现很好。在[20]中，作者回顾了这种不同的方法，包括零、一和少数的拍摄学习方法。在[21]中，作者回顾了不同的小样本学习方法，并将这些方法分为基于数据的方法、基于模型的方法和基于算法的方法。到目前为止，这种类型的已知方法主要集中在分类（如[15]、[22]、[23]）、分割（如[11]、[24]）和其他任务（如[25]、[26]）。但是，没有一篇评论或调查论文专门涉及医学图像上的这类方法。因此，这是一个值得探索的课题，但要减轻医学图像数据收集的负担，这是个挑战。</p>
<p>另一方面，一些工作旨在通过数据增强（而不是专家注释）扩大和丰富训练集来解决这一难题。这类方法不仅包括传统的数据增强操作，如高斯模糊[27]、外观增强（如[28]、[29]）、空间变换（如[30]、[31]）等，还包括合成方法，如CycleGANs[32]、条件GANs[33]、SGAN[34]、掩膜引导的GAN[35]等。</p>
<p><strong>2） 注释获取负担的方法：</strong></p>
<p>一般来说，在注释阶段给收集到的医学图像样本贴上人工标签（特别是对分割任务的像素级注释）是很耗时的，而且需要专家的努力。为了解决这个问题，一方面，一些方法专注于以更有效的方式选择少数最有价值的样本进行注释和模型训练，称为主动学习[16]。一般来说，主动学习策略会选择最有价值的样本，给它们贴上标签，然后把它们加入训练集进行进一步训练。许多主动学习框架已被提出用于医学图像分割（如[36]-[38]）、分类和检测（如[39]-[41]）。另一方面，一些方法试图以互动的方式附加注释和监督，称为互动学习。它可以通过允许专家注释者交互式地纠正模型产生的预测来加速注释过程[17]，这可能对分割任务产生重大影响（例如，[42]，[43]）。具体来说，[42]中的作者使以完全监督方式训练的CNN能够根据用户的互动（例如，用户提供边界框或涂鸦）进一步适应测试图像。在[44]中，作者回顾了交互式医学图像分割的方法。</p>
<p><strong>3）模型训练问题的方法：</strong></p>
<p>在训练阶段，模型性能和训练集规模之间存在着明显的权衡。如果只在少数样本上训练，深度学习模型可能容易过度拟合，而扩大训练集可能会产生更好的性能，但会产生昂贵的时间和专家努力成本。一些研究试图解决这一矛盾，探索更有效地训练深度学习模型的策略，并在训练阶段更好地利用小的或不完善的标注训练集的有限标注数据。在[45]中，作者回顾了医学图像分析中的这种类型的方法。例如，转移学习（包括预训练的操作；如[13]，[46]）和多任务学习方法[47]-[49]等方法提出要利用其他任务的特点或领域知识来完成目标任务。这些方法可以减轻目标任务的数据和注释负担，同时也可以加速训练过程[17]。此外，一些弱监督方法（如[50]，[51]）和半监督方法（如[52]-[55]）被给予利用未标记的数据或不完善的数据与良好标记的数据相结合来训练高性能的分割模型。</p>
<p>上述方法在减轻医学图像分析中的数据和注释负担方面取得了显著的成就，目的是以较少的数据和注释成本训练出更有效的模型。其中一些方法可以在多种情况下使用，一些方法可以在某些情况下协同工作以获得更好的性能。例如，在[56]中，作者整合了主动学习和迁移学习来减少注释的工作量。在本文中，我们关注的是只有很少的医学图像可用于注释和模型训练的极端情况，以探索一个更高效和有效的医学图像分割的框架。</p>
<h2 id="方法"><a class="markdownIt-Anchor" href="#方法"></a> 方法</h2>
<p>在这一节中，我们介绍了我们的IFSL方法，它由一个新的基于医学先验的少许学习网络（MPrNet）和一个基于互动学习的测试时间优化算法（IL-TOA）组成，如图2所示。</p>
<h3 id="a-问题设置"><a class="markdownIt-Anchor" href="#a-问题设置"></a> <strong>A. 问题设置</strong></h3>
<p>我们遵循[3]中关于小样本分割的表述。给定一个支持集S = {(Ii s, Y i s (l))}K i=1，这是一个由目标任务的K个图像和二元遮罩对组成的小集合，其中Ii s是第i个图像，Y i s (l)是目标语义类l∈Ltest的相应二元遮罩，目标是学习一个模型M，以生成一个给定支持集S的查询图像Iq的二元遮罩Mq(l)，表示为Mq(l) = F（M；S，Iq）。为了训练这个模型M，我们可以获得大量的其他语义类别的图像-掩码对来训练这个模型，表示为D = {(Ii d, Y i d )}N i=1，其中Y i d是训练图像Ii d的二进制掩码，N是训练图像的数量。D中也许有多个语义类别（Ltrain），Ltrain ∩Ltest = ∅。在本文中，我们简单地将目标类的分割过程称为测试阶段，而将其他任务的模型训练过程称为训练阶段。表一说明了本节中使用的符号。</p>
<h3 id="b-基于医学先验的少许学习网络mprnet"><a class="markdownIt-Anchor" href="#b-基于医学先验的少许学习网络mprnet"></a> <strong>B. 基于医学先验的少许学习网络(MPrNet)</strong></h3>
<p>我们的MPrNet采用编码器-解码器结构[1]，遵循现有的小样本分割模型[11], [15]的共同思路（如图1所示）。在编码器中，我们为支持图像建立一个空间分支和一个纹理分支，并为查询图像建立一个查询分支。在解码器中，我们简单地遵循sSENet[11]的解码器结构，建立两个分支（一个解码器-Q分支和一个解码器-S分支）。此外，在编码器和解码器之间还引入了一个基于余弦相似度的融合（CosF）模块。这种结构是通过采用医学图像的两种强先验知识而精心设计的[12]：（1）医学图像中目标器官的位置通常是相对固定的，（2）医学图像的一般纹理性质与非医学图像是可以区分的。</p>
<p><strong>1）编码</strong></p>
<p>在编码器中，我们从两个角度采用上述两个知识先验：针对支持图像的不同特征，采用不同结构的单独分支（空间分支和纹理分支）；两个分支与查询分支的具体信息交互方式。为清晰起见，在图2中，编码器中三个分支（空间分支、纹理分支和查询分支）的数据流分别用灰色、蓝色和黑色的线条表示。</p>
<p>a) 三个分支</p>
<p>首先，通过考虑空间信息的第一知识先验，我们建立了具有轻量级结构和大感受野的空间分支。如图2所示，该分支由四个连续的5×5 na-̈ve ConvBlocks组成。大的内核尺寸（5×5）用于大的感受野，使这个分支能够意识到目标的更全面的空间特征，而轻量级的结构减轻了大内核尺寸带来的沉重参数负担。然后，我们建立了纹理分支来捕捉支持图像的更详细的纹理特征。受VGGNet[?]结构的启发，纹理分支由四个重复的卷积块组成，其中有两个小核大小（3×3）的卷积，每个卷积块后面都有一个ReLU和一个2×2的最大池操作来进行下采样（如图2所示）。最后，我们用与纹理分支相同的结构建立查询分支，以捕捉查询图像与支持图像相似的详细特征（因为支持图像和查询图像是针对同一目标器官的，通常表现出相当相似的特征）。在这个结构中，我们选择三个分支有对称的布局（每个有四个块），这有助于把匹配块之间的强互动。</p>
<p>b) 信息交互时尚</p>
<p>信息交互在小样本的分割模型中起着重要作用[11]，它决定了支持图像的信息如何能够指导查询图像的特征提取和分割。在编码器中，我们通过三个步骤完成信息交互。首先，我们建立了从空间分支到查询分支的定向连接，将空间分支中的四个ConvBlocks分别与查询分支中的相应块进行桥接（如图2所示）。这些操作是基于这样的先验知识：支持-查询图像对中的特定目标器官通常出现在一个相对固定的位置。因此，这种一对一的连接和互动有助于传达捕获的支持图像的全局空间特征，以指导查询特征提取，从低级到高级。</p>
<p>接下来，基于纹理知识先验和这两个分支的相同结构，在纹理分支和查询分支之间引入了一个分权操作。这一操作有助于支持图像的提取的纹理特征以更直接和有效的方式指导查询分支。当然，由于不直接的相互作用，查询图像的提取特征也可能影响支持图像的特征提取。</p>
<p>最后，我们引入了 “通道挤压与空间激发”（sSE）块，以促进空间分支和查询分支之间的特征互动。sSE块是最近提出的一个计算单元[11]，它沿通道挤压特征图以获得空间注意力。在本研究中，我们用它来挤压空间分支的特征图，并激发查询分支的特征图，在第一步中把它加入到定向连接中。sSE块的详细结构见图2中标为sSE块的子图，其中挤压和激发的操作分别用s和σ表示。也就是说，编码器中的第i个sSE块（i∈{1，2，3，4}）从空间分支中的第i个ConvBlock（数据流由灰线̈表示）和查询分支中的第i个基本块（数据流由黑线≠表示）接收特征，并将纳入的特征传递给查询分支的第（i+1）个基本块（数据流由黑线Æ表示）。</p>
<p><strong>2）基于余弦相似度的融合（CosF）模块。</strong></p>
<p>为了利用支持掩码作为注意力来指导查询分割，我们提出了一个基于余弦相似度的融合（CosF）模块，并将其放在编码器之后。该模块测量支持图像的特征与查询图像的特征之间的相似性，然后将相似性作为查询特征的注意权重。具体来说，我们首先对ˆMtex进行掩蔽平均池化，这是由直接在ˆMtex上掩蔽Ys（在每个通道）来产生前景和背景特征，如[6]。接下来，我们将前景和背景特征连接起来，形成一个整体图，并计算其与ˆMque的余弦距离。直观地说，我们在图3中说明了屏蔽平均池和计算余弦距离的操作。生成的余弦距离矩阵（大小为1×32×32）然后通过每个通道的点乘作为查询图像特征（ˆ Mque）的注意权重。</p>
<p>给出支持图像Is、相应的地面真实掩膜Ys和查询图像Iq，CosF模块的计算步骤总结为公式（1）:</p>
<img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210251323628.png" style="zoom:50%;" />
<p>其中，由纹理分支捕获的Is的特征用ˆMtex表示，由查询分支获得的Iq的特征用ˆMque表示。ˆ MD que是CosF模块的输出特征，也是解码器-Q的输入。M AP表示进行屏蔽平均池的操作。</p>
<p>D表示计算余弦距离的过程，可以用下面的公式（2）来概括（也在图3的黑色虚线框中说明）:</p>
<img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210251324729.png" style="zoom:50%;" />
<p>其中cosij是上述余弦距离矩阵的一个元素，A是一个大小为1024的矢量，通过连接前景和背景特征得到（如图3所示），Bij是查询特征B的一个大小为1024的分量矢量（图3中用黄色立方体表示）。具体来说，B（大小为1024×32×32）是通过沿通道维度复制和串联ˆMtex（大小为512×32×32）得到的。通过这种方式，每个Bij的长度等于A的长度，因此可以计算出这两个向量之间的余弦相似度。为了进一步描述ˆ D的计算细节，我们可以简单地将矢量Bij表示为b，那么，上面的公式（2）可以写成：</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210251324608.png" alt="" /></p>
<p>其中Ak和bk分别是向量A和b的元素（即Bij），K是向量A和b的长度（本工作中K=1024）。在这一阶段，我们根据经验采用余弦距离来衡量特征的相似性，遵循[9]中的工作，该工作表明余弦距离与其他距离函数（如平方欧氏距离）相比，有助于获得更稳定和更好的性能。</p>
<p><strong>3）结构比较和讨论。</strong></p>
<p>现在我们讨论一下我们的MPrNet和sSENet[11]之间的区别。sSENet被认为是第一个用于医学图像的小样本学习分割模型，我们把它作为我们的强基线。在[11]中，作者证明了支持和查询分支之间的强连接是有效的，并且sSE块是有效的。因此，在我们的MPrNet中，我们继承了sSENet的基本解码器结构，也引入了sSE块。此外，我们构建了一个新的编码器结构，并将CosF模块放在编码器和解码器之间。此外，还有三个明显的区别。(1) 在sSENet中，支持图像直接与相应的地面真实掩码相连接，作为支持/条件分支的单一输入。然而，我们的MPrNet分别接收支持图像和其掩码。更具体地说，空间分支和纹理分支接收支持图像，而CosF模块接收支持掩码。通过这种方式，我们的MPrNet可能会更好地保留图像的原始特征。需要注意的是，这些特征在本质上与二进制掩码的特征不同，简单地将它们连接起来（如在sSENet中）可能会破坏图像的原始特征[9]。(2) 在[11]中，只用了一个轻量级的支持分支（表示为条件分支）来捕捉支持输入的特征，这对于医学图像的复杂特征可能是不够的。与sSENet和其他以前只有一个支持图像分支的网络不同，我们的MPrNet首先为支持图像构建了两个分支。这两个不同结构的分支集中于医学图像的不同特征，并互补地工作（如IVC.3节所示）。(3) 此外，与sSENet的天真的编码器-解码器结构相比，我们在编码器和解码器之间引入CosF模块。这个模块利用支持掩码作为注意力来进行查询分割，其有效性在第四节C中得到了验证。</p>
<h3 id="c-基于互动学习的测试时间优化算法"><a class="markdownIt-Anchor" href="#c-基于互动学习的测试时间优化算法"></a> C. 基于互动学习的测试时间优化算法</h3>
<p>如图2所示，我们提出了一种基于交互式学习的测试时间优化算法（IL-TTOA），以加强我们在目标任务上训练的MPrNet，以解决第一节中讨论的已知的小样本分割模型的缺点。首先，我们选择目标任务的几个未注释的样本，形成一个小数据集，Do = {Ii e}N ∗ i=1。样本数量N∗的值可能取决于具体的任务。接下来，我们在Do上用IL-TOA加强训练好的MPrNet（M(θ0)），并得到一个优化的模型M(θ)，用于目标任务的最终使用。在IL-TOA中，我们以交互方式加入人类监督作为指导或修正，并对Do进行多次梯度下降步骤，直到分割达到理想的质量水平或人为中断。在这个过程中，我们面临两个主要问题。(1) 如何更好地利用额外的人类监督，使训练好的模型能够获得更好的能力来捕捉未见过的目标任务的独特特征。(2) 如何使训练后的模型保持在训练过程中获得的一般能力。出于这两方面的考虑，我们设计了一个特定的可获得性感知损失函数（LO）来平衡这两种能力。</p>
<p><strong>1) IL-TOA的工作流程</strong></p>
<p>从一个经过训练的MPrNet M (θ0)开始，参数为θ0，我们对每个样本In e∈Do进行基本的循环处理。“预测-修正-修改”，这在算法1中得到了更精确的说明。对于每个样本In e，我们对这个循环进行Iter迭代。图6给出了一个例子，Do中有20个片断，10次迭代。预测是指使用当前模型M来预测In e的二进制掩码Mq。纠正是指要求人类专家通过指出任何错误区域来纠正预测的掩码（Mq）。这个操作被表示为O(-)，增加的修正信息被表示为Mp。Mp是一个{1, 0, -1}中数值的矩阵，表示每个像素被修正为什么标签。具体来说，被校正为正的像素被设置为1，被校正为负的像素被设置为-1。修改意味着通过运行梯度下降步骤来更新模型M中的参数（θ∗），使我们的LO损失最小。在这个过程中，我们利用一个像素级的加法（⊕）来结合预测图Mq和校正图Mp，并生成校正后的掩码Mg。因此，Mq中的假阴性像素在Mg中被纠正为1（对应于真阳性），而Mq中的假阳性像素在Mg中被纠正为0（对应于真阴性）。然后，Mg被当作一个假的真实掩码来计算预测掩码Mq的Dice损失。Dice损失是我们LO损失的一部分，用于指导参数优化。</p>
<p>2）获得能力感知损失函数（LO）。</p>
<p>我们的LO损失定义在下面的公式（4）中。它结合了一个天真的Dice损失（Ldice (θ∗)）和一个正则器。前者主要衡量IL-TOA中额外的监督和修正所造成的损失。后者根据参数对以前任务的重要性（Ωi,j）对参数的变化进行惩罚。对那些被认为是重要的参数的变化可以进行严厉的惩罚，这样就可以防止以前任务的重要知识被覆盖[57]。<img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210251342452.png" alt="" /></p>
<p>正则器适应了[57]中提出的记忆感知突触（MAS），它最初是用于终身学习策略中的分类任务。正如文献[57]中所讨论的，它根据预测输出对参数变化的敏感性来衡量网络中每个参数的重要性。在这项工作中，我们将MAS适应于我们的分割任务，将分割视为给每个像素分配一个一致的语义类别标签（器官或背景）的任务。除了将图像视为[57]中的一个数据点外，我们将一个像素视为一个数据点。因此，按照[57]，我们将每个像素xpi的学习函数相对于参数θi,j的梯度表示为gi,j(xpi)。θ是 &quot;旧 &quot;网络参数（在几个任务上训练的网络），其值相对固定，θ∗是每次迭代中更新的 &quot;新 &quot;网络参数。参数θi,j的重要性权重Ωi,j可以通过以下方式获得。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202210251344529.png" alt="" /></p>
<p>在这里，我们使用N个图像来计算Ωi,j，每个图像包含W×H像素。在实际应用中，考虑到这种计算的时间和空间成本，在结果相似的情况下，我们可以对图像进行4倍的降样。对于每个目标任务，Ω在训练集上只计算一次。通过这种方式，我们允许在目标任务的额外监督信息的帮助下改变参数，而从以前的任务中学到的重要知识可以得到保护。因此，我们的IL-TTOA很可能为临床场景中的小样本分割模型提供了一个新的观点，这使得医生能够以互动和实时的方式通知模型意识到目标任务的遗漏特征。</p>
<h2 id="实验和评估"><a class="markdownIt-Anchor" href="#实验和评估"></a> 实验和评估</h2>
<h3 id="a-数据集"><a class="markdownIt-Anchor" href="#a-数据集"></a> A. 数据集</h3>
<p>公共数据集。KiTS19挑战数据集[58]和LiTS挑战数据集[59]。KiTS19和LiTS都是用于器官分割挑战的训练集。KiTS19包含209个三维CT堆栈，其中有肾脏的切片级注释。LiTS包含131个带有肝脏注释的三维CT堆栈。图4给出了每个数据集的一个例子。</p>
<p>内部数据集。胃部数据集和乳腺数据集。胃部数据集包含153个带有胃部切片级注释的三维CT堆栈。乳房数据集是用于乳腺癌（BC）放疗的右侧临床靶区（CTV），包含114个三维CT堆栈和相应的CTV注释。CTV分割是成功治疗的一个重要步骤，因为CTV通常包括有潜在肿瘤扩散或亚临床疾病的组织。然而，CTV的自动分割是非常具有挑战性的，因为低对比度的可见性没有明确的划分[60] （如图4所示）。</p>
<p>所有CT堆栈的大小为256×256×C，C∈[160，220]是三维堆栈中的切片数量（经过厚度为2mm的均匀重采样）。在这项工作中，我们在二维图像上进行了所有的实验（即用堆栈的切片作为输入）。</p>
<h3 id="b-实验设置"><a class="markdownIt-Anchor" href="#b-实验设置"></a> B. 实验设置</h3>
<p>在本研究中，我们将每个目标器官/组织视为一个语义类，只有少数标记的图像，而其他任务的数据集被打包成一个训练集D用于训练模型。为方便起见，在下面的实验中，四个分割任务被表示为乳房、肾脏、肝脏和胃。例如，当我们以肝脏为目标任务进行实验时，其他三个数据集被用作训练集D。按照[11]中切片采样策略的基准设置，以下实验中使用的所有CT堆栈由感兴趣的器官所在的中央CT切片组成，大小为256×256×C（C∈{[50，96]，[47，91]，[20，119]，[31，106]}，分别用于乳房、肾、肝和胃任务）。</p>
<p><strong>1) Constructing Support-Query Image Pairs</strong></p>
<p>具体来说，我们提出了两种不同的方式，分别为训练任务和目标任务形成支持-查询图像对。在训练阶段，我们采用切片采样策略[11]的基准设置来形成支持-查询对。一个明显的扩展是，支持图像和查询图像是从两个不同的CT堆（同一任务）中选择的，这对于实际应用可能更有效。具体来说，我们把每个CT堆栈分成K个大小相等的片组，索引为1到K（从顶部开始，如[11]）。每次我们形成一个支持-查询对时，都会随机选择两个CT堆栈。对于其中一个，在第i组中随机选择的切片为查询图像；对于另一个，其第i组的中心切片被作为支持图像。</p>
<p>对于目标任务，只有一个CT堆栈的K片被用作支持图像。具体来说，这个CT堆栈被分为K组（在这项工作中K=10），每组的中心切片被注释，形成支持集S={(I i s, Y i s (l))}K=10 i=1（从顶部索引，像上一段的操作）。这个任务的其他CT堆栈形成了测试集Dtest。此外，我们把Dtest分成一个小的子集Do = {Ii e}N ∗ i=1和一个子集Dr（如图5所示）。Do，有N个∗片，从N个∗CT堆栈中随机选择，被我们的IFSL用来优化MPrNet。Dr，则用于最终测试。每个任务的Do和Dr的数据大小见表二。在优化阶段和最终测试阶段，我们通过两个步骤形成支持-查询对。首先，Do和Dr中的每个CT堆被分为K=10组。因此，给定一个CT堆栈中的查询图像，我们可以准确地确定它属于该堆栈的哪一组。假设一个查询图像来自一个堆栈的第i组（从顶部开始索引）。然后我们从S中选择第i个支持图像Ii s来形成支持-查询对。通过这种方式，小样本分割模型可以像训练时那样工作，从不同CT堆栈的匹配组中接收支持查询对。在临床应用中，可能需要额外的人工互动来指示CT堆栈中目标器官的开始和结束切片。</p>
<p><strong>2) Implementation Details:</strong></p>
<p>对于训练任务，我们使用天真的Dice loss来训练我们的MPrNet，50个epochs，批次大小为1。在初始学习率为le-3的情况下，我们使用多项式衰减学习率政策来降低它。我们使用迷你批随机梯度下降（SGD）作为优化器，动量=0.9，权重衰减=le-4。</p>
<p>在使用我们的IL-TOA进行模型优化的过程中，我们只需像[61]中那样模拟用户的修正，假设用户点击最大的误差区域。我们通过比较模型预测和地面实况并选择其中心像素来确定这个误差区域。然后我们用一个4×4的核来扩展它。在这项工作中，对于我们的IL-TTOA的所有优化实验，我们在Do中的T=20个切片上依次优化训练好的模型，并在每个切片上均匀地进行Iter=10次迭代而不中断（20个切片是随机排序的）。这意味着10×20个人工点被交互式地添加为额外的监督和修正（如图6所示）。对于我们的LO损失，在计算Ω值之前，我们通过4×4maxpooling对预测的掩模进行下采样，超参数λ为0.2（因为对比实验表明λ=0.2效果最好）。Dice-Sørensen系数（DSC）指标用于衡量实验中所有方法的性能。</p>
<h3 id="c-results-and-analysis"><a class="markdownIt-Anchor" href="#c-results-and-analysis"></a> C. Results and Analysis</h3>
<p><strong>1）与目前最先进的 &quot;小样本 &quot;分割模型的比较。</strong></p>
<p>表三将我们的MPrNet和IFSL方法与几个最先进的小样本分割模型进行了比较。为了进行公平的比较，所有的实验都是按照上面讨论的设置来分割数据并形成支持-查询对。所有被比较的小样本的分割模型都是按照原始论文中的方法实现的。</p>
<p>表三的第一个观察结果是，我们的MPrNet和IFSL方法在每个任务上的表现都超过了所有最先进的小样本分割模型，并在四个任务上产生了更高的平均DSC。尽管Co-FCN[5]和SGOne[6]在自然图像的小样本分割上取得了成功，但它们在我们的医学数据集上表现不佳。这可能是由于非医疗图像和医疗图像之间的性质有很大的不同，以及缺乏对医疗图像的预训练模型[13]。对于肝脏和肾脏的分割任务（也是[11]中的目标任务），sSENet的表现与原论文中的结果不完全相同，这是因为训练任务和我们使用的数据集不同。其次，我们注意到，我们的MPrNet在某些任务（肝脏和胃）上的表现要好得多，但在其他任务上的表现却有点差（DSC值低于DSC的60%）。很可能，这是不可避免的，因为训练和目标任务之间没有重叠，所以MPrNet在训练阶段获得的能力和知识可能与目标任务不太匹配。例如，MPrNet在训练过程中可能获得了更多关注纹理特征的能力，但空间特征对目标任务更重要。因此，我们认为，如果我们能够根据目标数据平衡纹理分支和空间分支的影响，MPrNet的性能将得到进一步提高。在这种情况下，IL-TOA作为一种补充，协助训练好的模型适应目标任务。如表三所示，在IL-TOA的帮助下，MPrNet可以在一个小的集合（如20个切片）上针对目标任务快速优化，并取得很大的改进（每个任务的DSC接近10%）。</p>
<p>图7显示了几个随机选择的样本（在数据集Dr中），以及不同模型的地面真实掩码和分割结果。为了更好的可视化，我们对所有的分割结果进行了简单的后处理，即扩张，获得最大的连接区域，以及侵蚀。与sSENet和PANet相比，我们的MPrNet在所有的任务中似乎表现得更好。显然，与所有其他模型相比，我们的IFSL方法在地面真实掩模方面取得了最有希望的性能。</p>
<p>此外，我们进行了一个实验作为软参考，以显示一个常用的有监督的深度学习模型（该模型以完全监督的方式在大规模数据集上训练）在这四个数据集上的表现。由于UNet[1]被广泛用作医学图像分割的强大架构，我们按照[1]的方式以天真的完全监督方式对U-Net进行了实验。对于每个分割任务，我们只需在数据集的80%的样本上训练一个U-Net模型，然后在剩余的样本（即数据集的20%）上测试它。换句话说，在N个（N∈{91，168，105，122}）CT堆栈上训练四个U-Net模型，每个堆栈包括n个切片（n在[160，220]范围内），分别用于乳腺、肾脏、肝脏和胃部任务（总共有{18928，36120，23100，26230}个切片）。表三的最右边一栏报告了不同方法对目标任务的注释数据。对于小样本的分割模型，只使用了10个带有完整注释的掩膜的切片，而对于U-Net模型，则使用了大约20000个切片（每个任务的大约20000个切片数是通过N的平均值和n的平均值相乘来估计的）。与U-Net相比，我们的IFSL在目标任务，特别是肝脏和胃部的注释成本大大降低，获得了令人鼓舞的结果。在这两个任务上，我们的方法和U-Net模型之间的DSC差异约为12%-17%，而我们的方法的注释负担远远小于U-Net（几乎是1/2000）。</p>
<p><strong>Discussion:</strong></p>
<p>我们应该注意到，在所有的情况下（使用很少的拍摄方法，纯粹训练目标分割的U-Net），这些模型在四个任务中的乳房表现最差。这可能表明，乳房任务比其他任务更难。如上所述，乳腺癌放疗的CTV的具体特点（对比度低，没有明确的分界线）使得CTV难以自动分割，在注释非常有限的情况下，这将更加艰难。当然，我们的方法可以在这个 &quot;硬 &quot;任务上取得更好的性能，代价是稍微增加目标任务的注释负担（如补充材料中所报告的）。</p>
<p><strong>2) Effects of IL-TTOA</strong></p>
<p>在这一节中，我们扩展了我们方法的主要思想，将交互式学习引入到小样本学习模型中，以加强几个表现良好的小样本分割模型，即PANet和sSENet（分别表示为PANet⊕IL-TOA和sSENet⊕ILTTOA）。在Dr上使用训练好的模型之前，我们用我们特定的IL-TTOA对其进行优化，具体过程如图6所示。我们对四个任务进行了实验，并在表四中列出了结果。值得注意的是，在我们的IL-TOA的帮助下，两个模型都取得了可喜的进步。sSENet甚至在肝脏的DSC上取得了超过30%的巨大进步。然而，与这两个模型相比，IL-TOA对我们MPrNet的贡献并不突出（大约7%-10%的改进）。我们认为这可能是由于我们的MPrNet在这四个小样本分割任务中的基本性能已经比其他模型好得多。虽然这些可喜的结果证明了我们想法的有效性，但我们相信，如果我们使用更多的目标任务数据来优化MPrNet，其性能可能会进一步提高（如补充材料中的报告）。</p>
<p>图8展示了有优化和无优化的模型的性能比较。对于每个任务，我们从最终的测试集Dr中随机选择一个切片，然后用训练好的MPrNet来分割该图像中的目标，并在第一行显示分割结果。之后，我们使用IL-TOA对Do进行优化，并随机选择在1、8、16和20片上优化的模型版本，对所选片中的目标进行分割。第二行到倒数第二行是分割的结果。值得注意的是，对于每个任务，分割结果从上到下逐渐变好。这些可喜的结果很好地验证了我们的IL-TOA在四个任务上的有效性。</p>
<ol start="3">
<li>Effects of the Individual Components of MPrNet:</li>
</ol>
<p>为了研究MPrNet中每个单独组件的效果，我们通过每次删除一个组件来进行消减研究。我们先后摘除了空间分支、纹理分支和CosF模块，构建了三个模型，在表五中用MPrNet Spa、MPrNet Tex和MPrNet CosF表示。由于[11]中的实验显示了具有强连接的解码器结构的有效性，我们保留所有具有解码器结构的模型作为我们MPrNet的最终版本。请注意，MPrNet的每个组件对四个任务的平均Dice得分都有4%-10%的提高。</p>
<p>此外，我们还进行了实验，以检查空间分支和纹理分支如何很好地集中于支持图像的不同特征，并相互补充地工作（在第III-B.3节讨论）。图9给出了几个例子，用梯度加权类激活映射（Grad-CAM）[62]来可视化这两个分支的捕获特征。对于每个分支，Grad-CAM使用流入最后共卷积层（瓶颈）的梯度信息来生成热图，从而突出该分支所涉及的鉴别性区域。更详细的Grad-CAM的计算方法在[62]中描述。在图9中，热图被重叠在原始支持图像上，红色区域表示被分支高度关注。<br />
从图9中可以看出，空间分支中的高激活区域与纹理分支中的高激活区域有很大不同。这一现象表明，这两个分支关注支持图像中的不同区域以捕捉不同的特征。具体来说，空间分支集中于一个任务的相对固定的区域（即任务的全局空间特征），特别是在使用同一支持图像的情况下。例如，图9(i)和图9(j)中的例子使用相同的切片作为支持图像。请注意，尽管两张查询图像中的目标形状明显不同，但空间分支关注的是几乎相同的区域（用红框表示）。在图9(m)和图9(n)的例子中也可以看到同样的情况。相反，即使有相同的支持图像，纹理分支也没有注意到任何特定的区域（例如，见图9(e)和图9(f)的例子）。</p>
<p>正如III-B节所讨论的，纹理分支和查询分支与分权操作有不直接的相互作用。因此，如图9(e)中的绿色方框所示，查询图像的特征会在很大程度上影响纹理分支的特征提取。另一个例子是图9(o)中的黄框；右上角的黄框可能表示纹理分支注意到了查询图像中目标的边界，而高亮区域似乎不在支持图像的区域内。由于这两个分支的具体结构和交互方式，这两个分支可以互补地工作，以捕捉支持图像的特征来指导查询分割。例如，在图9(o)的例子中，支持图像中的目标形状与查询图像中的目标形状完全不同。在这种情况下，空间分支集中于支持图像的全局空间特征（如上所述），而纹理分支则捕获与查询图像相关的鉴别和特殊支持特征（例如，查询图像中目标的区域或边界），如黄色方框中所示。</p>
<p><strong>4) Sensitivity of the Hyper-parameter λ:</strong></p>
<p>为了考察λ的敏感性，我们根据经验进行实验，将λ的值从0增加到1.5，步长为0.1。按照[57]，λ的影响从两个方面来估计。1）模型性能，和2）模型遗忘。对于前者，我们按照上述IV-B节的设置，以不同的λ值对我们的IFSL方法进行了实验。图10中报告了四个任务的平均DSCs，用Avg.Per表示。对于后者，我们遵循终身学习中常见的遗忘估计研究[63]，将训练任务视为 “以前遇到的任务”。具体来说，我们的IFSL方法在目标任务上的遗忘（简单表示为F orgetting）被定义为DSC在 &quot;以前遇到的任务 &quot;上的退化程度。这意味着我们通过在目标任务上通过IL-TOA优化之前和之后，在 &quot;以前遇到的任务 &quot;上对MPrNet进行两次评估，并计算这两次表现之间的差异，从而获得F orgetting值。这些性能分别用Per.Old和Per.New表示，差值用|Per.Old-Per.New|计算。为了进行综合估计，我们对四个任务的F orgetting值进行平均，以获得平均遗忘值（图10中表示为Avg.For）。更详细的设置和实验报告见补充材料。</p>
<p>如图10所示，平均性能（Avg.Per）首先得到改善，然后随着λ值的增加而降低。最佳性能出现在λ=0.2（DSC=64.49%）。这可以解释为λ=0.2可能在训练阶段获得的力量和IL-TOA中额外监督的影响之间达到了最佳平衡。因此，我们在本工作的所有实验中都选择λ=0.2。另一个观察结果是，平均遗忘值随着λ的增大而减少。这一现象似乎与我们的LO的理论预测相一致（在第III-C.2节），即λ越大，对参数变化的惩罚越多，允许遗忘的知识越少（特别是，λ=0意味着我们的LO相当于一个天真的Dice loss）。</p>
<h2 id="discussion-and-conclusions"><a class="markdownIt-Anchor" href="#discussion-and-conclusions"></a> DISCUSSION AND CONCLUSIONS</h2>
<p>在本文中，我们首次将交互式学习融入到小样本分割方法中，以减轻传统监督式深度学习方法的注释负担，并解决已知小样本分割方法的常见问题。我们新的交互式小样本学习（IFSL）方法有效地使小样本分割模型在目标任务上以交互和可控的方式得到加强。实验结果证明了我们的IFSL方法的优越性，它在DSC中比所有最先进的小样本分割方法高出20%以上（如第四节C.1所示）。</p>
<p>除了我们的新想法和明显的性能改进，我们还发现了未来研究的几个关键方向。首先，在实践中，我们的交互式策略（IL-TTOA）可能会产生额外的时间成本（与天真的小样本的分割模型相比），因为它需要专家指出错误区域来修改我们的MPrNet的几个迭代过程。因此，开发一种更有效的方法来接收和利用对目标任务的额外监督是一个重要的方向，这可能会在性能和时间成本之间产生一个更好的权衡。第二，医学图像中的其他目标可能会出现在不同的位置（例如，CT堆栈中的脑瘤或病理图像中的病变）。将我们目前的方法应用于这些目标可能仍然面临巨大的挑战，我们将在未来的研究中考虑这个问题。也许，在主动学习策略的帮助下，仔细选择信息量最大的样本作为支持图像或用于细化阶段（用IL-TOA）会对上述两个方向有所帮助。</p>
<p>总而言之，我们发现了现有的小样本分割方法的缺点，即缺乏对目标任务的优化，并提出了一种新的IFSL方法来弥补这些缺点。据我们所知，我们是第一个在目标任务上通过额外的交互式监督使小样本的分割模型得到加强。此外，在临床应用中，我们的新优化算法（ILTTOA）可以以人类可控的方式，即时加强任何小样本的深度学习分割模型。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/LMF/" title="LMF"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">LMF</div></div><div class="info-2"><div class="info-item-1"> 1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Lightweight-deep-learning-methods-for-panoramic-dental-X-ray-image-segmentation/" title="Lightweight deep learning methods for panoramic dental X-ray image segmentation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Lightweight deep learning methods for panoramic dental X-ray image segmentation</div></div><div class="info-2"><div class="info-item-1"> Lightweight deep learning methods for panoramic dental X-ray image segmentation  Abstract 牙科X射线图像分割有助于协助临床医生检查牙齿状况和识别牙科疾病。在X射线成像系统中可能需要不使用云计算的快速和轻量级的分割算法来实现。本文旨在研究用于牙科X射线图像分割的轻量级深度学习方法，以便部署在边缘设备上，如牙科X射线成像系统。本文提出了一种使用知识提炼的新型轻量级神经网络方案。提出的轻量级方法和一些现有的轻量级深度学习方法在全景牙科X光图像数据集上进行了训练。这些轻量级的方法通过使用几个准确度指标进行了评估和比较。提出的轻量级方法只需要0.33万个参数（7:5兆字节）用于训练模型，而与其他轻量级方法相比，它在IoU（0.804）和Dice（0.89）方面取得了最佳性能。这项工作表明，所提出的牙科X射线图像分割方法需要很小的内存存储，同时它取得了比较好的性能。该方法可以部署在边缘设备上，并有可能帮助临床医生减轻他们的日常工作流程，提高分析质量。  Experimental results ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">CA</div></div><div class="info-2"><div class="info-item-1"> Coordinate Attention for Efficient Mobile Network Design  Abstract 最近关于移动网络设计的研究已经证明了通道注意（如SE注意力机制）对于提高模型性能的显著效果，但他们通常忽略了位置信息，而位置信息对于生成空间选择性注意图是很重要的。在本文中，我们提出了一种新的移动网络注意机制，将位置信息嵌入到通道注意中，我们称之为...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#interactive-few-shot-learning-limited-supervision-better-medical-image-segmentation"><span class="toc-text"> Interactive Few-shot Learning: Limited Supervision, Better Medical Image Segmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text"> 摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-text"> 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-text"> 相关工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-text"> 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#a-%E9%97%AE%E9%A2%98%E8%AE%BE%E7%BD%AE"><span class="toc-text"> A. 问题设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#b-%E5%9F%BA%E4%BA%8E%E5%8C%BB%E5%AD%A6%E5%85%88%E9%AA%8C%E7%9A%84%E5%B0%91%E8%AE%B8%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9Cmprnet"><span class="toc-text"> B. 基于医学先验的少许学习网络(MPrNet)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#c-%E5%9F%BA%E4%BA%8E%E4%BA%92%E5%8A%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%B5%8B%E8%AF%95%E6%97%B6%E9%97%B4%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-text"> C. 基于互动学习的测试时间优化算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E5%92%8C%E8%AF%84%E4%BC%B0"><span class="toc-text"> 实验和评估</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#a-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text"> A. 数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#b-%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="toc-text"> B. 实验设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#c-results-and-analysis"><span class="toc-text"> C. Results and Analysis</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#discussion-and-conclusions"><span class="toc-text"> DISCUSSION AND CONCLUSIONS</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA">CA</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>