<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>ECA-Net Efficient Channel Attention for Deep Convolutional Neural Networks | qjl988</title><meta name="author" content="qjl988"><meta name="copyright" content="qjl988"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ECA-Net ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks  Abstract 最近，通道注意机制被证明在提高深度卷积神经网络（CNN）的性能方面具有巨大的潜力。然而，大多数现有的方法致力于开发更复杂的注意力模块以实现更好的性能，这不可避免地增加了模型的复杂性。为了克服性能和复杂性之间的矛">
<meta property="og:type" content="article">
<meta property="og:title" content="ECA-Net Efficient Channel Attention for Deep Convolutional Neural Networks">
<meta property="og:url" content="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ECA/index.html">
<meta property="og:site_name" content="qjl988">
<meta property="og:description" content="ECA-Net ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks  Abstract 最近，通道注意机制被证明在提高深度卷积神经网络（CNN）的性能方面具有巨大的潜力。然而，大多数现有的方法致力于开发更复杂的注意力模块以实现更好的性能，这不可避免地增加了模型的复杂性。为了克服性能和复杂性之间的矛">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qjl988.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-12-10T15:28:34.000Z">
<meta property="article:modified_time" content="2024-12-10T17:04:00.804Z">
<meta property="article:author" content="qjl988">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qjl988.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qjl988.github.io/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ECA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ECA-Net Efficient Channel Attention for Deep Convolutional Neural Networks',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qjl988</span></a><a class="nav-page-title" href="/"><span class="site-name">ECA-Net Efficient Channel Attention for Deep Convolutional Neural Networks</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">ECA-Net Efficient Channel Attention for Deep Convolutional Neural Networks</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T17:04:00.804Z" title="更新于 2024-12-11 01:04:00">2024-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">7.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>25分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="eca-net"><a class="markdownIt-Anchor" href="#eca-net"></a> ECA-Net</h1>
<p>ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks</p>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>最近，通道注意机制被证明在提高深度卷积神经网络（CNN）的性能方面具有巨大的潜力。然而，大多数现有的方法致力于开发更复杂的注意力模块以实现更好的性能，这不可避免地增加了模型的复杂性。为了克服性能和复杂性之间的矛盾，本文提出了一个高效通道注意（ECA）模块，它只涉及少数几个参数，同时带来明显的性能增益。**通过剖析SENet中的通道注意模块，我们实证表明避免降维对学习通道注意很重要，而适当的跨通道交互可以保持性能，同时显著降低模型的复杂性。因此，我们提出了一种无需降维的局部跨通道交互策略，它可以通过一维卷积有效地实现。**此外，我们开发了一种自适应选择一维卷积核大小的方法，以确定局部跨通道交互的覆盖范围。我们提出的ECA模块是高效的，例如，我们的模块对ResNet50骨干的参数和计算量分别是80对24.37M和4.7e-4 GFLOPs对3.86 GFLOPs，在Top-1的准确性方面，性能提升超过了2%。我们对我们的ECA模块进行了广泛的评估，包括图像分类、物体检测和实例分割，并以ResNets和MobileNetV2为骨架。实验结果表明，我们的模块效率更高，同时在性能上优于同类产品。</p>
<h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1. Introduction</h2>
<p>深度卷积神经网络（CNN）在计算机视觉界得到了广泛的应用，并在图像分类、物体检测和语义分割等广泛的任务中取得了巨大的进展。从开创性的AlexNet[17]开始，许多研究都在不断研究以进一步提高深度CNN的性能[29, 30, 11, 15, 19, 20, 32]。最近，将通道注意力纳入卷积块引起了很多人的兴趣，在性能改进方面显示出巨大的潜力[14, 33, 13, 4, 9, 18, 7]。其中一个代表性的方法是挤压和激发网络（SENet）[14]，它为每个卷积块学习通道注意力，为各种深度CNN架构带来明显的性能增益。</p>
<p>在SENet[14]中设置了挤压（即特征聚集）和激发（即特征重新校准）后，一些研究通过捕捉更复杂的通道依赖性[33, 4, 9, 7]或与额外的空间注意力相结合来改进SE块[33, 13, 7]。尽管这些方法取得了更高的精度，但它们往往带来了更高的模型复杂性，并承受着更重的计算负担。与上述以较高的模型复杂度为代价获得较好性能的方法不同，本文反而关注一个问题： 能否以一种更有效的方式学习有效的通道注意？</p>
<p>为了回答这个问题，我们首先重新审视SENet中的通道注意模块。具体来说，给定输入特征后，SE模块首先对每个通道独立采用全局平均池，然后用两个全连接（FC）层，用非线性函数和Sigmoid函数生成通道权重。这两个FC层的设计是为了捕捉非线性的跨通道互动，这涉及到控制模型复杂性的降维。虽然这种策略在后续的通道注意力模块中被广泛使用[33, 13, 9]，但我们的经验研究表明，降维会给通道注意力预测带来副作用，而且捕捉所有通道的依赖关系是低效和不必要的。</p>
<p>因此，本文提出了一个用于深度CNN的高效通道关注（ECA）模块，它避免了降维，并以有效的方式捕获跨通道的互动。如图2所示，在不降维的情况下进行通道全局平均池化后，我们的ECA通过考虑每个通道和它的k个邻居来捕获局部的跨通道互动。这种方法被证明可以保证效率和效果。请注意，我们的ECA可以通过大小为k的快速一维卷积来有效实现，其中内核大小k代表本地跨通道交互的覆盖率，即有多少邻居参与了一个通道的注意力预测。为了避免通过交叉验证手动调整k，我们开发了一种方法来适应性地确定k，其中互动的覆盖率（即内核大小k）与通道维度成正比。如图1和表3所示，与骨干模型[11]相比，带有我们的ECA模块（称为ECA-Net）的深度CNN引入的额外参数很少，计算量可以忽略不计，同时带来明显的性能提升。例如，对于具有24.37M参数和3.86GFLOPs的ResNet-50来说，ECA-Net50的额外参数和计算量分别为80和4.7e4 GFLOPs；同时，ECA-Net50在Top-1的准确度方面比ResNet-50高出2.28%。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/img/202306142327674.png" alt="" /></p>
<blockquote>
<p>图2. 我们的高效信道关注（ECA）模块的示意图。考虑到通过全局平均池（GAP）获得的聚合特征，ECA通过执行大小为k的快速一维卷积来生成通道权重，其中k是通过通道维度C的映射自适应确定的。</p>
</blockquote>
<p>表1总结了现有的注意力模块是否进行了通道降维（DR）、跨通道交互和轻量级模型，我们可以看到我们的ECA模块通过避免通道降维来学习有效的通道注意力，同时以极轻量级的方式捕捉跨通道交互。为了评估我们的方法，我们在ImageNet-1K[6]和MS COCO[23]的各种任务中使用不同的深度CNN架构进行了实验。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230614233146383.png" alt="image-20230614233146383" /></p>
<blockquote>
<p>表1. 现有的注意力模块在是否没有通道降维（No DR）、跨通道互动和比SE更少的参数（用轻量级表示）方面的比较。</p>
</blockquote>
<p>本文的贡献总结如下。</p>
<p>(1) 我们对SE块进行了剖析，并通过经验证明避免降维和适当的跨通道交互对学习有效和高效的通道注意力分别很重要。</p>
<p>(2) 基于以上分析，我们尝试为深度CNN开发一个极其轻量级的通道注意模块，提出了高效通道注意（ECA），在带来明显改进的同时，增加了很少的模型复杂性。</p>
<p>(3) 在ImageNet-1K和MS COCO上的实验结果表明，我们的方法比最先进的方法具有更低的模型复杂性，同时取得了非常有竞争力的性能。</p>
<h2 id="2-related-work"><a class="markdownIt-Anchor" href="#2-related-work"></a> 2. Related Work</h2>
<p>事实证明，注意力机制是增强深度CNN的一个潜在手段。SE-Net[14]首次提出了一种学习通道注意力的有效机制，并取得了可喜的成绩。随后，注意力模块的发展大致可以分为两个方向：（1）增强特征聚合；（2）结合通道和空间注意力。具体来说，CBAM[33]同时采用了平均和最大集合来聚合特征。GSoP[9]引入了二阶池化以实现更有效的特征聚合。GE[13]使用深度卷积[5]来聚合特征，探索空间扩展。CBAM[33]和scSE[27]使用内核大小为k×k的二维卷积计算空间注意力，然后将其与通道注意力相结合。GCNet[2]与非本地（NL）神经网络[32]有着相似的理念，开发了一个简化的NL网络，并与SE块集成，形成了一个轻量级的模块来模拟长距离的依赖性。双重注意网络（A2-Nets）[4]为NL块引入了一个新颖的关系函数，用于图像或视频识别。双重注意力网络（DAN）[7]同时考虑基于NL的通道和空间注意力，用于语义分割。然而，上述大多数基于NL的注意力模块由于其较高的模型复杂性，只能用于单个或几个卷积块。显然，上述所有的方法都侧重于开发复杂的注意力模块以获得更好的性能。与它们不同的是，我们的ECA旨在以低模型复杂度学习有效的通道注意。</p>
<p>我们的工作也与高效卷积有关，它是为轻型CNN设计的。两个广泛使用的高效卷积是分组卷积[36, 34, 16]和深度可分离卷积[5, 28, 37, 24]。如表2所示，尽管这些高效的卷积涉及的参数较少，但它们在注意力模块中显示出的效果不大。我们的ECA模块旨在捕捉局部跨通道的交互作用，这与通道局部卷积[35]和通道明智卷积[8]有一些相似之处；与它们不同的是，我们的方法研究了一个具有自适应核大小的一维卷积来代替通道注意模块中的FC层。与分组和深度可分离卷积相比，我们的方法以较低的模型复杂度实现了更好的性能。</p>
<h2 id="3-proposed-method"><a class="markdownIt-Anchor" href="#3-proposed-method"></a> 3. Proposed Method</h2>
<p>在这一节中，我们首先重温了SENet[14]中的通道关注模块（即SE块）。然后，我们通过分析降维和跨通道交互的影响，对SE模块进行经验诊断。这促使我们提出我们的ECA模块。此外，我们开发了一种自适应确定ECA参数的方法，并最终展示了如何将其用于深度CNN。</p>
<h3 id="31-revisiting-channel-attention-in-se-block"><a class="markdownIt-Anchor" href="#31-revisiting-channel-attention-in-se-block"></a> 3.1. Revisiting Channel Attention in SE Block</h3>
<p>让一个卷积块的输出为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>W</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X∈R^{W×H×C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span></span></span></span></span></span></span></span></span>，其中W、H和C是宽度、高度和通道尺寸（即滤波器的数量）。因此，SE块中通道的权重可以计算为</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515201147442.png" alt="image-20230515201147442" /></p>
<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>W</mi><mi>H</mi></mrow></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>W</mi><mo separator="true">,</mo><mi>H</mi></mrow></msubsup><msub><mi>X</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">g(X)=\frac{1}{WH}\sum^{W,H}_{i=1,j=1}X_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.417049em;vertical-align:-0.43581800000000004em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029000000000005em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>是通道元素的全局平均池（GAP），σ是一个Sigmoid函数。设</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515201135318.png" alt="image-20230515201135318" /></p>
<p>其中ReLU表示整流线性单元[25]。为了避免模型的高复杂性，W1和W2的大小分别被设定为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>×</mo><mo stretchy="false">(</mo><mfrac><mi>C</mi><mi>r</mi></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">C×( \frac{C}{r} )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mfrac><mi>C</mi><mi>r</mi></mfrac><mo stretchy="false">)</mo><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">( \frac{C}{r} )×C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>。我们可以看到，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">{</mo><msub><mi>W</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>W</mi><mn>2</mn></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">f\{W_1,W_2\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>涉及通道注意块的所有参数。虽然公式（2）中的降维可以降低模型的复杂性，但它破坏了通道和其权重之间的直接对应关系。例如，一个单一的FC层使用所有通道的线性组合来预测每个通道的权重。但公式（2）首先将通道特征投射到一个低维空间，然后再将其映射回来，使得通道和其权重之间的对应关系是间接的。</p>
<h3 id="32-efficient-channel-attention-eca-module"><a class="markdownIt-Anchor" href="#32-efficient-channel-attention-eca-module"></a> 3.2. Efficient Channel Attention (ECA) Module</h3>
<p>在重新审视了SE块之后，我们进行了经验比较，以分析通道降维和跨通道互动对通道注意力学习的影响。根据这些分析，我们提出了我们的高效通道注意（ECA）模块。</p>
<h4 id="321-avoiding-dimensionality-reduction"><a class="markdownIt-Anchor" href="#321-avoiding-dimensionality-reduction"></a> 3.2.1 Avoiding Dimensionality Reduction</h4>
<p>如上所述，公式（2）中的降维使得通道和其权重之间的对应关系是间接的。为了验证其效果，我们将原始的SE块与它的三个变体（即SE-Var1、SE-Var2和SEVar3）进行比较，它们都不进行降维。如表2所示，没有参数的SE-Var1仍然优于原始网络，表明通道注意力有能力提高深度CNN的性能。同时，SE-Var2独立学习每个通道的权重，在涉及较少参数的情况下，略优于SE块。这可能表明，通道和它的权重需要直接对应，同时避免降维比考虑非线性通道依赖性更重要。此外，SEVar3采用一个单一的FC层比两个FC层在SE块中的降维表现更好。所有上述结果清楚地表明，避免降维有助于学习有效的通道注意力。因此，我们开发了没有通道降维的ECA模块。</p>
<p><img src="/img/loading.gif" data-original="images/image-20230624163758165.png" alt="image-20230624163758165" /></p>
<blockquote>
<p>表2.使用ResNet-50作为骨干模型在ImageNet上对各种通道注意模块的比较。#.Param.表示通道注意模块的参数数；⊙表示元素的乘积；GC和C1D分别表示分组卷积和一维卷积；k是C1D的核大小。</p>
</blockquote>
<h4 id="322-local-cross-channel-interaction"><a class="markdownIt-Anchor" href="#322-local-cross-channel-interaction"></a> 3.2.2 Local Cross-Channel Interaction</h4>
<p>鉴于聚合的特征y∈RC没有降维，通道注意力可以通过以下方式学习</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515202002481.png" alt="image-20230515201725149" /></p>
<p>其中W是一个C×C的参数矩阵。特别是，对于SE-Var2和SE-Var3，我们有</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515201737276.png" alt="image-20230515201737276" /></p>
<p>其中SE-Var2的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>v</mi><mi>a</mi><mi>r</mi><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">W_{var2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是一个对角线矩阵，涉及C个参数；SE-Var3的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>v</mi><mi>a</mi><mi>r</mi><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">W_{var2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是一个全矩阵，涉及C×C参数。如公式（4）所示，关键的区别是SE-Var3考虑了跨通道的互动，而SEVar2没有，因此SE-Var3取得了更好的性能。这一结果表明，跨通道互动有利于学习通道注意力。然而，SEVar3需要大量的参数，导致模型的复杂性很高，特别是对于大的通道数量。</p>
<p>SE-Var2和SE-Var3之间一个可能的折衷办法是将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>v</mi><mi>a</mi><mi>r</mi><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">W_{var2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>扩展为一个块状对角线矩阵，即</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515201535213.png" alt="image-20230515202002481" /></p>
<p>其中公式（5）将通道分为G组，每组包括C/G通道，并在每组中独立学习通道注意力，以局部方式捕捉跨通道的互动。因此，它涉及C2/G参数。从卷积的角度来看，SE-Var2、SEVar3和公式（5）可以分别被视为深度可分离卷积、FC层和组卷积。这里，带有分组卷积的SE块（SE-GC）由σ(GCG(y)) = σ(WGy)表示。然而，如[24]所示，过多的分组卷积会增加内存访问成本，从而降低计算效率。此外，如表2所示，不同分组的SE-GC与SE-Var2相比没有带来任何收益，表明它不是一个有效的捕捉局部跨通道交互的方案。原因可能是SE-GC完全抛弃了不同组间的依赖关系。</p>
<p>在本文中，我们探索了另一种捕捉局部跨通道交互的方法，目的是保证效率和效果。具体来说，我们采用一个带状矩阵Wk来学习通道注意力，而<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">W_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>具有</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515202234267.png" alt="image-20230515202234267" /></p>
<p>显然，公式（6）中的Wk涉及k×C的参数，通常比公式（5）的参数要少。此外，公式（6）避免了公式（5）中不同组之间的完全独立。如表2所示，公式（6）中的方法（即ECA-NS）优于公式（5）的SE-GC。对于公式（6），计算yi的权重时只考虑yi和它的k个邻居之间的相互作用，即</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515202343253.png" alt="image-20230515202300247" /></p>
<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="normal">Ω</mi><mi>i</mi><mi>k</mi></msubsup></mrow><annotation encoding="application/x-tex">Ω_{i}^{k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.107772em;vertical-align:-0.258664em;"></span><span class="mord"><span class="mord">Ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span></span></span></span>表示<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的k个相邻通道的集合。</p>
<p>一个更有效的方法是使所有通道共享相同的学习参数，即</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515202300247.png" alt="image-20230515202343253" /></p>
<p>请注意，这种策略可以很容易地通过内核大小为k的快速一维卷积来实现，即</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515202354277.png" alt="image-20230515202354277" /></p>
<p>其中C1D表示一维卷积。这里，公式（9）中的方法被称为高效通道关注（ECA）模块，它只涉及k个参数。如表2所示，我们的ECA模块在k=3的情况下取得了与SE-var3相似的结果，同时模型的复杂度要低得多，通过适当地捕捉局部跨通道的互动，保证了效率和效果。</p>
<h4 id="323-coverage-of-local-cross-channel-interaction"><a class="markdownIt-Anchor" href="#323-coverage-of-local-cross-channel-interaction"></a> 3.2.3 Coverage of Local Cross-Channel Interaction</h4>
<p>由于我们的ECA模块（9）旨在适当地捕捉局部的跨通道交互，所以需要确定交互的覆盖范围（即1D卷积的核大小k）。对于各种CNN结构中具有不同通道数的卷积块，可以手动调整优化的交互覆盖率。然而，通过交叉验证进行手动调整将花费大量的计算资源。分组卷积已经成功地采用来改进CNN架构[36, 34, 16]，其中高维（低维）通道涉及长距离（短距离）的卷积，给定固定的组数。分享类似的理念，互动的覆盖率（即一维卷积的核大小）与通道维度C成正比是合理的。 换句话说，在k和C之间可能存在一个映射φ：</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515202558836.png" alt="image-20230515202542215" /></p>
<p>最简单的映射是一个线性函数，即φ(k) = γ ∗ k - b。然而，线性函数所描述的关系太有限了。另一方面，众所周知，信道维度C（即滤波器的数量）通常被设定为2的幂。因此，我们引入一个可能的解决方案，将线性函数φ（k）=γ ∗ k - b扩展为非线性函数，即：</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515202611641.png" alt="image-20230515202558836" /></p>
<p>然后，给定信道维度C，内核大小k可以通过以下方式自适应确定</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515202542215.png" alt="image-20230515202611641" /></p>
<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>t</mi><msub><mi mathvariant="normal">∣</mi><mrow><mi>o</mi><mi>d</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">|t|_{odd}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal">t</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>表示最接近的t的奇数。在本文中，我们在所有的实验中都将γ和b分别设置为2和1。显然，通过映射ψ，高维信道有更长的范围互动，而低维信道通过使用非线性映射进行更短的范围互动。</p>
<h3 id="33-eca-module-for-deep-cnns"><a class="markdownIt-Anchor" href="#33-eca-module-for-deep-cnns"></a> 3.3. ECA Module for Deep CNNs</h3>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515203243699.png" alt="image-20230515202730373" /></p>
<blockquote>
<p>图2.我们的高效信道关注（ECA）模块的示意图。考虑到通过全局平均池（GAP）获得的聚合特征，ECA通过执行大小为k的快速一维卷积来生成通道权重，其中k是通过通道维度C的映射自适应确定的。</p>
</blockquote>
<p>图2说明了我们ECA模块的概况。在使用GAP聚合卷积特征而不降维后，ECA模块首先自适应地确定核大小k，然后执行一维卷积，接着使用Sigmoid函数来学习通道注意力。为了将我们的ECA应用于深度CNN，我们按照[14]中的相同配置，用我们的ECA模块取代SE块。由此产生的网络被命名为ECA-Net。图3给出了我们ECA的PyTorch代码。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515202833461.png" alt="image-20230515202833461" /></p>
<blockquote>
<p>图3.我们ECA模块的PyTorch代码。</p>
</blockquote>
<h2 id="4-experiments"><a class="markdownIt-Anchor" href="#4-experiments"></a> 4. Experiments</h2>
<p>在这一节中，我们分别使用ImageNet[6]和MS COCO[23]对所提出的方法在大规模图像分类、物体检测和实例分割上进行评估。具体来说，我们首先评估了内核大小对我们ECA模块的影响，并与ImageNet上最先进的对应模块进行比较。然后，我们使用Faster R-CNN[26]、Mask R-CNN[10]和RetinaNet[22]验证我们的ECA-Net在MS COCO上的有效性。</p>
<h3 id="41-implementation-details"><a class="markdownIt-Anchor" href="#41-implementation-details"></a> 4.1. Implementation Details</h3>
<p>为了评估我们在ImageNet分类上的ECA-Net，我们采用了四个广泛使用的CNN作为骨干模型，包括ResNet-50[11]、ResNet-101[11]、ResNet-512[11]和MobileNetV2[28]。为了用我们的ECA训练ResNets，我们采用了与[11, 14]中完全相同的数据增强和超参数设置。具体来说，输入图像被随机裁剪为224×224，并随机进行水平翻转。网络的参数通过随机梯度下降法（SGD）进行优化，权重衰减为1e-4，动量为0.9，迷你批次大小为256。所有的模型都是在100个历时内通过设置初始学习率为0.1来训练的，每30个历时减少10倍。对于用我们的ECA训练MobileNetV2，我们遵循[28]中的设置，在400个历时内用SGD训练网络，权重衰减为4e-5，动量为0.9，小批量大小为96。初始学习率被设定为0.045，并通过0.98的线性衰减率来降低。为了在验证集上进行测试，输入图像的短边首先被调整为256，并使用224×224的中心裁剪进行评估。所有的模型都由PyTorch工具箱1实现。</p>
<p>我们使用Faster R-CNN[26]、Mask R-CNN[10]和RetinaNet[22]在MS COCO上进一步评估我们的方法，其中ResNet-50和ResNet-101以及FPN[21]被作为骨干模型。我们通过使用MMDetection工具包[3]实现所有检测器，并采用默认设置。具体来说，输入图像的短边被调整为800，然后使用SGD优化所有模型，权重衰减为1e-4，动量为0.9，小批量大小为8（4个GPU，每个GPU有2幅图像）。学习率初始化为0.01，并在8和11个历时后分别减少10倍。我们在COCO的train2017上的12个历时内训练所有的检测器，并在val2017上报告结果以进行比较。所有程序都在配备有四个RTX 2080Ti GPU和一个Intel® Xeon Silver 4112 CPU@2.60GHz 的PC上运行。</p>
<h3 id="42-image-classification-on-imagenet-1k"><a class="markdownIt-Anchor" href="#42-image-classification-on-imagenet-1k"></a> 4.2. Image Classification on ImageNet-1K</h3>
<p>在这里，我们首先评估了核大小对我们ECA模块的影响，并验证了我们自适应确定核大小的方法的有效性，然后我们用ResNet-50、ResNet-101、ResNet-152和MobileNetV2与最先进的同行和CNN模型进行比较。</p>
<h4 id="421-effect-of-kernel-size-k-on-eca-module"><a class="markdownIt-Anchor" href="#421-effect-of-kernel-size-k-on-eca-module"></a> 4.2.1 Effect of Kernel Size (k) on ECA Module</h4>
<p>As shown in Eq. (9), our ECA module involves a parameter k, i.e., kernel 1D卷积的大小。在这一部分，我们评估了它对ECA模块的影响，并验证了我们自适应选择核大小的方法的有效性。为此，我们采用ResNet-50和ResNet-101作为骨干模型，通过设置k为3到9，用我们的ECA模块训练它们。结果如图4所示，从中我们可以看出以下几点。</p>
<p>首先，当k在所有卷积块中固定时，ECA模块对ResNet-50和ResNet-101分别在k=9和k=5时获得了最佳结果。由于ResNet101有更多的中间层，主导了ResNet-101的性能，它可能更倾向于小核大小。此外，这些结果表明，不同的深度CNN有不同的最佳k，而k对ECA-Net的性能有明显影响。此外，ResNet-101的精度波动（0.5%）比ResNet50的精度波动（0.15%）大，我们猜测原因是深层网络比浅层网络对固定核大小更敏感。此外，由公式（12）自适应确定的核大小通常优于固定核大小，同时它可以避免通过交叉验证手动调整参数k。以上结果证明了我们的自适应核大小选择在获得更好和稳定的结果方面的有效性。最后，不同k数的ECA模块一直优于SE块，验证了避免降维和局部跨通道交互对学习通道注意力的积极影响。</p>
<h4 id="422-comparisons-using-different-deep-cnns"><a class="markdownIt-Anchor" href="#422-comparisons-using-different-deep-cnns"></a> 4.2.2 Comparisons Using Different Deep CNNs</h4>
<p>ResNet-50 我们使用ResNet-50在ImageNet上比较了我们的ECA模块和几种最先进的注意力方法，包括SENet[14]、CBAM[33]、A2-Nets[4]、AA-Net[1]、GSoP-Net1[9]和GCNet[2]。评价指标包括效率（即网络参数、每秒浮点运算（FLOPs）和训练/推理速度）和有效性（即Top-1/Top5的准确性）。为了进行比较，我们复制了ResNet和SENet的结果[14]，并在其原始论文中报告了其他比较方法的结果。为了测试各种模型的训练/推理速度，我们采用了公开的被比较的CNN的模型，并在同一计算平台上运行它们。结果见表3，我们可以看到我们的ECA-Net与原始的ResNet-50有着几乎相同的模型复杂度（即网络参数、FLOPs和速度），而在Top-1的准确度上取得了2.28%的提高。与最先进的同行（即SENet、CBAM、A2-Nets、AA-Net、GSoP-Net1和GCNet）相比，ECA-Net获得了更好或有竞争力的结果，同时受益于较低的模型复杂性。</p>
<p>ResNet-101 以ResNet-101为骨干模型，我们将ECA-Net与SENet[14]、CBAM[33]和AA-Net[1]进行比较。从表3中我们可以看出，ECA-Net在模型复杂度几乎相同的情况下比原来的ResNet-101高出1.8%。在ResNet-50上也有同样的趋势，ECA-Net优于SENet和CBAM，而它在较低的模型复杂度下对AA-Net非常有竞争力。请注意，AA-Net是通过Inception数据增强和不同的学习率设置进行训练的。</p>
<p>ResNet-152 使用ResNet-152作为骨干模型，我们将ECA-Net与SENet[14]进行比较。从表3可以看出，ECA-Net在模型复杂度几乎相同的情况下，比原来的ResNet-152提高了约1.3%的准确率。与SENet相比，ECANet在模型复杂度较低的情况下实现了0.5%的Top-1收益。关于ResNet-50、ResNet101和ResNet-152的结果表明了我们的ECA模块在广泛使用的ResNet架构上的有效性。</p>
<p>MobileNetV2 除了ResNet架构，我们还验证了我们的ECA模块在轻量级CNN架构上的有效性。为此，我们采用MobileNetV2[28]作为骨干模型，并将我们的ECA模块与SE块进行比较。特别是，在MobileNetV2的每个 &quot;瓶颈 &quot;的残余连接之前，我们将SE块和ECA模块集成到卷积层，SE块的参数r被设置为8。所有的模型都使用完全相同的设置进行训练。表3中的结果显示，我们的ECA-Net在Top-1的准确率方面比原来的MobileNetV2和SENet分别提高了约0.9%和0.14%。此外，我们的ECA-Net比SENet具有更小的模型尺寸和更快的训练/推理速度。以上结果再次验证了我们ECA模块的效率和效果。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515203526654.png" alt="image-20230515203445888" /></p>
<blockquote>
<p>表3.ImageNet上不同注意力方法在网络参数（#.Param.）、每秒浮点运算（FLOPs）、训练或推理速度（每秒一帧，FPS）以及Top-1/Top-5准确性（%）方面的比较。†:由于A2-Net和AA-Net的源代码和模型是公开的，我们没有比较它们的运行时间。♦:AA-Net是通过Inception数据增强和不同的学习率设置进行训练的。</p>
</blockquote>
<h4 id="423-comparisons-with-other-cnn-models"><a class="markdownIt-Anchor" href="#423-comparisons-with-other-cnn-models"></a> 4.2.3 Comparisons with Other CNN Models</h4>
<p>在这一部分的最后，我们将我们的ECA-Net50和ECA-Net101与其他最先进的CNN模型进行比较，包括ResNet-200[12]、Inception-v3[31]、ResNeXt[34]、DenseNet[15]。这些CNN模型具有更深更广的架构，其结果都是从原始论文中复制出来的。如表4所示，ECA-Net101优于ResNet-200，表明我们的ECA-Net可以用更少的计算成本提高深度CNN的性能。同时，我们的ECA-Net101对ResNeXt-101非常有竞争力，而后者采用了更多的卷积滤波器和昂贵的分组卷积。此外，ECA-Net50与DenseNet-264（k=32）、DenseNet-161（k=48）和Inception-v3相当，但它的模型复杂性较低。所有上述结果表明，我们的ECA-Net与最先进的CNN相比表现良好，同时受益于更低的模型复杂度。请注意，我们的ECA也有很大的潜力来进一步提高所比较的CNN模型的性能。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515203721673.png" alt="image-20230515203526654" /></p>
<blockquote>
<p>表4.与ImageNet上最先进的CNN的比较。</p>
</blockquote>
<h3 id="43-object-detection-on-ms-coco"><a class="markdownIt-Anchor" href="#43-object-detection-on-ms-coco"></a> 4.3. Object Detection on MS COCO</h3>
<p>在本小节中，我们使用Faster R-CNN[26]、Mask R-CNN[10]和RetinaNet[22]对我们的ECA-Net在物体检测任务上进行评估。我们主要将ECA-Net与ResNet和SENet进行比较。所有的CNN模型都在ImageNet上进行了预训练，然后通过微调转移到MS COCO。</p>
<h4 id="431-comparisons-using-faster-r-cnn"><a class="markdownIt-Anchor" href="#431-comparisons-using-faster-r-cnn"></a> 4.3.1 Comparisons Using Faster R-CNN</h4>
<p>使用Faster R-CNN作为基本检测器，我们采用50层和101层的ResNets以及FPN[21]作为骨干模型。如表5所示，整合SE块或我们的ECA模块可以以明显的幅度提高物体检测的性能。同时，我们的ECA在使用ResNet-50和ResNet-101的AP方面分别优于SE块0.3%和0.7%。</p>
<h4 id="432-comparisons-using-mask-r-cnn"><a class="markdownIt-Anchor" href="#432-comparisons-using-mask-r-cnn"></a> 4.3.2 Comparisons Using Mask R-CNN</h4>
<p>使用Faster R-CNN作为基本检测器，我们采用50层和101层的ResNets以及FPN[21]作为骨干模型。如表5所示，整合SE块或我们的ECA模块可以以明显的幅度提高物体检测的性能。同时，我们的ECA在使用ResNet-50和ResNet-101的AP方面分别优于SE块0.3%和0.7%。我们进一步利用Mask R-CNN来验证我们的ECA-Net在物体检测任务上的有效性。如表5所示，在50层和101层的设置下，我们的ECA模块在AP方面分别比原始ResNet高出1.8%和1.9%。同时，使用ResNet-50和ResNet-101作为骨干模型，ECA模块比SE块分别取得了0.3%和0.6%的收益。使用ResNet-50，ECA优于一个NL[32]，使用较低的模型复杂度可与GC块[2]媲美。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515203445888.png" alt="image-20230515203643092" /></p>
<blockquote>
<p>表5.不同方法对COCO val2017的物体检测结果。</p>
</blockquote>
<h4 id="433-comparisons-using-retinanet"><a class="markdownIt-Anchor" href="#433-comparisons-using-retinanet"></a> 4.3.3 Comparisons Using RetinaNet</h4>
<p>此外，我们还验证了我们的ECA-Net在使用单级检测器（即RetinaNet）进行物体检测的有效性。如表5所示，在50层和101层的网络中，我们的ECA-Net在AP方面分别比原始的ResNet高出1.8%和1.4%。同时，ECANet对ResNet-50和ResNet-101的SE-Net的改进分别超过0.2%和0.4%。综上所述，表5中的结果表明，我们的ECA-Net可以很好地适用于物体检测任务。具体来说，ECA模块比原来的ResNet有明显的改进，同时在使用较低的模型复杂度的情况下优于SE块。特别是，我们的ECA模块对于通常更难检测的小物体取得了更多的收益。</p>
<h3 id="44-instance-segmentation-on-ms-coco"><a class="markdownIt-Anchor" href="#44-instance-segmentation-on-ms-coco"></a> 4.4. Instance Segmentation on MS COCO</h3>
<p>此外，我们还验证了我们的ECA-Net在使用单级检测器（即RetinaNet）进行物体检测的有效性。如表5所示，在50层和101层的网络中，我们的ECA-Net在AP方面分别比原始的ResNet高出1.8%和1.4%。同时，ECANet对ResNet-50和ResNet-101的SE-Net的改进分别超过0.2%和0.4%。综上所述，表5中的结果表明，我们的ECA-Net可以很好地适用于物体检测任务。具体来说，ECA模块比原来的ResNet有明显的改进，同时在使用较低的模型复杂度时胜过了SE块。特别是，我们的ECA模块对于通常更难检测的小物体取得了更多的收益。然后，我们给出了在MS COCO上使用Mask R-CNN的ECA模块的实例分割结果。如表6所示，ECA模块比原始的ResNet取得了明显的收益，同时在模型复杂度较低的情况下比SE块表现更好。对于作为骨干网的ResNet-50，具有较低模型复杂度的ECA优于NL[32]，并与GC块[2]相当。这些结果验证了我们的ECA模块对各种任务具有良好的泛化能力。</p>
<p><img src="/img/loading.gif" data-original="http://qjl988-tuchuang.oss-cn-beijing.aliyuncs.com/typora/image-20230515203643092.png" alt="image-20230515203721673" /></p>
<blockquote>
<p>表6.在COCO val2017上使用Mask R-CNN的不同方法的实例分割结果。</p>
</blockquote>
<h2 id="5-conclusion"><a class="markdownIt-Anchor" href="#5-conclusion"></a> 5. Conclusion</h2>
<p>在本文中，我们专注于为低模型复杂度的深度CNN学习有效的通道注意力。为此，我们提出了一个高效的通道关注（ECA）模块，它通过快速的一维卷积产生通道关注，其内核大小可以通过通道维度的非线性映射自适应地确定。实验结果表明，我们的ECA是一个极其轻量级的即插即用模块，可以提高各种深度CNN架构的性能，包括广泛使用的ResNets和轻量级MobileNetV2。此外，我们的ECA-Net在物体检测和实例分割任务中表现出良好的泛化能力。在未来，我们将把我们的ECA模块应用于更多的CNN架构（如ResNeXt和Inception[31]），并进一步研究ECA与空间注意力模块的结合。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Generalized-Few-shot-Semantic-Segmentation/" title="Generalized Few-shot Semantic Segmentation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Generalized Few-shot Semantic Segmentation</div></div><div class="info-2"><div class="info-item-1"> Generalized Few-shot Semantic Segmentation  摘要 训练语义分割模型需要大量精细注释的数据，因此很难快速适应不满足这一条件的新类。FewShot...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/HDMNet/" title="Hierarchical Dense Correlation Distillation for Few-Shot Segmentation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Hierarchical Dense Correlation Distillation for Few-Shot Segmentation</div></div><div class="info-2"><div class="info-item-1"> Hierarchical Dense Correlation Distillation for Few-Shot Segmentation  Abstract 小样本语义分割（FSS）旨在通过小样本形成与类无关的模型，对未见类别进行分割。以往局限于语义特征和原型表示的方法存在分割粒度过粗和训练集过拟合的问题。在这项工作中，我们设计了基于transformer架构的分层去耦匹配网络（HDMNet），以挖掘像素级支持相关性。自关注模块用于协助建立分层密集特征，以此完成查询和支持特征之间的级联匹配。此外，我们还提出了一个匹配模块，以减少训练集的过拟合，并引入相关性提炼，利用粗分辨率的语义对应关系来促进细粒度分割。我们的方法在实验中表现出色。我们在 COCO-20i 数据集的一次拍摄设置和五次拍摄分割中分别获得了 50.0% 和 56.0% 的 mIoU。  1.Introduction   图 1. PASCAL-5i [33] 和 COCO-20i [30] 上相关值的激活图。基线容易给训练过程中充分见证的类别（如 &quot;人...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">ASGNet</div></div><div class="info-2"><div class="info-item-1"> ASGNet  Abstract 原型学习被广泛应用于小样本分割。通常情况下，通过平均全局对象信息，从支持特征中获得单一原型。然而，使用一个原型来表示所有信息可能会导致模糊不清。在本文中，我们提出了两个用于多原型提取和分配的新型模块，分别名为超像素引导聚类（SGC）和引导原型分配（GPA）。具体来说，SGC 是一种无参数、无训练的方法，它通过聚合相似的特征向量来提取更具代表性的原型，而 GPA 则能够选择匹配的原型，从而提供更准确的指导。通过将 SGC 和 GPA 整合在一起，我们提出了自适应超像素引导网络 (ASGNet)，它是一种轻量级模型，能适应物体的比例和形状变化。此外，我们的网络还可以很容易地推广到 k 个镜头的分割，并在不增加计算成本的情况下实现大幅改进。特别是，我们在 COCO 上进行的评估表明，ASGNet 在 5 镜头分割方面比最先进的方法高出 5%。  1....</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Head-Free Lightweight Semantic Segmentation with Linear Transformer</div></div><div class="info-2"><div class="info-item-1"> Head-Free Lightweight Semantic Segmentation with Linear Transformer  Abstract 现有的语义分割工作主要集中在设计有效的解码器上；然而，整体结构所带来的计算负荷长期以来一直被忽视，这阻碍了它们在资源有限的硬件上的应用。在本文中，我们提出了一个专门用于语义分割的无头轻量级架构，名为自适应频率变换器（AFFormer）。AFFormer采用了一个并行的架构来利用原型表征作为特定的可学习的局部描述，它取代了解码器并保留了高分辨率特征上丰富的图像语义。虽然去掉解码器后压缩了大部分的计算，但并行结构的准确性仍然受到低计算资源的限制。因此，我们采用异质运算器（CNN和Vision...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</div></div><div class="info-2"><div class="info-item-1"> Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network Automatic Classification and Segmentation of Teeth on 3D Dental Model Using Hierarchical Deep Learning Network ...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">AXIAL-ATTENTION</div></div><div class="info-2"><div class="info-item-1"> AXIAL ATTENTION  ABSTRACT We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">CA</div></div><div class="info-2"><div class="info-item-1"> Coordinate Attention for Efficient Mobile Network Design  Abstract 最近关于移动网络设计的研究已经证明了通道注意（如SE注意力机制）对于提高模型性能的显著效果，但他们通常忽略了位置信息，而位置信息对于生成空间选择性注意图是很重要的。在本文中，我们提出了一种新的移动网络注意机制，将位置信息嵌入到通道注意中，我们称之为...</div></div></div></a><a class="pagination-related" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Biformer/" title="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention</div></div><div class="info-2"><div class="info-item-1">BiFormer: Vision Transformer with Bi-Level Routing Attention BiFormer: Vision Transformer with Bi-Level Routing Attention  2. Related Works Vision transformers. 变形器是一个神经网络家族，它采用信道明智的MLP块来进行每个位置的嵌入（信道混合），采用注意力[42]块来进行跨位置关系建模（空间混合）。变形器最初是为自然语言处理提出的[13, 42]，然后由DETR[1]和ViT[15]等开创性工作引入到计算机视觉。与CNN相比，最大的区别是，transformer使用注意力作为卷积的替代，以实现全局上下文建模。然而，由于香草注意计算所有空间位置的成对特征亲和力，它产生了高计算负担和沉重的内存足迹，特别是对于高分辨率输入。因此，一个重要的研究方向是寻求更有效的注意力机制。 Efficient attention mechanisms....</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qjl988</div><div class="author-info-description">钱家黎的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qjl988"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qjl988" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/mjh1667002013" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=728831102&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1976083684@qq.com" target="_blank" title="Email"><i class="fas fa-email"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#eca-net"><span class="toc-text"> ECA-Net</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-introduction"><span class="toc-text"> 1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-related-work"><span class="toc-text"> 2. Related Work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-proposed-method"><span class="toc-text"> 3. Proposed Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-revisiting-channel-attention-in-se-block"><span class="toc-text"> 3.1. Revisiting Channel Attention in SE Block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-efficient-channel-attention-eca-module"><span class="toc-text"> 3.2. Efficient Channel Attention (ECA) Module</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#321-avoiding-dimensionality-reduction"><span class="toc-text"> 3.2.1 Avoiding Dimensionality Reduction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#322-local-cross-channel-interaction"><span class="toc-text"> 3.2.2 Local Cross-Channel Interaction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#323-coverage-of-local-cross-channel-interaction"><span class="toc-text"> 3.2.3 Coverage of Local Cross-Channel Interaction</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-eca-module-for-deep-cnns"><span class="toc-text"> 3.3. ECA Module for Deep CNNs</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-experiments"><span class="toc-text"> 4. Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-implementation-details"><span class="toc-text"> 4.1. Implementation Details</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-image-classification-on-imagenet-1k"><span class="toc-text"> 4.2. Image Classification on ImageNet-1K</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#421-effect-of-kernel-size-k-on-eca-module"><span class="toc-text"> 4.2.1 Effect of Kernel Size (k) on ECA Module</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#422-comparisons-using-different-deep-cnns"><span class="toc-text"> 4.2.2 Comparisons Using Different Deep CNNs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#423-comparisons-with-other-cnn-models"><span class="toc-text"> 4.2.3 Comparisons with Other CNN Models</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-object-detection-on-ms-coco"><span class="toc-text"> 4.3. Object Detection on MS COCO</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#431-comparisons-using-faster-r-cnn"><span class="toc-text"> 4.3.1 Comparisons Using Faster R-CNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#432-comparisons-using-mask-r-cnn"><span class="toc-text"> 4.3.2 Comparisons Using Mask R-CNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#433-comparisons-using-retinanet"><span class="toc-text"> 4.3.3 Comparisons Using RetinaNet</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#44-instance-segmentation-on-ms-coco"><span class="toc-text"> 4.4. Instance Segmentation on MS COCO</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-conclusion"><span class="toc-text"> 5. Conclusion</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/ASGNet/" title="ASGNet">ASGNet</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/AFFformer/" title="Head-Free Lightweight Semantic Segmentation with Linear Transformer">Head-Free Lightweight Semantic Segmentation with Linear Transformer</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchical-Deep-Learning-Networks/" title="Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network">Automatic-Classification-and-Segmentation-of-Teeth-on-3D-Dental-Model-Using-Hierarchica-Deep-Learning-Network</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/Axial-attention/" title="AXIAL-ATTENTION">AXIAL-ATTENTION</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/10/%E8%AE%BA%E6%96%87/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/CA/" title="CA">CA</a><time datetime="2024-12-10T15:28:34.000Z" title="发表于 2024-12-10 23:28:34">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By qjl988</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>